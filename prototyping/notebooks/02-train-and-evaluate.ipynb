{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02 — Train & Evaluate YOLOv8-Seg Board Detection\n",
    "\n",
    "**Pipeline:** YOLO-seg dataset (from notebook 01) → pretrained baseline →\n",
    "fine-tune YOLOv8n-seg → validate (mask mAP) → qualitative evaluation →\n",
    "pick checkpoint → export ONNX\n",
    "\n",
    "**Inputs:**\n",
    "- `data/yolo-seg-board/` — YOLO-seg dataset (377 train / 93 val, single class `board`)\n",
    "- `yolov8n-seg.pt` — Ultralytics pretrained nano segmentation weights (downloaded automatically)\n",
    "\n",
    "**Outputs:**\n",
    "- `runs/segment/board-detect/weights/best.pt` — Best fine-tuned checkpoint\n",
    "- `runs/segment/board-detect/weights/best.onnx` — ONNX export for deployment\n",
    "- `runs/segment/board-detect/metrics.json` — Key metrics for experiment tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 67\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --- Paths (same convention as notebook 01) ---\n",
    "PROJECT_ROOT = Path.cwd().parent  # prototyping/\n",
    "DATA_DIR = PROJECT_ROOT / \"prototyping/data\"\n",
    "DATASET_DIR = DATA_DIR / \"yolo-seg-board\"\n",
    "DATA_YAML = DATASET_DIR / \"data.yaml\"\n",
    "RUNS_DIR = PROJECT_ROOT / \"trevor-misc/data/fine-tuning-runs\"\n",
    "\n",
    "# --- Training Config ---\n",
    "MODEL_VARIANT = \"yolov8s-seg.pt\"  # SMALL model (11M params) - better capacity than nano (3.2M)\n",
    "                                  # nano was too small → poor detection rate + bad masks\n",
    "                                  # swap to yolov8m-seg.pt (27M) if small still struggles\n",
    "EXPERIMENT_NAME = \"board-detect\"\n",
    "EPOCHS = 100\n",
    "IMGSZ = 640\n",
    "BATCH = 16               # tune down to 8 if OOM; -1 for auto-batch\n",
    "PATIENCE = 20            # early stopping patience\n",
    "OPTIMIZER = \"auto\"       # lets Ultralytics pick AdamW or SGD\n",
    "DEVICE = \"mps\"           # \"mps\" for Apple Silicon, \"0\" for CUDA, \"cpu\" for CPU\n",
    "\n",
    "# --- Fine-Tuning Config ---\n",
    "# CRITICAL: For small datasets (<500 images), freeze backbone layers to prevent overfitting.\n",
    "# Freezing preserves pretrained COCO features while training only the detection head.\n",
    "# freeze=10 → freeze first 10 layers (backbone mostly frozen, neck/head train)\n",
    "# freeze=24 → more aggressive (nearly entire backbone frozen)\n",
    "FREEZE_LAYERS = 10       # Freeze first 10 backbone layers to prevent overfitting\n",
    "\n",
    "# Lower learning rate for fine-tuning (default 0.01 is for training from scratch).\n",
    "# Fine-tuning requires 10-100x lower LR to avoid catastrophic forgetting of pretrained weights.\n",
    "LEARNING_RATE = 0.005    # Lower LR for fine-tuning (vs default 0.01)\n",
    "\n",
    "# --- Augmentation overrides ---\n",
    "# Notebook 01's synthetic generator already applies: brightness (0.7–1.3),\n",
    "# contrast (0.8–1.2), Gaussian noise (sigma 0–15), per-channel color shift (±10),\n",
    "# rotation (±15°), and perspective (strength=0.05).\n",
    "# Dial back YOLO's overlapping augmentations to avoid double-randomization.\n",
    "AUG_OVERRIDES = dict(\n",
    "    hsv_h=0.005,         # default 0.015 — slight hue jitter only\n",
    "    hsv_s=0.3,           # default 0.7\n",
    "    hsv_v=0.2,           # default 0.4\n",
    "    degrees=5.0,         # default 0.0 — very mild (synth already does ±15)\n",
    "    translate=0.05,      # default 0.1\n",
    "    scale=0.2,           # default 0.5\n",
    "    shear=0.0,           # keep off\n",
    "    perspective=0.0,     # synth already does this\n",
    "    flipud=0.0,          # boards don't appear upside-down in practice\n",
    "    fliplr=0.5,          # keep — board is symmetric\n",
    "    mosaic=0.5,          # default 1.0 — halve; synth images already vary a lot\n",
    "    mixup=0.0,           # no sense for single-object seg\n",
    "    copy_paste=0.0,      # keep off\n",
    ")\n",
    "\n",
    "print(f\"Dataset:     {DATA_YAML}\")\n",
    "print(f\"Model:       {MODEL_VARIANT}\")\n",
    "print(f\"Device:      {DEVICE}\")\n",
    "print(f\"Epochs:      {EPOCHS}\")\n",
    "print(f\"Image size:  {IMGSZ}\")\n",
    "print(f\"Batch:       {BATCH}\")\n",
    "print(f\"Patience:    {PATIENCE}\")\n",
    "print(f\"Freeze:      {FREEZE_LAYERS} layers\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Runs dir:    {RUNS_DIR}\")\n",
    "print(f\"Torch:       {torch.__version__}\")\n",
    "print(f\"CUDA avail:  {torch.cuda.is_available()}\")\n",
    "print(f\"MPS avail:   {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Dataset Sanity Check — Verify YAML & Visualize Polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display data.yaml\n",
    "with open(DATA_YAML) as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "\n",
    "print(\"=== data.yaml ===\")\n",
    "for k, v in data_cfg.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Count images per split and type\n",
    "for split in [\"train\", \"val\"]:\n",
    "    img_dir = DATASET_DIR / \"images\" / split\n",
    "    all_imgs = sorted(img_dir.glob(\"*.jpg\"))\n",
    "    real = [p for p in all_imgs if p.name.startswith(\"real_\")]\n",
    "    synth = [p for p in all_imgs if p.name.startswith(\"synth_\")]\n",
    "    print(f\"\\n{split}: {len(all_imgs)} images ({len(real)} real, {len(synth)} synthetic)\")\n",
    "\n",
    "# Verify image-label pairing integrity\n",
    "for split in [\"train\", \"val\"]:\n",
    "    img_stems = {p.stem for p in (DATASET_DIR / \"images\" / split).glob(\"*.jpg\")}\n",
    "    lbl_stems = {p.stem for p in (DATASET_DIR / \"labels\" / split).glob(\"*.txt\")}\n",
    "    assert img_stems == lbl_stems, f\"{split}: image/label mismatch!\"\n",
    "print(\"\\nIntegrity check passed: every image has a matching label.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_yolo_seg_label(img_path, label_path, ax):\n",
    "    \"\"\"Load image + YOLO-seg label, draw polygon overlay.\"\"\"\n",
    "    img = cv2.imread(str(img_path))\n",
    "    h, w = img.shape[:2]\n",
    "    label_text = label_path.read_text().strip()\n",
    "    parts = label_text.split()\n",
    "    coords = [float(x) for x in parts[1:]]\n",
    "    pts = np.array(\n",
    "        [[coords[i] * w, coords[i + 1] * h] for i in range(0, len(coords), 2)],\n",
    "        dtype=np.int32,\n",
    "    )\n",
    "    vis = img.copy()\n",
    "    cv2.polylines(vis, [pts.reshape(-1, 1, 2)], True, (0, 255, 0), 3)\n",
    "    overlay = vis.copy()\n",
    "    cv2.fillPoly(overlay, [pts.reshape(-1, 1, 2)], (0, 255, 0))\n",
    "    vis = cv2.addWeighted(overlay, 0.15, vis, 0.85, 0)\n",
    "    ax.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(img_path.name, fontsize=7)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "# Sample 4 train + 4 val\n",
    "train_imgs = sorted((DATASET_DIR / \"images\" / \"train\").glob(\"*.jpg\"))\n",
    "val_imgs = sorted((DATASET_DIR / \"images\" / \"val\").glob(\"*.jpg\"))\n",
    "sample = random.sample(train_imgs, min(4, len(train_imgs))) + random.sample(val_imgs, min(4, len(val_imgs)))\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for ax, img_path in zip(axes.flatten(), sample):\n",
    "    lbl_path = DATASET_DIR / \"labels\" / img_path.parent.name / f\"{img_path.stem}.txt\"\n",
    "    visualize_yolo_seg_label(img_path, lbl_path, ax)\n",
    "plt.suptitle(\"Dataset Sanity Check (green = label polygon)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Pretrained Baseline — Before Fine-Tuning\n",
    "\n",
    "Run the pretrained COCO model on our validation images to establish a baseline.\n",
    "COCO does not have a \"board\" class, so we expect zero or near-zero detections —\n",
    "this confirms the model needs fine-tuning and gives us a before/after comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = YOLO(MODEL_VARIANT)\n",
    "\n",
    "val_sample = random.sample(val_imgs, min(8, len(val_imgs)))\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for ax, img_path in zip(axes.flatten(), val_sample):\n",
    "    results = pretrained_model.predict(str(img_path), imgsz=IMGSZ, conf=0.25, verbose=False)\n",
    "    annotated = results[0].plot()\n",
    "    ax.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
    "    n_det = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "    ax.set_title(f\"{img_path.name[:25]}\\n{n_det} detections\", fontsize=7)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Pretrained Baseline (COCO weights, no fine-tuning)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Expected: zero or near-zero relevant detections (COCO has no 'board' class).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Train YOLOv8s-Seg — Fine-Tune on Board Dataset\n",
    "\n",
    "Core training cell. This uses **YOLOv8s-seg (small, 11M params)** instead of nano (3.2M params)\n",
    "because the nano model was too small:\n",
    "- Only detected 25% of boards (4/16)\n",
    "- Mask quality was very poor (IoU ~0.5, rectangular/boxy masks)\n",
    "- Training was unstable (metrics bouncing wildly)\n",
    "\n",
    "The small model has **3.5x more capacity** for learning precise boundaries and should give:\n",
    "- Better detection rate (>80% of boards)\n",
    "- Better mask quality (IoU >0.75)\n",
    "- More stable training\n",
    "\n",
    "**Fine-tuning hyperparameters** to prevent overfitting on our small dataset (377 train, 23 real):\n",
    "- **freeze=10**: Freezes first 10 backbone layers, preserving pretrained COCO features\n",
    "- **lr0=0.005**: Lower learning rate (vs default 0.01) to avoid catastrophic forgetting\n",
    "\n",
    "Augmentation is dialed back because the synthetic generator in notebook 01 already applies\n",
    "heavy domain randomization (brightness, contrast, noise, color shift, rotation, perspective).\n",
    "\n",
    "To swap model size: change `MODEL_VARIANT` in Section 1 to `yolov8m-seg.pt` (medium, 27M params)\n",
    "if small still struggles. Training time scales roughly 2–3x per size step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(MODEL_VARIANT)\n",
    "\n",
    "results = model.train(\n",
    "    data=str(DATA_YAML),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    patience=PATIENCE,\n",
    "    optimizer=OPTIMIZER,\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    deterministic=True,\n",
    "    project=str(RUNS_DIR / \"segment\"),\n",
    "    name=EXPERIMENT_NAME,\n",
    "    exist_ok=True,\n",
    "    pretrained=True,\n",
    "    # CRITICAL CHANGES FOR FINE-TUNING:\n",
    "    freeze=FREEZE_LAYERS,    # Freeze backbone to prevent overfitting on small dataset\n",
    "    lr0=LEARNING_RATE,       # Lower learning rate for fine-tuning\n",
    "    # Augmentation overrides (reduced — see Section 1 comments)\n",
    "    **AUG_OVERRIDES,\n",
    "    # Training behavior\n",
    "    verbose=True,\n",
    "    save=True,\n",
    "    save_period=-1,      # only save best and last\n",
    "    plots=True,          # save training plots\n",
    "    val=True,            # validate after each epoch\n",
    "    close_mosaic=10,     # disable mosaic for final 10 epochs\n",
    ")\n",
    "\n",
    "TRAIN_DIR = Path(model.trainer.save_dir)\n",
    "print(f\"\\nTraining complete. Results saved to: {TRAIN_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Training Curves\n",
    "\n",
    "Review loss curves and mAP progression to verify training converged properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_csv = TRAIN_DIR / \"results.csv\"\n",
    "df = pd.read_csv(results_csv)\n",
    "df.columns = df.columns.str.strip()  # Ultralytics pads column names with spaces\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Row 1: Losses (train + val)\n",
    "loss_cols = [\n",
    "    (\"train/box_loss\", \"val/box_loss\", \"Box Loss\"),\n",
    "    (\"train/seg_loss\", \"val/seg_loss\", \"Seg Loss\"),\n",
    "    (\"train/cls_loss\", \"val/cls_loss\", \"Cls Loss\"),\n",
    "]\n",
    "for ax, (train_col, val_col, title) in zip(axes[0], loss_cols):\n",
    "    if train_col in df.columns:\n",
    "        ax.plot(df[\"epoch\"], df[train_col], \"b-\", linewidth=1.5, label=\"train\")\n",
    "    if val_col in df.columns:\n",
    "        ax.plot(df[\"epoch\"], df[val_col], \"r-\", linewidth=1.5, label=\"val\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Metrics\n",
    "metric_cols = [\n",
    "    (\"metrics/precision(B)\", \"Precision\"),\n",
    "    (\"metrics/recall(B)\", \"Recall\"),\n",
    "    (\"metrics/mAP50-95(M)\", \"Mask mAP50-95\"),\n",
    "]\n",
    "for ax, (col, title) in zip(axes[1], metric_cols):\n",
    "    if col in df.columns:\n",
    "        ax.plot(df[\"epoch\"], df[col], \"g-\", linewidth=1.5)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Training Curves\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best epoch\n",
    "if \"metrics/mAP50-95(M)\" in df.columns:\n",
    "    best_row = df.loc[df[\"metrics/mAP50-95(M)\"].idxmax()]\n",
    "    print(f\"Best mask mAP50-95: {best_row['metrics/mAP50-95(M)']:.4f} at epoch {int(best_row['epoch'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Validate — Mask mAP Metrics on Val Set\n",
    "\n",
    "Run `model.val()` on the best checkpoint to get official metrics.\n",
    "Key metric for board segmentation: **mask mAP50-95** (averaged over IoU thresholds 0.50–0.95)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights = TRAIN_DIR / \"weights\" / \"best.pt\"\n",
    "assert best_weights.exists(), f\"best.pt not found at {best_weights}\"\n",
    "\n",
    "best_model = YOLO(str(best_weights))\n",
    "val_results = best_model.val(\n",
    "    data=str(DATA_YAML),\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    device=DEVICE,\n",
    "    split=\"val\",\n",
    "    verbose=True,\n",
    "    plots=True,\n",
    ")\n",
    "\n",
    "box_map50 = val_results.box.map50\n",
    "box_map5095 = val_results.box.map\n",
    "mask_map50 = val_results.seg.map50\n",
    "mask_map5095 = val_results.seg.map\n",
    "\n",
    "print(\"\\n=== Validation Metrics (best.pt) ===\")\n",
    "print(f\"  Box  mAP@50:    {box_map50:.4f}\")\n",
    "print(f\"  Box  mAP@50-95: {box_map5095:.4f}\")\n",
    "print(f\"  Mask mAP@50:    {mask_map50:.4f}\")\n",
    "print(f\"  Mask mAP@50-95: {mask_map5095:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_weights = TRAIN_DIR / \"weights\" / \"last.pt\"\n",
    "assert last_weights.exists(), f\"last.pt not found at {last_weights}\"\n",
    "\n",
    "last_model = YOLO(str(last_weights))\n",
    "last_val = last_model.val(\n",
    "    data=str(DATA_YAML), imgsz=IMGSZ, batch=BATCH, device=DEVICE, split=\"val\", verbose=False\n",
    ")\n",
    "\n",
    "print(\"=== Checkpoint Comparison ===\")\n",
    "print(f\"{'Checkpoint':<12} {'Box mAP50':>10} {'Box mAP50-95':>14} {'Mask mAP50':>12} {'Mask mAP50-95':>15}\")\n",
    "print(\"-\" * 67)\n",
    "print(f\"{'best.pt':<12} {box_map50:>10.4f} {box_map5095:>14.4f} {mask_map50:>12.4f} {mask_map5095:>15.4f}\")\n",
    "print(f\"{'last.pt':<12} {last_val.box.map50:>10.4f} {last_val.box.map:>14.4f} {last_val.seg.map50:>12.4f} {last_val.seg.map:>15.4f}\")\n",
    "\n",
    "use_best = mask_map5095 >= last_val.seg.map\n",
    "chosen = \"best.pt\" if use_best else \"last.pt\"\n",
    "print(f\"\\nChosen checkpoint: {chosen}\")\n",
    "\n",
    "CHOSEN_WEIGHTS = best_weights if use_best else last_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Qualitative Evaluation — Visual Predictions on Val Set\n",
    "\n",
    "Predict on validation images and display overlays. This is the most important\n",
    "QA step — numbers can look good while masks are subtly wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = YOLO(str(CHOSEN_WEIGHTS))\n",
    "\n",
    "val_all = sorted((DATASET_DIR / \"images\" / \"val\").glob(\"*.jpg\"))\n",
    "n_display = min(16, len(val_all))\n",
    "display_imgs = random.sample(val_all, n_display)\n",
    "\n",
    "n_cols = 4\n",
    "n_rows = math.ceil(n_display / n_cols)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_path in enumerate(display_imgs):\n",
    "    preds = eval_model.predict(str(img_path), imgsz=IMGSZ, conf=0.25, verbose=False)\n",
    "    annotated = preds[0].plot()\n",
    "    axes[idx].imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
    "    conf = preds[0].boxes.conf[0].item() if len(preds[0].boxes) > 0 else 0.0\n",
    "    tag = \"real\" if img_path.name.startswith(\"real_\") else \"synth\"\n",
    "    axes[idx].set_title(f\"{img_path.name[:25]}\\n[{tag}] conf={conf:.3f}\", fontsize=7)\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "for idx in range(n_display, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"Fine-Tuned Model Predictions on Val Set\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on real-only val images (most important for production)\n",
    "real_val = sorted(p for p in val_all if p.name.startswith(\"real_\"))\n",
    "print(f\"Real val images: {len(real_val)}\")\n",
    "\n",
    "if len(real_val) > 0:\n",
    "    n_real_display = min(len(real_val), 8)\n",
    "    n_cols_r = min(4, n_real_display)\n",
    "    n_rows_r = math.ceil(n_real_display / n_cols_r)\n",
    "    fig, axes = plt.subplots(n_rows_r, n_cols_r, figsize=(5 * n_cols_r, 5 * n_rows_r))\n",
    "    if n_real_display == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten() if hasattr(axes, \"flatten\") else [axes]\n",
    "\n",
    "    for idx, img_path in enumerate(real_val[:n_real_display]):\n",
    "        preds = eval_model.predict(str(img_path), imgsz=IMGSZ, conf=0.25, verbose=False)\n",
    "        annotated = preds[0].plot()\n",
    "        axes[idx].imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
    "        conf = preds[0].boxes.conf[0].item() if len(preds[0].boxes) > 0 else 0.0\n",
    "        axes[idx].set_title(f\"{img_path.name}\\nconf={conf:.3f}\", fontsize=8)\n",
    "        axes[idx].axis(\"off\")\n",
    "\n",
    "    for idx in range(n_real_display, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\"Predictions on Real Val Images (most important for production)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No real images in val set — all predictions are on synthetic data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Mask Quality — IoU Between Predicted and Ground-Truth Polygons\n",
    "\n",
    "Compute per-image mask IoU for all val images to quantify mask accuracy beyond mAP.\n",
    "Split by real vs synthetic to catch domain-transfer gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask_iou(pred_mask, gt_polygon, img_shape):\n",
    "    \"\"\"Compute IoU between a predicted binary mask and a ground-truth polygon.\"\"\"\n",
    "    h, w = img_shape[:2]\n",
    "    gt_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    cv2.fillPoly(gt_mask, [gt_polygon.reshape(-1, 1, 2)], 1)\n",
    "\n",
    "    # Resize predicted mask to image dimensions (pred comes at model output size)\n",
    "    if pred_mask.shape != (h, w):\n",
    "        pred_mask = cv2.resize(\n",
    "            pred_mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST\n",
    "        )\n",
    "\n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "    return intersection / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "iou_records = []\n",
    "for img_path in tqdm(val_all, desc=\"Computing mask IoUs\"):\n",
    "    # Load GT polygon\n",
    "    lbl_path = DATASET_DIR / \"labels\" / \"val\" / f\"{img_path.stem}.txt\"\n",
    "    label_text = lbl_path.read_text().strip().split()\n",
    "    coords = [float(x) for x in label_text[1:]]\n",
    "    img = cv2.imread(str(img_path))\n",
    "    h, w = img.shape[:2]\n",
    "    gt_pts = np.array(\n",
    "        [[coords[i] * w, coords[i + 1] * h] for i in range(0, len(coords), 2)],\n",
    "        dtype=np.int32,\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    preds = eval_model.predict(str(img_path), imgsz=IMGSZ, conf=0.25, verbose=False)\n",
    "\n",
    "    if preds[0].masks is not None and len(preds[0].masks) > 0:\n",
    "        pred_mask = preds[0].masks.data[0].cpu().numpy()\n",
    "        iou = compute_mask_iou(pred_mask, gt_pts, (h, w))\n",
    "        conf = preds[0].boxes.conf[0].item()\n",
    "    else:\n",
    "        iou = 0.0\n",
    "        conf = 0.0\n",
    "\n",
    "    tag = \"real\" if img_path.name.startswith(\"real_\") else \"synth\"\n",
    "    iou_records.append({\"filename\": img_path.name, \"type\": tag, \"iou\": iou, \"conf\": conf})\n",
    "\n",
    "iou_df = pd.DataFrame(iou_records)\n",
    "\n",
    "print(\"\\n=== Mask IoU Summary ===\")\n",
    "print(f\"  Overall mean IoU: {iou_df['iou'].mean():.4f}\")\n",
    "real_ious = iou_df[iou_df[\"type\"] == \"real\"][\"iou\"]\n",
    "synth_ious = iou_df[iou_df[\"type\"] == \"synth\"][\"iou\"]\n",
    "if len(real_ious) > 0:\n",
    "    print(f\"  Real mean IoU:    {real_ious.mean():.4f}\")\n",
    "if len(synth_ious) > 0:\n",
    "    print(f\"  Synth mean IoU:   {synth_ious.mean():.4f}\")\n",
    "print(f\"  Min IoU:          {iou_df['iou'].min():.4f} ({iou_df.loc[iou_df['iou'].idxmin(), 'filename']})\")\n",
    "print(f\"  Images with IoU < 0.5: {(iou_df['iou'] < 0.5).sum()}\")\n",
    "\n",
    "# Histogram\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(iou_df[\"iou\"], bins=20, edgecolor=\"black\", alpha=0.7)\n",
    "ax.axvline(iou_df[\"iou\"].mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {iou_df['iou'].mean():.3f}\")\n",
    "ax.set_xlabel(\"Mask IoU\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Per-Image Mask IoU Distribution (Val Set)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vuvd3pz8mdq",
   "metadata": {},
   "source": [
    "## 8b. Real vs Synthetic Performance Breakdown\n",
    "\n",
    "Since we have only 5 real validation images vs 88 synthetic, the overall metrics may hide\n",
    "domain-transfer issues. This cell explicitly compares real vs synthetic performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ub08i3e5g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate real vs synthetic images from validation set\n",
    "real_val_imgs = sorted(p for p in val_all if p.name.startswith(\"real_\"))\n",
    "synth_val_imgs = sorted(p for p in val_all if p.name.startswith(\"synth_\"))\n",
    "\n",
    "print(f\"Real validation images:  {len(real_val_imgs)}\")\n",
    "print(f\"Synth validation images: {len(synth_val_imgs)}\")\n",
    "\n",
    "# Compute metrics separately for real and synthetic\n",
    "def eval_subset(img_list, subset_name):\n",
    "    \"\"\"Evaluate model on a subset of images.\"\"\"\n",
    "    if len(img_list) == 0:\n",
    "        return None\n",
    "    \n",
    "    ious = []\n",
    "    confs = []\n",
    "    detect_count = 0\n",
    "    \n",
    "    for img_path in img_list:\n",
    "        # Load GT\n",
    "        lbl_path = DATASET_DIR / \"labels\" / \"val\" / f\"{img_path.stem}.txt\"\n",
    "        label_text = lbl_path.read_text().strip().split()\n",
    "        coords = [float(x) for x in label_text[1:]]\n",
    "        img = cv2.imread(str(img_path))\n",
    "        h, w = img.shape[:2]\n",
    "        gt_pts = np.array(\n",
    "            [[coords[i] * w, coords[i + 1] * h] for i in range(0, len(coords), 2)],\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        \n",
    "        # Predict\n",
    "        preds = eval_model.predict(str(img_path), imgsz=IMGSZ, conf=0.25, verbose=False)\n",
    "        \n",
    "        if preds[0].masks is not None and len(preds[0].masks) > 0:\n",
    "            pred_mask = preds[0].masks.data[0].cpu().numpy()\n",
    "            iou = compute_mask_iou(pred_mask, gt_pts, (h, w))\n",
    "            conf = preds[0].boxes.conf[0].item()\n",
    "            detect_count += 1\n",
    "        else:\n",
    "            iou = 0.0\n",
    "            conf = 0.0\n",
    "        \n",
    "        ious.append(iou)\n",
    "        confs.append(conf)\n",
    "    \n",
    "    return {\n",
    "        \"count\": len(img_list),\n",
    "        \"detected\": detect_count,\n",
    "        \"detection_rate\": detect_count / len(img_list),\n",
    "        \"mean_iou\": np.mean(ious),\n",
    "        \"mean_conf\": np.mean([c for c in confs if c > 0]) if detect_count > 0 else 0.0,\n",
    "        \"ious\": ious,\n",
    "    }\n",
    "\n",
    "real_metrics = eval_subset(real_val_imgs, \"real\")\n",
    "synth_metrics = eval_subset(synth_val_imgs, \"synth\")\n",
    "\n",
    "print(\"\\n=== Real vs Synthetic Performance ===\")\n",
    "print(f\"{'Metric':<20} {'Real':>12} {'Synthetic':>12} {'Gap':>12}\")\n",
    "print(\"-\" * 58)\n",
    "\n",
    "if real_metrics:\n",
    "    print(f\"{'Images':<20} {real_metrics['count']:>12} {synth_metrics['count']:>12} {'-':>12}\")\n",
    "    print(f\"{'Detection Rate':<20} {real_metrics['detection_rate']:>12.2%} {synth_metrics['detection_rate']:>12.2%} {abs(real_metrics['detection_rate'] - synth_metrics['detection_rate']):>11.2%}\")\n",
    "    print(f\"{'Mean IoU':<20} {real_metrics['mean_iou']:>12.4f} {synth_metrics['mean_iou']:>12.4f} {abs(real_metrics['mean_iou'] - synth_metrics['mean_iou']):>12.4f}\")\n",
    "    print(f\"{'Mean Confidence':<20} {real_metrics['mean_conf']:>12.4f} {synth_metrics['mean_conf']:>12.4f} {abs(real_metrics['mean_conf'] - synth_metrics['mean_conf']):>12.4f}\")\n",
    "    \n",
    "    print(\"\\n⚠️  NOTE: With only 5 real validation images, real metrics have high variance.\")\n",
    "    print(\"    Consider collecting 20+ real images for more reliable evaluation.\")\n",
    "    \n",
    "    # Visual comparison\n",
    "    if len(real_val_imgs) > 0 and len(synth_val_imgs) > 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Real IoU distribution\n",
    "        axes[0].hist(real_metrics['ious'], bins=10, edgecolor='black', alpha=0.7, color='coral')\n",
    "        axes[0].axvline(real_metrics['mean_iou'], color='red', linestyle='--', \n",
    "                       label=f\"Mean: {real_metrics['mean_iou']:.3f}\")\n",
    "        axes[0].set_xlabel(\"IoU\")\n",
    "        axes[0].set_ylabel(\"Count\")\n",
    "        axes[0].set_title(f\"Real Images (n={len(real_val_imgs)})\")\n",
    "        axes[0].legend()\n",
    "        axes[0].set_xlim(0, 1)\n",
    "        \n",
    "        # Synthetic IoU distribution\n",
    "        axes[1].hist(synth_metrics['ious'], bins=20, edgecolor='black', alpha=0.7, color='lightblue')\n",
    "        axes[1].axvline(synth_metrics['mean_iou'], color='blue', linestyle='--',\n",
    "                       label=f\"Mean: {synth_metrics['mean_iou']:.3f}\")\n",
    "        axes[1].set_xlabel(\"IoU\")\n",
    "        axes[1].set_ylabel(\"Count\")\n",
    "        axes[1].set_title(f\"Synthetic Images (n={len(synth_val_imgs)})\")\n",
    "        axes[1].legend()\n",
    "        axes[1].set_xlim(0, 1)\n",
    "        \n",
    "        plt.suptitle(\"IoU Distribution: Real vs Synthetic\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No real validation images found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 9. Export to ONNX\n",
    "\n",
    "Export the chosen checkpoint for deployment. ONNX is the target format for\n",
    "inference in the web app (via ONNX Runtime in the browser or server-side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = YOLO(str(CHOSEN_WEIGHTS))\n",
    "\n",
    "export_path = export_model.export(\n",
    "    format=\"onnx\",\n",
    "    imgsz=IMGSZ,\n",
    "    simplify=True,       # run onnxslim to optimize the graph\n",
    "    dynamic=False,       # fixed input shape for simpler deployment\n",
    "    half=False,          # FP32 for maximum compatibility\n",
    ")\n",
    "\n",
    "onnx_path = Path(export_path)\n",
    "print(f\"ONNX exported to: {onnx_path}\")\n",
    "print(f\"File size: {onnx_path.stat().st_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke-test: load ONNX back through Ultralytics and compare to PyTorch\n",
    "assert onnx_path.exists(), f\"ONNX file not found: {onnx_path}\"\n",
    "\n",
    "onnx_model = YOLO(str(onnx_path))\n",
    "test_img = random.choice(val_all)\n",
    "\n",
    "pt_results = eval_model.predict(str(test_img), imgsz=IMGSZ, conf=0.25, verbose=False)\n",
    "onnx_results = onnx_model.predict(str(test_img), imgsz=IMGSZ, conf=0.25, verbose=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(pt_results[0].plot(), cv2.COLOR_BGR2RGB))\n",
    "pt_conf = pt_results[0].boxes.conf[0].item() if len(pt_results[0].boxes) > 0 else 0\n",
    "axes[0].set_title(f\"PyTorch (conf={pt_conf:.3f})\", fontsize=10)\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(onnx_results[0].plot(), cv2.COLOR_BGR2RGB))\n",
    "onnx_conf = onnx_results[0].boxes.conf[0].item() if len(onnx_results[0].boxes) > 0 else 0\n",
    "axes[1].set_title(f\"ONNX (conf={onnx_conf:.3f})\", fontsize=10)\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Smoke Test: PyTorch vs ONNX — {test_img.name}\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Confidence difference: {abs(pt_conf - onnx_conf):.6f}\")\n",
    "if abs(pt_conf - onnx_conf) < 0.01:\n",
    "    print(\"ONNX export verified — predictions match PyTorch within tolerance.\")\n",
    "else:\n",
    "    print(\"WARNING: ONNX predictions differ significantly from PyTorch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 10. Summary & Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"experiment\": EXPERIMENT_NAME,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_variant\": MODEL_VARIANT,\n",
    "    \"epochs_configured\": EPOCHS,\n",
    "    \"epochs_completed\": int(df[\"epoch\"].max()) if \"df\" in dir() else EPOCHS,\n",
    "    \"imgsz\": IMGSZ,\n",
    "    \"batch\": BATCH,\n",
    "    \"seed\": SEED,\n",
    "    \"device\": DEVICE,\n",
    "    \"augmentation_overrides\": AUG_OVERRIDES,\n",
    "    \"dataset\": {\n",
    "        \"train_images\": len(list((DATASET_DIR / \"images\" / \"train\").glob(\"*.jpg\"))),\n",
    "        \"val_images\": len(list((DATASET_DIR / \"images\" / \"val\").glob(\"*.jpg\"))),\n",
    "        \"classes\": data_cfg[\"names\"],\n",
    "    },\n",
    "    \"validation_metrics\": {\n",
    "        \"box_mAP50\": round(box_map50, 4),\n",
    "        \"box_mAP50_95\": round(box_map5095, 4),\n",
    "        \"mask_mAP50\": round(mask_map50, 4),\n",
    "        \"mask_mAP50_95\": round(mask_map5095, 4),\n",
    "    },\n",
    "    \"mask_iou\": {\n",
    "        \"mean\": round(float(iou_df[\"iou\"].mean()), 4),\n",
    "        \"real_mean\": round(float(real_ious.mean()), 4) if len(real_ious) > 0 else None,\n",
    "        \"synth_mean\": round(float(synth_ious.mean()), 4) if len(synth_ious) > 0 else None,\n",
    "        \"min\": round(float(iou_df[\"iou\"].min()), 4),\n",
    "    },\n",
    "    \"chosen_checkpoint\": str(CHOSEN_WEIGHTS.name),\n",
    "    \"artifacts\": {\n",
    "        \"weights_pt\": str(CHOSEN_WEIGHTS),\n",
    "        \"weights_onnx\": str(onnx_path),\n",
    "        \"train_dir\": str(TRAIN_DIR),\n",
    "    },\n",
    "}\n",
    "\n",
    "metrics_path = TRAIN_DIR / \"metrics.json\"\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(f\"Metrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"  Model:             {MODEL_VARIANT}\")\n",
    "print(f\"  Chosen checkpoint: {CHOSEN_WEIGHTS.name}\")\n",
    "print(f\"  Epochs completed:  {metrics['epochs_completed']}\")\n",
    "print()\n",
    "print(\"  Validation Metrics:\")\n",
    "print(f\"    Mask mAP@50:     {mask_map50:.4f}\")\n",
    "print(f\"    Mask mAP@50-95:  {mask_map5095:.4f}\")\n",
    "print(f\"    Mean mask IoU:   {iou_df['iou'].mean():.4f}\")\n",
    "if len(real_ious) > 0:\n",
    "    print(f\"    Real-img IoU:    {real_ious.mean():.4f}\")\n",
    "print()\n",
    "print(\"  Artifacts:\")\n",
    "print(f\"    PyTorch weights: {CHOSEN_WEIGHTS}\")\n",
    "print(f\"    ONNX export:     {onnx_path}\")\n",
    "print(f\"    Metrics JSON:    {metrics_path}\")\n",
    "print(f\"    Training dir:    {TRAIN_DIR}\")\n",
    "print()\n",
    "print(\"  Next steps:\")\n",
    "print(\"    - Copy best.onnx to the web app for inference\")\n",
    "print(\"    - If mask mAP50-95 < 0.85, consider:\")\n",
    "print(\"      - More real training photos\")\n",
    "print(\"      - Trying yolov8s-seg.pt (small) model\")\n",
    "print(\"      - Adjusting augmentation knobs\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796ac34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
