{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 — Full CV Pipeline Prototype\n",
    "\n",
    "**Goal:** Wire together YOLOv8s-seg (board detection) + v0 BoggleCNN (letter classification) into an end-to-end pipeline: raw board photo → letter matrix.\n",
    "\n",
    "**Pipeline stages:**\n",
    "1. YOLO board detection (segmentation mask)\n",
    "2. Mask cleanup (morphological ops)\n",
    "3. Quad fitting (contour → 4 corners)\n",
    "4. Perspective warp (top-down square board)\n",
    "5. Grid size detection (intensity profile valleys)\n",
    "6. Grid-based tile extraction\n",
    "7. Tile preprocessing (v0-compatible)\n",
    "8. CNN inference (letter classification)\n",
    "\n",
    "**Inputs:**\n",
    "- `yolov8s-seg.pt` — Pretrained COCO segmentation model (23.9 MB)\n",
    "- `legacy/models/boggle_cnn.pth` — V0 CNN weights (1.6 MB)\n",
    "- `data/raw/` — 38 board photos (4000x3000 JPGs)\n",
    "\n",
    "**Outputs:**\n",
    "- Validated end-to-end pipeline with accuracy metrics\n",
    "- ONNX exports of both models to `models/`\n",
    "\n",
    "See: `trevor-misc/docs/02-18-26/full-cv-pipeline-plan.md` for detailed design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 67\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"PyTorch {torch.__version__}, OpenCV {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd().parent / \"prototyping\"  # prototyping/\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"legacy/models\"\n",
    "LEGACY_DIR = PROJECT_ROOT / \"legacy\"\n",
    "\n",
    "# Board detection config\n",
    "BOARD_CLASSES = {65, 66, 67}  # COCO: remote, keyboard, cell phone\n",
    "YOLO_CONF = 0.25\n",
    "YOLO_IMGSZ = 640\n",
    "\n",
    "# Tile preprocessing config\n",
    "TARGET_TILE_SIZE = 100  # CNN expects 100x100\n",
    "GRID_INSET_RATIO = 0.10  # 10% inset per side to avoid grid lines\n",
    "\n",
    "# CNN class labels (from legacy/settings.py — inlined for self-containment)\n",
    "CLASS_LABELS = [\n",
    "    \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\",\n",
    "    \"N\", \"O\", \"P\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\",\n",
    "    \"Qu\", \"Er\", \"Th\", \"In\", \"An\", \"He\", \"BLOCK\",\n",
    "]\n",
    "\n",
    "raw_files = sorted(RAW_DIR.glob(\"*.jpg\"))\n",
    "print(f\"Raw photos: {len(raw_files)}\")\n",
    "print(f\"Models dir: {MODELS_DIR} (exists: {MODELS_DIR.exists()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO(str(PROJECT_ROOT / \"yolov8s-seg.pt\"))\n",
    "print(f\"YOLO model loaded: yolov8s-seg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoggleCNN(nn.Module):\n",
    "    \"\"\"V0 BoggleCNN architecture (inlined from legacy/cnn.py).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.15),\n",
    "            nn.Conv2d(8, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.15),\n",
    "            nn.Conv2d(16, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 * 100, 128),  # 32 channels * 10*10 spatial\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, len(CLASS_LABELS)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "cnn_model = BoggleCNN()\n",
    "cnn_model.load_state_dict(\n",
    "    torch.load(\n",
    "        LEGACY_DIR / \"models\" / \"boggle_cnn.pth\",\n",
    "        map_location=\"cpu\",\n",
    "        weights_only=True,\n",
    "    )\n",
    ")\n",
    "cnn_model.eval()\n",
    "print(f\"CNN loaded: {sum(p.numel() for p in cnn_model.parameters()):,} parameters, {len(CLASS_LABELS)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = raw_files[4]\n",
    "test_img = cv2.imread(str(test_img_path))\n",
    "print(f\"Test image: {test_img_path.name} ({test_img.shape[1]}x{test_img.shape[0]})\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "ax.set_title(test_img_path.name)\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Board Detection + Mask Cleanup (Stages 1–2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_board(image, model, conf=YOLO_CONF, imgsz=YOLO_IMGSZ):\n",
    "    \"\"\"Run YOLO inference and return the best board detection.\n",
    "\n",
    "    Returns (mask, box, confidence) or (None, None, 0.0) on failure.\n",
    "    mask is uint8 (0/255) at original image resolution.\n",
    "    \"\"\"\n",
    "    # Try at configured confidence first, then retry lower if no board classes found\n",
    "    for conf_thresh in [conf, 0.15, 0.10]:\n",
    "        results = model(image, conf=conf_thresh, verbose=False, imgsz=imgsz)\n",
    "        result = results[0]\n",
    "\n",
    "        if result.masks is None or len(result.masks) == 0:\n",
    "            continue\n",
    "\n",
    "        # Filter for board-like COCO classes only\n",
    "        board_indices = [\n",
    "            i\n",
    "            for i, cls in enumerate(result.boxes.cls)\n",
    "            if int(cls) in BOARD_CLASSES\n",
    "        ]\n",
    "\n",
    "        if not board_indices:\n",
    "            continue\n",
    "\n",
    "        # Select highest confidence among board-class detections\n",
    "        best_idx = max(board_indices, key=lambda i: result.boxes.conf[i].item())\n",
    "\n",
    "        # Resize mask to original image resolution\n",
    "        mask = result.masks.data[best_idx].cpu().numpy()\n",
    "        h, w = image.shape[:2]\n",
    "        mask = cv2.resize(mask, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "        mask = (mask > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "        return mask, result.boxes[best_idx], result.boxes.conf[best_idx].item()\n",
    "\n",
    "    return None, None, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_mask(mask):\n",
    "    \"\"\"Clean up raw segmentation mask with morphological ops + largest component.\"\"\"\n",
    "    kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel_close)\n",
    "\n",
    "    kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_open)\n",
    "\n",
    "    # Keep largest connected component\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask)\n",
    "    if num_labels <= 1:\n",
    "        return mask\n",
    "\n",
    "    largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
    "    return (labels == largest_label).astype(np.uint8) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mask, box, det_conf = detect_board(test_img, yolo_model)\n",
    "print(f\"Detection confidence: {det_conf:.3f}\")\n",
    "if box is not None:\n",
    "    print(f\"Detected class: {yolo_model.names[int(box.cls)]}\")\n",
    "\n",
    "if raw_mask is None:\n",
    "    raise RuntimeError(\"No board detected in test image!\")\n",
    "\n",
    "clean_mask = cleanup_mask(raw_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "overlay = test_img.copy()\n",
    "overlay[raw_mask > 0] = (\n",
    "    overlay[raw_mask > 0] * 0.5 + np.array([0, 255, 0]) * 0.5\n",
    ").astype(np.uint8)\n",
    "axes[1].imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f\"Raw YOLO Mask (conf={det_conf:.3f})\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(clean_mask, cmap=\"gray\")\n",
    "axes[2].set_title(\"Cleaned Mask\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Board Detection: {test_img_path.name}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Quad Fitting + Perspective Warp (Stages 3–4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quad(mask):\n",
    "    \"\"\"Fit a quadrilateral to the cleaned mask.\n",
    "\n",
    "    Returns (corners_4x2_float32, method_string) or (None, reason).\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None, \"no_contours\"\n",
    "\n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Try progressively coarser approximation until we get 4 points\n",
    "    for eps_mult in np.arange(0.02, 0.10, 0.005):\n",
    "        epsilon = eps_mult * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        if len(approx) == 4:\n",
    "            return approx.reshape(4, 2).astype(np.float32), f\"approxPolyDP(eps={eps_mult:.3f})\"\n",
    "\n",
    "    # Fallback: minimum area rectangle\n",
    "    rect = cv2.minAreaRect(contour)\n",
    "    corners = cv2.boxPoints(rect).astype(np.float32)\n",
    "    return corners, \"minAreaRect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_corners(pts):\n",
    "    \"\"\"Order corners as: top-left, top-right, bottom-right, bottom-left.\"\"\"\n",
    "    s = pts.sum(axis=1)\n",
    "    d = np.diff(pts, axis=1).squeeze()\n",
    "    return np.array(\n",
    "        [\n",
    "            pts[np.argmin(s)],  # TL: smallest x+y\n",
    "            pts[np.argmin(d)],  # TR: smallest y-x\n",
    "            pts[np.argmax(s)],  # BR: largest x+y\n",
    "            pts[np.argmax(d)],  # BL: largest y-x\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "\n",
    "def warp_board(image, corners, pad_pct=0.07):\n",
    "    \"\"\"Warp image to top-down square view using 4 corner points.\n",
    "\n",
    "    pad_pct: expand each corner outward from centroid by this fraction (e.g. 0.03 = 3%).\n",
    "             Prevents edge tiles from being clipped by a tight mask.\n",
    "\n",
    "    Returns (warped_image, side_length).\n",
    "    \"\"\"\n",
    "    ordered = order_corners(corners)\n",
    "\n",
    "    # Expand corners outward from centroid\n",
    "    if pad_pct > 0:\n",
    "        centroid = ordered.mean(axis=0)\n",
    "        ordered = centroid + (1 + pad_pct) * (ordered - centroid)\n",
    "\n",
    "    side_lengths = [\n",
    "        np.linalg.norm(ordered[i] - ordered[(i + 1) % 4]) for i in range(4)\n",
    "    ]\n",
    "    size = int(np.ceil(max(side_lengths)))\n",
    "\n",
    "    dst = np.array([[0, 0], [size, 0], [size, size], [0, size]], dtype=np.float32)\n",
    "    M = cv2.getPerspectiveTransform(ordered, dst)\n",
    "    warped = cv2.warpPerspective(image, M, (size, size))\n",
    "\n",
    "    return warped, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corners, quad_method = fit_quad(clean_mask)\n",
    "print(f\"Quad fitting method: {quad_method}\")\n",
    "print(f\"Corners:\\n{corners}\")\n",
    "\n",
    "warped, warp_size = warp_board(test_img, corners)\n",
    "print(f\"Warped board size: {warp_size}x{warp_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "vis = test_img.copy()\n",
    "ordered = order_corners(corners)\n",
    "labels_txt = [\"TL\", \"TR\", \"BR\", \"BL\"]\n",
    "colors = [(0, 0, 255), (0, 255, 0), (255, 0, 0), (255, 255, 0)]\n",
    "for pt, label, color in zip(ordered, labels_txt, colors):\n",
    "    cv2.circle(vis, tuple(pt.astype(int)), 15, color, -1)\n",
    "    cv2.putText(\n",
    "        vis,\n",
    "        label,\n",
    "        tuple(pt.astype(int) + np.array([20, -10])),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1.5,\n",
    "        color,\n",
    "        3,\n",
    "    )\n",
    "cv2.polylines(vis, [ordered.astype(int).reshape(-1, 1, 2)], True, (0, 255, 0), 3)\n",
    "axes[0].imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(\"Detected Corners\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f\"Warped Board ({warp_size}x{warp_size})\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C+ Tile Detection (Contour-Based)\n",
    "\n",
    "Instead of cropping to the tile area and dividing uniformly, we find individual tile contours directly — matching the v0 approach. This is more robust to uneven spacing and frame artifacts.\n",
    "\n",
    "**Strategy:** HSV saturation (Otsu) separates white tiles from the colored frame → light morph ops to clean up each tile blob → find contours → filter for tile-sized roughly-square shapes → sort into grid by centroid positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from scipy.ndimage import label as ndlabel, center_of_mass, maximum_filter\n",
    "\n",
    "\n",
    "def find_tile_centers(warped, debug=False):\n",
    "    \"\"\"Find tile center positions from local maxima of the distance transform.\n",
    "\n",
    "    Strategy:\n",
    "    1. Blur → Otsu → binary mask of tiles vs frame\n",
    "    2. Distance transform → peaks at tile centers\n",
    "    3. Local maxima detection → candidate points (may include noise)\n",
    "\n",
    "    Returns (centroids_Nx2, debug_info_or_None).  Centroids are (x, y).\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "    h_img, w_img = gray.shape[:2]\n",
    "\n",
    "    # Blur to smooth out letter ink\n",
    "    blur_sigma = max(5, int(w_img * 0.02))\n",
    "    ksize = blur_sigma * 6 + 1\n",
    "    blurred = cv2.GaussianBlur(gray, (ksize, ksize), blur_sigma)\n",
    "\n",
    "    # Otsu threshold\n",
    "    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Distance transform\n",
    "    dist = cv2.distanceTransform(binary, cv2.DIST_L2, 5)\n",
    "\n",
    "    # Local maxima: each tile center is a peak in the distance map.\n",
    "    neighborhood = max(3, int(w_img * 0.10))\n",
    "    local_max = (maximum_filter(dist, size=neighborhood) == dist)\n",
    "    local_max &= (dist > dist.max() * 0.15)\n",
    "\n",
    "    # Label connected maxima regions and get centroids\n",
    "    labeled, n_peaks = ndlabel(local_max)\n",
    "    if n_peaks == 0:\n",
    "        if debug:\n",
    "            return np.empty((0, 2)), {\n",
    "                \"blurred\": blurred, \"binary\": binary, \"dist\": dist,\n",
    "                \"local_max\": local_max, \"n_peaks\": 0, \"blur_sigma\": blur_sigma,\n",
    "                \"neighborhood\": neighborhood,\n",
    "            }\n",
    "        return np.empty((0, 2)), None\n",
    "\n",
    "    # center_of_mass returns (row, col) = (y, x)\n",
    "    raw_centroids = center_of_mass(dist, labeled, range(1, n_peaks + 1))\n",
    "    centroids = np.array([(x, y) for y, x in raw_centroids])\n",
    "\n",
    "    debug_info = None\n",
    "    if debug:\n",
    "        debug_info = {\n",
    "            \"blurred\": blurred,\n",
    "            \"binary\": binary,\n",
    "            \"dist\": dist,\n",
    "            \"local_max\": local_max.astype(np.uint8) * 255,\n",
    "            \"n_peaks\": n_peaks,\n",
    "            \"centroids\": centroids,\n",
    "            \"blur_sigma\": blur_sigma,\n",
    "            \"neighborhood\": neighborhood,\n",
    "        }\n",
    "\n",
    "    return centroids, debug_info\n",
    "\n",
    "\n",
    "def _group_by_proximity(values, tolerance):\n",
    "    \"\"\"Group 1D values into clusters where consecutive sorted values differ by < tolerance.\n",
    "\n",
    "    Returns list of arrays, each containing indices into the original *values* array.\n",
    "    \"\"\"\n",
    "    order = np.argsort(values)\n",
    "    sorted_vals = values[order]\n",
    "\n",
    "    groups = []\n",
    "    current = [order[0]]\n",
    "    for i in range(1, len(sorted_vals)):\n",
    "        if sorted_vals[i] - sorted_vals[i - 1] < tolerance:\n",
    "            current.append(order[i])\n",
    "        else:\n",
    "            groups.append(np.array(current))\n",
    "            current = [order[i]]\n",
    "    groups.append(np.array(current))\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "def _score_combo(groups, centroids_or_values, axis_idx=None):\n",
    "    \"\"\"Score a combination of groups by population × spacing uniformity.\n",
    "\n",
    "    Returns (score, sorted_centers).  Higher score = better grid candidate.\n",
    "    \"\"\"\n",
    "    if axis_idx is not None:\n",
    "        ctrs = np.sort([np.mean(centroids_or_values[g, axis_idx]) for g in groups])\n",
    "    else:\n",
    "        ctrs = np.sort([np.mean(centroids_or_values[g]) for g in groups])\n",
    "\n",
    "    gaps = np.diff(ctrs)\n",
    "    mean_gap = np.mean(gaps)\n",
    "    if mean_gap <= 0:\n",
    "        return -1, ctrs\n",
    "\n",
    "    cv = np.std(gaps) / mean_gap  # coefficient of variation; 0 = perfectly uniform\n",
    "    population = sum(len(g) for g in groups)\n",
    "    score = population * max(0.01, 1 - cv)\n",
    "    return score, ctrs\n",
    "\n",
    "\n",
    "def infer_grid_from_centroids(centroids, image_shape, grid_range=(4, 7)):\n",
    "    \"\"\"Infer a regular NxN grid from (noisy) peak positions.\n",
    "\n",
    "    Enforces square-grid + uniform-spacing constraints:\n",
    "    1. Group y-coords by proximity → candidate rows\n",
    "    2. For each N in [6, 5, 4]: score all N-choose-k combinations of rows\n",
    "       by population × spacing uniformity.  Best combo wins.\n",
    "    3. From the best rows' peaks, repeat for columns.\n",
    "\n",
    "    Returns (grid_size, row_centers, col_centers, tile_size).\n",
    "    \"\"\"\n",
    "    if len(centroids) < grid_range[0]:\n",
    "        return 0, np.array([]), np.array([]), 0\n",
    "\n",
    "    h_img, w_img = image_shape[:2]\n",
    "    min_tile_spacing = min(h_img, w_img) / grid_range[1]\n",
    "    tol = min_tile_spacing * 0.25\n",
    "\n",
    "    # --- Cluster y-values into candidate rows ---\n",
    "    row_groups = _group_by_proximity(centroids[:, 1], tol)\n",
    "    row_sizes = np.array([len(g) for g in row_groups])\n",
    "\n",
    "    # --- Try square grid sizes from largest to smallest ---\n",
    "    for N in range(grid_range[1] - 1, grid_range[0] - 1, -1):\n",
    "        if len(row_groups) < N:\n",
    "            continue\n",
    "\n",
    "        # Pre-filter to top 2*N most populated groups (keeps combos manageable)\n",
    "        sorted_indices = np.argsort(-row_sizes)\n",
    "        candidates = [int(i) for i in sorted_indices[: 2 * N] if row_sizes[i] >= 2]\n",
    "        if len(candidates) < N:\n",
    "            continue\n",
    "\n",
    "        # Score all N-combos of rows\n",
    "        best_row_score = -1\n",
    "        best_row_result = None\n",
    "\n",
    "        for combo in combinations(candidates, N):\n",
    "            groups = [row_groups[i] for i in combo]\n",
    "            score, ctrs = _score_combo(groups, centroids, axis_idx=1)\n",
    "            if score > best_row_score:\n",
    "                best_row_score = score\n",
    "                best_row_result = (groups, ctrs)\n",
    "\n",
    "        if best_row_result is None:\n",
    "            continue\n",
    "\n",
    "        row_groups_sel, row_ctrs = best_row_result\n",
    "\n",
    "        # Collect x-values from peaks in selected rows only\n",
    "        valid_idx = np.concatenate(row_groups_sel)\n",
    "        valid_xs = centroids[valid_idx, 0]\n",
    "\n",
    "        # Cluster x-values into candidate columns\n",
    "        col_groups = _group_by_proximity(valid_xs, tol)\n",
    "        col_sizes = np.array([len(g) for g in col_groups])\n",
    "        if len(col_groups) < N:\n",
    "            continue\n",
    "\n",
    "        # Pre-filter to top 2*N col groups\n",
    "        sorted_c = np.argsort(-col_sizes)\n",
    "        c_candidates = [int(i) for i in sorted_c[: 2 * N] if col_sizes[i] >= 2]\n",
    "        if len(c_candidates) < N:\n",
    "            continue\n",
    "\n",
    "        # Score all N-combos of columns\n",
    "        best_col_score = -1\n",
    "        best_col_ctrs = None\n",
    "\n",
    "        for combo in combinations(c_candidates, N):\n",
    "            c_groups = [col_groups[i] for i in combo]\n",
    "            score, ctrs = _score_combo(c_groups, valid_xs)\n",
    "            if score > best_col_score:\n",
    "                best_col_score = score\n",
    "                best_col_ctrs = ctrs\n",
    "\n",
    "        if best_col_ctrs is None:\n",
    "            continue\n",
    "\n",
    "        col_ctrs = best_col_ctrs\n",
    "\n",
    "        # Tile size from median spacing\n",
    "        y_sp = np.median(np.diff(row_ctrs)) if len(row_ctrs) > 1 else h_img / N\n",
    "        x_sp = np.median(np.diff(col_ctrs)) if len(col_ctrs) > 1 else w_img / N\n",
    "        tile_size = (y_sp + x_sp) / 2\n",
    "\n",
    "        return N, row_ctrs, col_ctrs, tile_size\n",
    "\n",
    "    # Fallback: no valid square grid found\n",
    "    return 0, np.array([]), np.array([]), 0\n",
    "\n",
    "\n",
    "def extract_tiles_from_grid(warped, row_centers, col_centers, tile_size, inset_ratio=GRID_INSET_RATIO):\n",
    "    \"\"\"Extract tiles from a regular grid defined by row/col centers.\n",
    "\n",
    "    Returns list of tile BGR images in row-major order.\n",
    "    \"\"\"\n",
    "    h_img, w_img = warped.shape[:2]\n",
    "    half = int(tile_size * (0.5 - inset_ratio))\n",
    "    tiles = []\n",
    "\n",
    "    for ry in np.sort(row_centers):\n",
    "        for cx in np.sort(col_centers):\n",
    "            cy, cx_int = int(round(ry)), int(round(cx))\n",
    "            y1, y2 = max(0, cy - half), min(h_img, cy + half)\n",
    "            x1, x2 = max(0, cx_int - half), min(w_img, cx_int + half)\n",
    "            tiles.append(warped[y1:y2, x1:x2])\n",
    "\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_tile_perspective(tile_bgr):\n",
    "    \"\"\"Straighten a tile crop using its content's oriented bounding box.\n",
    "\n",
    "    The board-level perspective warp may leave residual per-tile rotation.\n",
    "    This function finds the white tile face via Otsu thresholding, computes\n",
    "    its min-area rotated rectangle, and warps to axis-align it.\n",
    "\n",
    "    Matches the legacy pipeline's per-tile warp_perspective_to_top_down()\n",
    "    applied via min_area_rectangle_contour in extract_tile_images().\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(tile_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    h, w = gray.shape[:2]\n",
    "\n",
    "    # Blur to suppress letter ink, then Otsu to find white tile face\n",
    "    blur_ksize = max(3, int(min(h, w) * 0.1) | 1)\n",
    "    blurred = cv2.GaussianBlur(gray, (blur_ksize, blur_ksize), 0)\n",
    "    _, binary = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find largest contour (the tile face)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return tile_bgr\n",
    "\n",
    "    largest = max(contours, key=cv2.contourArea)\n",
    "\n",
    "    # Skip if contour is too small (degenerate case)\n",
    "    if cv2.contourArea(largest) < 0.3 * h * w:\n",
    "        return tile_bgr\n",
    "\n",
    "    # Min-area rotated rectangle → 4 corners\n",
    "    rect = cv2.minAreaRect(largest)\n",
    "    box = cv2.boxPoints(rect).astype(np.float32)\n",
    "\n",
    "    # Order corners: TL, TR, BR, BL (sum/diff method, same as order_corners)\n",
    "    s = box.sum(axis=1)\n",
    "    d = np.diff(box, axis=1).squeeze()\n",
    "    ordered = np.array([\n",
    "        box[np.argmin(s)],   # TL\n",
    "        box[np.argmin(d)],   # TR\n",
    "        box[np.argmax(s)],   # BR\n",
    "        box[np.argmax(d)],   # BL\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # Warp to square output (legacy forces maxWidth = maxHeight)\n",
    "    side_lengths = [np.linalg.norm(ordered[i] - ordered[(i + 1) % 4]) for i in range(4)]\n",
    "    size = int(np.ceil(max(side_lengths)))\n",
    "    if size < 10:\n",
    "        return tile_bgr\n",
    "\n",
    "    dst = np.array([[0, 0], [size, 0], [size, size], [0, size]], dtype=np.float32)\n",
    "    M = cv2.getPerspectiveTransform(ordered, dst)\n",
    "    warped = cv2.warpPerspective(tile_bgr, M, (size, size))\n",
    "\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find tile centers via distance transform local maxima\n",
    "centroids, dbg = find_tile_centers(warped, debug=True)\n",
    "print(f\"Found {dbg['n_peaks']} raw peaks (blur sigma={dbg['blur_sigma']}, neighborhood={dbg['neighborhood']})\")\n",
    "\n",
    "# Infer grid with square constraint\n",
    "grid_size, row_centers, col_centers, tile_size = infer_grid_from_centroids(centroids, warped.shape)\n",
    "print(f\"Inferred grid: {grid_size}x{grid_size} ({len(row_centers)} rows, {len(col_centers)} cols)\")\n",
    "print(f\"Estimated tile size: {tile_size:.0f}px\")\n",
    "\n",
    "# Extract tiles from inferred grid\n",
    "tiles = extract_tiles_from_grid(warped, row_centers, col_centers, tile_size)\n",
    "print(f\"Extracted {len(tiles)} tiles\")\n",
    "\n",
    "# Show row-group filtering details\n",
    "min_tile_spacing = min(warped.shape[:2]) / 7\n",
    "tol = min_tile_spacing * 0.25\n",
    "row_groups = _group_by_proximity(centroids[:, 1], tol)\n",
    "row_sizes = [len(g) for g in row_groups]\n",
    "print(f\"\\nRow clustering (tol={tol:.0f}px): {len(row_groups)} groups, sizes={row_sizes}\")\n",
    "print(f\"Square constraint → kept top {grid_size} rows by population, top {grid_size} cols\")\n",
    "\n",
    "# Diagnostics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "axes[0, 0].imshow(dbg[\"blurred\"], cmap=\"gray\")\n",
    "axes[0, 0].set_title(f\"Gaussian blur (sigma={dbg['blur_sigma']})\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "axes[0, 1].imshow(dbg[\"dist\"], cmap=\"hot\")\n",
    "axes[0, 1].set_title(\"Distance transform\")\n",
    "axes[0, 1].axis(\"off\")\n",
    "\n",
    "# All peaks with grid-intersection markers\n",
    "vis_peaks = cv2.cvtColor((dbg[\"dist\"] / dbg[\"dist\"].max() * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "# Mark peaks near grid intersections green, others red\n",
    "for cx, cy in centroids:\n",
    "    near_row = any(abs(cy - r) < tol for r in row_centers)\n",
    "    near_col = any(abs(cx - c) < tol for c in col_centers)\n",
    "    color = (0, 255, 0) if (near_row and near_col) else (0, 0, 255)\n",
    "    cv2.circle(vis_peaks, (int(cx), int(cy)), 10, color, 3)\n",
    "axes[0, 2].imshow(cv2.cvtColor(vis_peaks, cv2.COLOR_BGR2RGB))\n",
    "n_on_grid = sum(1 for cx, cy in centroids\n",
    "    if any(abs(cy - r) < tol for r in row_centers) and any(abs(cx - c) < tol for c in col_centers))\n",
    "axes[0, 2].set_title(f\"Peaks: {n_on_grid} on-grid (green) / {dbg['n_peaks'] - n_on_grid} off (red)\")\n",
    "axes[0, 2].axis(\"off\")\n",
    "\n",
    "# Inferred grid on warped image\n",
    "vis_grid = warped.copy()\n",
    "for ry in row_centers:\n",
    "    cv2.line(vis_grid, (0, int(ry)), (vis_grid.shape[1], int(ry)), (0, 255, 0), 2)\n",
    "for cx in col_centers:\n",
    "    cv2.line(vis_grid, (int(cx), 0), (int(cx), vis_grid.shape[0]), (0, 255, 0), 2)\n",
    "axes[1, 0].imshow(cv2.cvtColor(vis_grid, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 0].set_title(f\"Inferred grid ({grid_size}x{grid_size})\")\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "# Extraction regions\n",
    "vis_extract = warped.copy()\n",
    "half = int(tile_size * (0.5 - GRID_INSET_RATIO))\n",
    "for ry in np.sort(row_centers):\n",
    "    for cx in np.sort(col_centers):\n",
    "        y, x = int(round(ry)), int(round(cx))\n",
    "        cv2.rectangle(vis_extract, (x - half, y - half), (x + half, y + half), (0, 255, 0), 2)\n",
    "axes[1, 1].imshow(cv2.cvtColor(vis_extract, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 1].set_title(\"Extraction regions\")\n",
    "axes[1, 1].axis(\"off\")\n",
    "\n",
    "# First row of tiles\n",
    "if tiles:\n",
    "    n_show = min(grid_size, len(tiles))\n",
    "    tile_strip = np.hstack([cv2.resize(t, (100, 100)) for t in tiles[:n_show]])\n",
    "    axes[1, 2].imshow(cv2.cvtColor(tile_strip, cv2.COLOR_BGR2RGB))\n",
    "    axes[1, 2].set_title(f\"First row of extracted tiles\")\n",
    "else:\n",
    "    axes[1, 2].text(0.5, 0.5, \"No tiles extracted\", ha=\"center\", va=\"center\",\n",
    "                    transform=axes[1, 2].transAxes, fontsize=14, color=\"red\")\n",
    "axes[1, 2].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Tile Detection: Distance Transform Peaks → Square Grid Constraint\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Grid Size Detection (Stage 5)\n",
    "\n",
    "Grid size is now inferred from the tile contour count (e.g. 36 contours → 6x6). The intensity-profile method below serves as a secondary validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_grid_size(warped, min_grid=4, max_grid=6):\n",
    "    \"\"\"Detect grid size from intensity-profile valleys.\n",
    "\n",
    "    Returns (grid_size, h_valleys, v_valleys, h_smooth, v_smooth).\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "    h_profile = gray.mean(axis=1)\n",
    "    v_profile = gray.mean(axis=0)\n",
    "\n",
    "    sigma = len(h_profile) * 0.02\n",
    "    h_smooth = gaussian_filter1d(h_profile, sigma)\n",
    "    v_smooth = gaussian_filter1d(v_profile, sigma)\n",
    "\n",
    "    min_distance = len(h_profile) * 0.10\n",
    "    margin = int(len(h_profile) * 0.05)\n",
    "\n",
    "    # Find valleys (invert signal to find minima via find_peaks)\n",
    "    h_valleys, _ = find_peaks(-h_smooth, distance=min_distance, prominence=5)\n",
    "    v_valleys, _ = find_peaks(-v_smooth, distance=min_distance, prominence=5)\n",
    "\n",
    "    # Keep only interior valleys (exclude edges)\n",
    "    h_interior = h_valleys[(h_valleys > margin) & (h_valleys < len(h_profile) - margin)]\n",
    "    v_interior = v_valleys[(v_valleys > margin) & (v_valleys < len(v_profile) - margin)]\n",
    "\n",
    "    grid_h = len(h_interior) + 1\n",
    "    grid_v = len(v_interior) + 1\n",
    "\n",
    "    # If both agree and are in valid range, use that\n",
    "    if grid_h == grid_v and min_grid <= grid_h <= max_grid:\n",
    "        return grid_h, h_interior, v_interior, h_smooth, v_smooth\n",
    "\n",
    "    h_valid = min_grid <= grid_h <= max_grid\n",
    "    v_valid = min_grid <= grid_v <= max_grid\n",
    "\n",
    "    if h_valid and not v_valid:\n",
    "        return grid_h, h_interior, v_interior, h_smooth, v_smooth\n",
    "    if v_valid and not h_valid:\n",
    "        return grid_v, h_interior, v_interior, h_smooth, v_smooth\n",
    "    if h_valid and v_valid:\n",
    "        grid_size = round((grid_h + grid_v) / 2)\n",
    "        grid_size = max(min_grid, min(max_grid, grid_size))\n",
    "        return grid_size, h_interior, v_interior, h_smooth, v_smooth\n",
    "\n",
    "    # Fallback: assume 6x6 (Super Big Boggle)\n",
    "    print(f\"WARNING: Grid detection ambiguous (h={grid_h}, v={grid_v}). Defaulting to 6.\")\n",
    "    return 6, h_interior, v_interior, h_smooth, v_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Peak-based grid: {grid_size}x{grid_size} ({len(row_centers)} rows × {len(col_centers)} cols)\")\n",
    "print(f\"Tile size: {tile_size:.0f}px, {len(centroids)} peaks found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Extraction regions on warped image\n",
    "vis_boxes = warped.copy()\n",
    "half = int(tile_size * (0.5 - GRID_INSET_RATIO))\n",
    "for ry in np.sort(row_centers):\n",
    "    for cx in np.sort(col_centers):\n",
    "        y, x = int(round(ry)), int(round(cx))\n",
    "        cv2.rectangle(vis_boxes, (x - half, y - half), (x + half, y + half), (0, 255, 0), 2)\n",
    "axes[0].imshow(cv2.cvtColor(vis_boxes, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(f\"Tile Grid ({grid_size}x{grid_size}, {len(tiles)} tiles)\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Full NxN grid of extracted tiles\n",
    "n = grid_size\n",
    "if len(tiles) >= n * n:\n",
    "    fig2, axes2 = plt.subplots(n, n, figsize=(2 * n, 2 * n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            idx = i * n + j\n",
    "            if idx < len(tiles):\n",
    "                axes2[i, j].imshow(cv2.cvtColor(tiles[idx], cv2.COLOR_BGR2RGB))\n",
    "            axes2[i, j].axis(\"off\")\n",
    "    plt.suptitle(f\"Extracted Tiles ({grid_size}x{grid_size})\", fontsize=14)\n",
    "\n",
    "# Intensity profiles (for reference)\n",
    "ip_grid_size, h_valleys, v_valleys, h_smooth, v_smooth = detect_grid_size(warped)\n",
    "axes[1].plot(h_smooth, \"b-\", linewidth=1, label=\"horizontal\")\n",
    "axes[1].plot(v_smooth, \"r-\", linewidth=1, alpha=0.7, label=\"vertical\")\n",
    "for v in h_valleys:\n",
    "    axes[1].axvline(v, color=\"b\", linestyle=\"--\", alpha=0.4)\n",
    "for v in v_valleys:\n",
    "    axes[1].axvline(v, color=\"r\", linestyle=\"--\", alpha=0.4)\n",
    "axes[1].set_title(f\"Intensity Profiles (validation: {ip_grid_size}x{ip_grid_size})\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Tile Extraction + Preprocessing (Stages 6–7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_grid_tiles(warped, grid_size, inset_ratio=GRID_INSET_RATIO):\n",
    "    \"\"\"Extract NxN tiles from the warped board image with inset.\"\"\"\n",
    "    h, w = warped.shape[:2]\n",
    "    cell_h, cell_w = h / grid_size, w / grid_size\n",
    "    tiles = []\n",
    "\n",
    "    for row in range(grid_size):\n",
    "        for col in range(grid_size):\n",
    "            cx = int(col * cell_w + cell_w / 2)\n",
    "            cy = int(row * cell_h + cell_h / 2)\n",
    "            half = int(cell_w * (0.5 - inset_ratio))\n",
    "\n",
    "            y1, y2 = max(0, cy - half), min(h, cy + half)\n",
    "            x1, x2 = max(0, cx - half), min(w, cx + half)\n",
    "            tiles.append(warped[y1:y2, x1:x2])\n",
    "\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _contour_depth(hierarchy, idx):\n",
    "    \"\"\"Walk the parent chain to compute contour depth.\"\"\"\n",
    "    depth = 0\n",
    "    while hierarchy[idx][3] != -1:\n",
    "        idx = hierarchy[idx][3]\n",
    "        depth += 1\n",
    "    return depth\n",
    "\n",
    "\n",
    "def preprocess_tile_v0(tile_bgr, target_size=TARGET_TILE_SIZE):\n",
    "    \"\"\"Replicate v0 preprocessing: adaptive threshold -> contour mask -> center -> resize.\n",
    "\n",
    "    Produces a 100x100 uint8 image (white letter on black background).\n",
    "\n",
    "    Key details matched to v0 (board_detection.py extract_tile_images + center_letter_image):\n",
    "    - Adaptive threshold block_size = tile_area * 0.015 (v0 uses mean_tile_area * 0.015)\n",
    "    - Only depth-1 contours are filtered by min_area; depth-2+ kept unconditionally\n",
    "    - Letter is centered in the ORIGINAL tile frame (not tight-cropped), preserving\n",
    "      the letter-to-canvas ratio the CNN was trained on\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(tile_bgr, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Adaptive threshold — v0 uses mean_tile_area * 0.015\n",
    "    tile_area = gray.shape[0] * gray.shape[1]\n",
    "    block_size = max(3, int(tile_area * 0.015) | 1)  # must be odd and >= 3\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, 5\n",
    "    )\n",
    "\n",
    "    # Binary cleanup\n",
    "    _, thresh = cv2.threshold(thresh, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Contour detection with hierarchy\n",
    "    contours, hierarchy = cv2.findContours(\n",
    "        thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "\n",
    "    if not contours or hierarchy is None:\n",
    "        return cv2.resize(gray, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    h = hierarchy[0]  # shape: (N, 4) -> [next, prev, child, parent]\n",
    "    min_area = tile_area * 0.003  # 0.3% — matches v0 production\n",
    "\n",
    "    # Build binary mask: depth-1 contours -> white, depth-2+ -> black\n",
    "    # V0 only applies min_area filter to depth-1; depth-2+ are drawn unconditionally\n",
    "    mask = np.zeros(gray.shape, dtype=np.uint8)\n",
    "    for i, cnt in enumerate(contours):\n",
    "        depth = _contour_depth(h, i)\n",
    "        if depth == 1:  # letter strokes — filter by area\n",
    "            if cv2.contourArea(cnt) < min_area:\n",
    "                continue\n",
    "            cv2.drawContours(mask, [cnt], -1, 255, cv2.FILLED)\n",
    "        elif depth >= 2:  # counter-spaces (holes in O, B, D, etc.) — always draw\n",
    "            cv2.drawContours(mask, [cnt], -1, 0, cv2.FILLED)\n",
    "\n",
    "    # Center letter in ORIGINAL tile frame (matches v0 center_letter_image).\n",
    "    # This preserves the letter-to-canvas ratio. The old tight-crop approach\n",
    "    # blew up letters to fill the entire 100x100 canvas, which doesn't match\n",
    "    # the training data where letters fill ~60-80% of the frame.\n",
    "    y_coords, x_coords = np.where(mask > 1)\n",
    "    if len(x_coords) > 0 and len(y_coords) > 0:\n",
    "        x_min, y_min = np.min(x_coords), np.min(y_coords)\n",
    "        x_max, y_max = np.max(x_coords), np.max(y_coords)\n",
    "        cropped = mask[y_min:y_max, x_min:x_max]\n",
    "        img_h, img_w = mask.shape\n",
    "        centered = np.zeros_like(mask)\n",
    "        start_x = (img_w - cropped.shape[1]) // 2\n",
    "        start_y = (img_h - cropped.shape[0]) // 2\n",
    "        centered[start_y : start_y + cropped.shape[0], start_x : start_x + cropped.shape[1]] = cropped\n",
    "        mask = centered\n",
    "\n",
    "    return cv2.resize(mask, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "def preprocess_tile_simple(tile_bgr, target_size=TARGET_TILE_SIZE):\n",
    "    \"\"\"Minimal preprocessing: grayscale -> invert -> Otsu threshold -> resize.\"\"\"\n",
    "    gray = cv2.cvtColor(tile_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    if gray.mean() > 127:\n",
    "        gray = 255 - gray\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    return cv2.resize(binary, (target_size, target_size), interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiles are already extracted via contour detection (cell above).\n",
    "# Show first row: raw crop / v0-preprocessed / simplified\n",
    "print(f\"Using {len(tiles)} tiles ({grid_size}x{grid_size}) from contour detection\")\n",
    "\n",
    "n_show = min(grid_size, len(tiles))\n",
    "fig, axes = plt.subplots(3, n_show, figsize=(2.5 * n_show, 8))\n",
    "\n",
    "for col in range(n_show):\n",
    "    tile = tiles[col]\n",
    "    v0_proc = preprocess_tile_v0(tile)\n",
    "    simple_proc = preprocess_tile_simple(tile)\n",
    "\n",
    "    axes[0, col].imshow(cv2.cvtColor(tile, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, col].set_title(f\"(0,{col})\", fontsize=8)\n",
    "    axes[0, col].axis(\"off\")\n",
    "\n",
    "    axes[1, col].imshow(v0_proc, cmap=\"gray\")\n",
    "    axes[1, col].set_title(\"v0\", fontsize=8)\n",
    "    axes[1, col].axis(\"off\")\n",
    "\n",
    "    axes[2, col].imshow(simple_proc, cmap=\"gray\")\n",
    "    axes[2, col].set_title(\"simple\", fontsize=8)\n",
    "    axes[2, col].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Raw\", fontsize=10)\n",
    "axes[1, 0].set_ylabel(\"V0 Preproc\", fontsize=10)\n",
    "axes[2, 0].set_ylabel(\"Simple\", fontsize=10)\n",
    "\n",
    "plt.suptitle(f\"Tile Preprocessing Comparison (row 0 of {grid_size}x{grid_size})\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full NxN grid of v0-preprocessed tiles\n",
    "fig, axes = plt.subplots(grid_size, grid_size, figsize=(2 * grid_size, 2 * grid_size))\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        idx = i * grid_size + j\n",
    "        proc = preprocess_tile_v0(tiles[idx])\n",
    "        axes[i, j].imshow(proc, cmap=\"gray\")\n",
    "        axes[i, j].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"All V0-Preprocessed Tiles\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-tile perspective correction: visual A/B comparison\n",
    "# Shows raw crop vs corrected crop vs preprocessed output for each variant\n",
    "corrected_tiles = [correct_tile_perspective(t) for t in tiles]\n",
    "\n",
    "n_show = min(grid_size, len(tiles))\n",
    "fig, axes = plt.subplots(4, n_show, figsize=(2.5 * n_show, 10))\n",
    "\n",
    "for col in range(n_show):\n",
    "    raw = tiles[col]\n",
    "    corrected = corrected_tiles[col]\n",
    "    proc_raw = preprocess_tile_v0(raw)\n",
    "    proc_corrected = preprocess_tile_v0(corrected)\n",
    "\n",
    "    axes[0, col].imshow(cv2.cvtColor(cv2.resize(raw, (100, 100)), cv2.COLOR_BGR2RGB))\n",
    "    axes[0, col].set_title(f\"(0,{col})\", fontsize=8)\n",
    "    axes[0, col].axis(\"off\")\n",
    "\n",
    "    axes[1, col].imshow(cv2.cvtColor(cv2.resize(corrected, (100, 100)), cv2.COLOR_BGR2RGB))\n",
    "    axes[1, col].axis(\"off\")\n",
    "\n",
    "    axes[2, col].imshow(proc_raw, cmap=\"gray\")\n",
    "    axes[2, col].axis(\"off\")\n",
    "\n",
    "    axes[3, col].imshow(proc_corrected, cmap=\"gray\")\n",
    "    axes[3, col].axis(\"off\")\n",
    "\n",
    "axes[0, 0].set_ylabel(\"Raw Crop\", fontsize=10)\n",
    "axes[1, 0].set_ylabel(\"Persp. Corrected\", fontsize=10)\n",
    "axes[2, 0].set_ylabel(\"V0 (no corr.)\", fontsize=10)\n",
    "axes[3, 0].set_ylabel(\"V0 (corrected)\", fontsize=10)\n",
    "\n",
    "plt.suptitle(\"Per-Tile Perspective Correction: Before vs After\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. CNN Inference (Stage 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tile(model, preprocessed_tile):\n",
    "    \"\"\"Run CNN inference on a single preprocessed 100x100 tile.\n",
    "\n",
    "    IMPORTANT: Feeds raw [0, 255] float values — the v0 CNN was trained this way.\n",
    "\n",
    "    Returns (letter, confidence).\n",
    "    \"\"\"\n",
    "    tensor = (\n",
    "        torch.from_numpy(preprocessed_tile.astype(np.float32))\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(0)\n",
    "    )  # (1, 1, 100, 100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        conf, idx = probs.max(dim=1)\n",
    "\n",
    "    return CLASS_LABELS[idx.item()], conf.item()\n",
    "\n",
    "\n",
    "def predict_tiles_batch(model, preprocessed_tiles):\n",
    "    \"\"\"Run CNN inference on a batch of preprocessed tiles.\n",
    "\n",
    "    Returns (letters_list, confidences_list).\n",
    "    \"\"\"\n",
    "    batch = np.stack(preprocessed_tiles).astype(np.float32)\n",
    "    tensor = torch.from_numpy(batch).unsqueeze(1)  # (N, 1, 100, 100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        confs, idxs = probs.max(dim=1)\n",
    "\n",
    "    letters = [CLASS_LABELS[i.item()] for i in idxs]\n",
    "    confidences = confs.tolist()\n",
    "    return letters, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0_preprocessed = [preprocess_tile_v0(t) for t in tiles]\n",
    "v0_letters, v0_confs = predict_tiles_batch(cnn_model, v0_preprocessed)\n",
    "\n",
    "print(\"V0-Preprocessed Results:\")\n",
    "for i in range(grid_size):\n",
    "    row_letters = v0_letters[i * grid_size : (i + 1) * grid_size]\n",
    "    row_confs = v0_confs[i * grid_size : (i + 1) * grid_size]\n",
    "    row_str = \" | \".join(f\"{l:>5s} ({c:.2f})\" for l, c in zip(row_letters, row_confs))\n",
    "    print(f\"  Row {i}: {row_str}\")\n",
    "\n",
    "print(f\"\\nMean confidence: {np.mean(v0_confs):.3f}\")\n",
    "print(f\"Min confidence:  {np.min(v0_confs):.3f}\")\n",
    "print(f\"Tiles < 0.5 conf: {sum(1 for c in v0_confs if c < 0.5)}/{len(v0_confs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_preprocessed = [preprocess_tile_simple(t) for t in tiles]\n",
    "simple_letters, simple_confs = predict_tiles_batch(cnn_model, simple_preprocessed)\n",
    "\n",
    "print(\"Simplified Results:\")\n",
    "for i in range(grid_size):\n",
    "    row_letters = simple_letters[i * grid_size : (i + 1) * grid_size]\n",
    "    row_confs = simple_confs[i * grid_size : (i + 1) * grid_size]\n",
    "    row_str = \" | \".join(f\"{l:>5s} ({c:.2f})\" for l, c in zip(row_letters, row_confs))\n",
    "    print(f\"  Row {i}: {row_str}\")\n",
    "\n",
    "print(f\"\\nMean confidence: {np.mean(simple_confs):.3f}\")\n",
    "\n",
    "agree = sum(1 for a, b in zip(v0_letters, simple_letters) if a == b)\n",
    "print(f\"\\nAgreement: {agree}/{len(v0_letters)} tiles ({100 * agree / len(v0_letters):.0f}%)\")\n",
    "print(f\"V0 mean conf: {np.mean(v0_confs):.3f} vs Simple mean conf: {np.mean(simple_confs):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "letter_matrix = np.array(v0_letters).reshape(grid_size, grid_size)\n",
    "conf_matrix = np.array(v0_confs).reshape(grid_size, grid_size)\n",
    "\n",
    "# Letter matrix with color-coded confidence\n",
    "ax = axes[0]\n",
    "ax.set_xlim(-0.5, grid_size - 0.5)\n",
    "ax.set_ylim(grid_size - 0.5, -0.5)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        c = conf_matrix[i, j]\n",
    "        color = \"green\" if c > 0.8 else \"orange\" if c > 0.5 else \"red\"\n",
    "        ax.text(\n",
    "            j, i, letter_matrix[i, j],\n",
    "            ha=\"center\", va=\"center\", fontsize=16, fontweight=\"bold\", color=color,\n",
    "        )\n",
    "ax.set_title(\"Predicted Letter Matrix\")\n",
    "ax.set_xticks(range(grid_size))\n",
    "ax.set_yticks(range(grid_size))\n",
    "ax.grid(True)\n",
    "\n",
    "# Confidence heatmap\n",
    "im = axes[1].imshow(conf_matrix, cmap=\"RdYlGn\", vmin=0, vmax=1)\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        axes[1].text(\n",
    "            j, i, f\"{conf_matrix[i, j]:.2f}\",\n",
    "            ha=\"center\", va=\"center\", fontsize=9,\n",
    "        )\n",
    "axes[1].set_title(\"Confidence Heatmap\")\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.suptitle(f\"CNN Results: {test_img_path.name}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D+ Grid Size Validation (confidence-based fallback)\n",
    "\n",
    "Now that tile extraction and CNN inference are defined, we can run the brute-force grid size validation as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_grid_size_by_confidence(warped_img, cnn, grid_candidates=(4, 5, 6)):\n",
    "    \"\"\"Test multiple grid sizes and pick the one with highest mean CNN confidence.\n",
    "\n",
    "    Uses the uniform grid method as a fallback/validation for contour detection.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for n in grid_candidates:\n",
    "        t = extract_grid_tiles(warped_img, n)\n",
    "        proc = [preprocess_tile_v0(tile) for tile in t]\n",
    "        _, confs = predict_tiles_batch(cnn, proc)\n",
    "        results[n] = {\n",
    "            \"mean_conf\": np.mean(confs),\n",
    "            \"min_conf\": np.min(confs),\n",
    "            \"n_high_conf\": sum(1 for c in confs if c > 0.8),\n",
    "            \"total\": n * n,\n",
    "        }\n",
    "\n",
    "    for n, r in sorted(results.items()):\n",
    "        print(\n",
    "            f\"  Grid {n}x{n}: mean_conf={r['mean_conf']:.3f}, \"\n",
    "            f\"min_conf={r['min_conf']:.3f}, high_conf={r['n_high_conf']}/{r['total']}\"\n",
    "        )\n",
    "\n",
    "    best = max(results, key=lambda n: results[n][\"mean_conf\"])\n",
    "    return best, results\n",
    "\n",
    "\n",
    "print(f\"Contour-based detection: {grid_size}x{grid_size}\")\n",
    "print(\"Confidence-based validation (uniform grid on warped):\")\n",
    "best_grid, grid_results = validate_grid_size_by_confidence(warped, cnn_model)\n",
    "print(f\"Best by confidence: {best_grid}x{best_grid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_board(image_path, grid_size_override=None, preprocessing=\"v0\"):\n",
    "    \"\"\"Full CV pipeline: image path -> letter matrix.\n",
    "\n",
    "    Returns a dict with 'letters', 'confidences', 'grid_size', 'mean_confidence',\n",
    "    and 'stages' (intermediate results). On failure, returns a dict with 'error'.\n",
    "    \"\"\"\n",
    "    preprocess_fn = preprocess_tile_v0 if preprocessing == \"v0\" else preprocess_tile_simple\n",
    "\n",
    "    image = cv2.imread(str(image_path))\n",
    "    if image is None:\n",
    "        return {\"error\": f\"Failed to load {image_path}\"}\n",
    "\n",
    "    stages = {\"image_shape\": image.shape}\n",
    "\n",
    "    # Stage 1: Board detection\n",
    "    mask, box, det_conf = detect_board(image, yolo_model)\n",
    "    if mask is None:\n",
    "        return {\"error\": \"No board detected\", \"stages\": stages}\n",
    "    stages[\"detection_conf\"] = det_conf\n",
    "    stages[\"detection_class\"] = yolo_model.names[int(box.cls)] if box is not None else None\n",
    "\n",
    "    # Stage 2: Mask cleanup\n",
    "    clean = cleanup_mask(mask)\n",
    "\n",
    "    # Stage 3: Quad fitting\n",
    "    corners, quad_method = fit_quad(clean)\n",
    "    if corners is None:\n",
    "        return {\"error\": \"Quad fitting failed\", \"stages\": stages}\n",
    "    stages[\"quad_method\"] = quad_method\n",
    "\n",
    "    # Stage 4: Perspective warp\n",
    "    warped_img, warp_sz = warp_board(image, corners)\n",
    "    stages[\"warp_size\"] = warp_sz\n",
    "    stages[\"warped\"] = warped_img\n",
    "\n",
    "    # Stage 5: Find tile centers via distance transform peaks\n",
    "    tile_centroids, _ = find_tile_centers(warped_img)\n",
    "    if len(tile_centroids) < 4:\n",
    "        return {\"error\": f\"Only found {len(tile_centroids)} tile peaks\", \"stages\": stages}\n",
    "\n",
    "    # Stage 5b: Infer grid from centroid clustering\n",
    "    gs, rows, cols, tsize = infer_grid_from_centroids(tile_centroids, warped_img.shape)\n",
    "    if grid_size_override:\n",
    "        gs = grid_size_override\n",
    "    stages[\"grid_size\"] = gs\n",
    "    stages[\"n_peaks\"] = len(tile_centroids)\n",
    "\n",
    "    # Stage 6: Extract tiles from inferred grid\n",
    "    tile_imgs = extract_tiles_from_grid(warped_img, rows, cols, tsize)\n",
    "\n",
    "    # Stage 6b: Per-tile perspective correction (matches legacy per-tile warp)\n",
    "    tile_imgs = [correct_tile_perspective(t) for t in tile_imgs]\n",
    "    stages[\"tiles\"] = tile_imgs\n",
    "\n",
    "    # Stages 7-8: Preprocessing + CNN\n",
    "    preprocessed = [preprocess_fn(t) for t in tile_imgs]\n",
    "    letters, confs = predict_tiles_batch(cnn_model, preprocessed)\n",
    "\n",
    "    letter_grid = [letters[i * gs : (i + 1) * gs] for i in range(gs)]\n",
    "    conf_grid = [confs[i * gs : (i + 1) * gs] for i in range(gs)]\n",
    "\n",
    "    return {\n",
    "        \"letters\": letter_grid,\n",
    "        \"confidences\": conf_grid,\n",
    "        \"grid_size\": gs,\n",
    "        \"mean_confidence\": float(np.mean(confs)),\n",
    "        \"min_confidence\": float(np.min(confs)),\n",
    "        \"stages\": stages,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on 4 photos spread across the dataset\n",
    "test_indices = [0, 10, 20, 30]\n",
    "test_paths = [raw_files[i] for i in test_indices if i < len(raw_files)]\n",
    "\n",
    "fig, axes = plt.subplots(len(test_paths), 3, figsize=(18, 5 * len(test_paths)))\n",
    "if len(test_paths) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for row, img_path in enumerate(test_paths):\n",
    "    result = analyze_board(img_path)\n",
    "\n",
    "    if \"error\" in result:\n",
    "        for col in range(3):\n",
    "            axes[row, col].text(\n",
    "                0.5, 0.5, f\"ERROR: {result['error']}\",\n",
    "                ha=\"center\", va=\"center\", transform=axes[row, col].transAxes,\n",
    "                fontsize=12, color=\"red\",\n",
    "            )\n",
    "            axes[row, col].set_title(img_path.name, fontsize=9)\n",
    "            axes[row, col].axis(\"off\")\n",
    "        continue\n",
    "\n",
    "    # Original\n",
    "    img = cv2.imread(str(img_path))\n",
    "    axes[row, 0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    axes[row, 0].set_title(img_path.name, fontsize=9)\n",
    "    axes[row, 0].axis(\"off\")\n",
    "\n",
    "    # Warped board\n",
    "    axes[row, 1].imshow(cv2.cvtColor(result[\"stages\"][\"warped\"], cv2.COLOR_BGR2RGB))\n",
    "    axes[row, 1].set_title(f\"Warped ({result['grid_size']}x{result['grid_size']})\", fontsize=9)\n",
    "    axes[row, 1].axis(\"off\")\n",
    "\n",
    "    # Letter matrix\n",
    "    gs = result[\"grid_size\"]\n",
    "    ax = axes[row, 2]\n",
    "    ax.set_xlim(-0.5, gs - 0.5)\n",
    "    ax.set_ylim(gs - 0.5, -0.5)\n",
    "    for i in range(gs):\n",
    "        for j in range(gs):\n",
    "            c = result[\"confidences\"][i][j]\n",
    "            color = \"green\" if c > 0.8 else \"orange\" if c > 0.5 else \"red\"\n",
    "            ax.text(\n",
    "                j, i, result[\"letters\"][i][j],\n",
    "                ha=\"center\", va=\"center\", fontsize=14, fontweight=\"bold\", color=color,\n",
    "            )\n",
    "    ax.set_title(f\"Mean conf: {result['mean_confidence']:.3f}\", fontsize=9)\n",
    "    ax.set_xticks(range(gs))\n",
    "    ax.set_yticks(range(gs))\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.suptitle(\"End-to-End Pipeline Results\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perspective Correction: Quantitative Before/After\n",
    "\n",
    "Compare CNN confidence and predictions with and without per-tile perspective correction across all 38 photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative comparison: with vs without perspective correction\n",
    "# Run on all photos, comparing per-tile confidence and predictions\n",
    "\n",
    "comparison_records = []\n",
    "\n",
    "for img_path in tqdm(raw_files, desc=\"Comparing with/without correction\"):\n",
    "    image = cv2.imread(str(img_path))\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    # Shared stages: detection → warp → tile extraction\n",
    "    mask, box, det_conf = detect_board(image, yolo_model)\n",
    "    if mask is None:\n",
    "        continue\n",
    "    clean = cleanup_mask(mask)\n",
    "    corners, _ = fit_quad(clean)\n",
    "    if corners is None:\n",
    "        continue\n",
    "    warped_img, _ = warp_board(image, corners)\n",
    "    tile_centroids, _ = find_tile_centers(warped_img)\n",
    "    if len(tile_centroids) < 4:\n",
    "        continue\n",
    "    gs, rows, cols, tsize = infer_grid_from_centroids(tile_centroids, warped_img.shape)\n",
    "    if gs == 0:\n",
    "        continue\n",
    "\n",
    "    raw_tiles = extract_tiles_from_grid(warped_img, rows, cols, tsize)\n",
    "\n",
    "    # Without correction\n",
    "    proc_raw = [preprocess_tile_v0(t) for t in raw_tiles]\n",
    "    letters_raw, confs_raw = predict_tiles_batch(cnn_model, proc_raw)\n",
    "\n",
    "    # With correction\n",
    "    corrected_tiles = [correct_tile_perspective(t) for t in raw_tiles]\n",
    "    proc_corr = [preprocess_tile_v0(t) for t in corrected_tiles]\n",
    "    letters_corr, confs_corr = predict_tiles_batch(cnn_model, proc_corr)\n",
    "\n",
    "    comparison_records.append({\n",
    "        \"filename\": img_path.name,\n",
    "        \"grid_size\": gs,\n",
    "        \"mean_conf_raw\": np.mean(confs_raw),\n",
    "        \"mean_conf_corr\": np.mean(confs_corr),\n",
    "        \"min_conf_raw\": np.min(confs_raw),\n",
    "        \"min_conf_corr\": np.min(confs_corr),\n",
    "        \"n_changed\": sum(a != b for a, b in zip(letters_raw, letters_corr)),\n",
    "        \"n_tiles\": len(raw_tiles),\n",
    "        \"n_low_raw\": sum(1 for c in confs_raw if c < 0.5),\n",
    "        \"n_low_corr\": sum(1 for c in confs_corr if c < 0.5),\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_records)\n",
    "\n",
    "# Summary stats\n",
    "print(\"=\" * 70)\n",
    "print(\"  PERSPECTIVE CORRECTION: BEFORE vs AFTER\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Boards evaluated:       {len(comp_df)}\")\n",
    "print(f\"  Mean conf (before):     {comp_df['mean_conf_raw'].mean():.4f}\")\n",
    "print(f\"  Mean conf (after):      {comp_df['mean_conf_corr'].mean():.4f}\")\n",
    "delta = comp_df['mean_conf_corr'].mean() - comp_df['mean_conf_raw'].mean()\n",
    "print(f\"  Delta:                  {delta:+.4f} ({'improved' if delta > 0 else 'worsened'})\")\n",
    "print(f\"  Min conf (before):      {comp_df['min_conf_raw'].mean():.4f}\")\n",
    "print(f\"  Min conf (after):       {comp_df['min_conf_corr'].mean():.4f}\")\n",
    "total_changed = comp_df['n_changed'].sum()\n",
    "total_tiles = comp_df['n_tiles'].sum()\n",
    "print(f\"  Predictions changed:    {total_changed}/{total_tiles} tiles ({100 * total_changed / total_tiles:.1f}%)\")\n",
    "print(f\"  Low-conf tiles before:  {comp_df['n_low_raw'].sum()}\")\n",
    "print(f\"  Low-conf tiles after:   {comp_df['n_low_corr'].sum()}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Per-board breakdown for boards with changes\n",
    "changed = comp_df[comp_df[\"n_changed\"] > 0].sort_values(\"n_changed\", ascending=False)\n",
    "if len(changed) > 0:\n",
    "    print(f\"\\nBoards with prediction changes ({len(changed)}):\")\n",
    "    for _, row in changed.iterrows():\n",
    "        delta_i = row[\"mean_conf_corr\"] - row[\"mean_conf_raw\"]\n",
    "        print(f\"  {row['filename']}: {row['n_changed']} tiles changed, \"\n",
    "              f\"conf {row['mean_conf_raw']:.3f} → {row['mean_conf_corr']:.3f} ({delta_i:+.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: before vs after confidence distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Paired bar chart: per-board mean confidence\n",
    "x = np.arange(len(comp_df))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width / 2, comp_df[\"mean_conf_raw\"], width, label=\"Without correction\", alpha=0.8)\n",
    "axes[0].bar(x + width / 2, comp_df[\"mean_conf_corr\"], width, label=\"With correction\", alpha=0.8)\n",
    "axes[0].set_xlabel(\"Board index\")\n",
    "axes[0].set_ylabel(\"Mean confidence\")\n",
    "axes[0].set_title(\"Per-Board Mean Confidence\")\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0.5, 1.0)\n",
    "\n",
    "# Scatter: before vs after (each point = one board)\n",
    "axes[1].scatter(comp_df[\"mean_conf_raw\"], comp_df[\"mean_conf_corr\"], s=40, alpha=0.7)\n",
    "lim = [min(comp_df[\"mean_conf_raw\"].min(), comp_df[\"mean_conf_corr\"].min()) - 0.02, 1.0]\n",
    "axes[1].plot(lim, lim, \"k--\", alpha=0.4, label=\"No change\")\n",
    "axes[1].set_xlabel(\"Mean conf (without correction)\")\n",
    "axes[1].set_ylabel(\"Mean conf (with correction)\")\n",
    "axes[1].set_title(\"Before vs After (above line = improved)\")\n",
    "axes[1].legend()\n",
    "axes[1].set_aspect(\"equal\")\n",
    "\n",
    "# Delta histogram\n",
    "deltas = comp_df[\"mean_conf_corr\"] - comp_df[\"mean_conf_raw\"]\n",
    "axes[2].hist(deltas, bins=20, edgecolor=\"black\", alpha=0.7)\n",
    "axes[2].axvline(0, color=\"black\", linestyle=\"--\", alpha=0.4)\n",
    "axes[2].axvline(deltas.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean: {deltas.mean():+.4f}\")\n",
    "axes[2].set_xlabel(\"Confidence delta (after - before)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_title(\"Distribution of Confidence Changes\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle(\"Per-Tile Perspective Correction Impact\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H. Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "batch_results = []\n",
    "for img_path in tqdm(raw_files, desc=\"Processing all boards\"):\n",
    "    result = analyze_board(img_path)\n",
    "    result[\"filename\"] = img_path.name\n",
    "    # Drop large intermediate data to save memory\n",
    "    if \"stages\" in result:\n",
    "        result[\"stages\"].pop(\"warped\", None)\n",
    "        result[\"stages\"].pop(\"tiles\", None)\n",
    "    batch_results.append(result)\n",
    "\n",
    "successes = [r for r in batch_results if \"error\" not in r]\n",
    "failures = [r for r in batch_results if \"error\" in r]\n",
    "print(f\"\\nSuccess: {len(successes)}/{len(batch_results)} ({100 * len(successes) / len(batch_results):.1f}%)\")\n",
    "print(f\"Failures: {len(failures)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_records = []\n",
    "for r in batch_results:\n",
    "    record = {\"filename\": r[\"filename\"]}\n",
    "    if \"error\" in r:\n",
    "        record[\"status\"] = \"FAIL\"\n",
    "        record[\"error\"] = r[\"error\"]\n",
    "        record[\"grid_size\"] = None\n",
    "        record[\"mean_conf\"] = None\n",
    "        record[\"min_conf\"] = None\n",
    "        record[\"det_conf\"] = r.get(\"stages\", {}).get(\"detection_conf\", None)\n",
    "    else:\n",
    "        record[\"status\"] = \"OK\"\n",
    "        record[\"error\"] = None\n",
    "        record[\"grid_size\"] = r[\"grid_size\"]\n",
    "        record[\"mean_conf\"] = r[\"mean_confidence\"]\n",
    "        record[\"min_conf\"] = r[\"min_confidence\"]\n",
    "        record[\"det_conf\"] = r[\"stages\"][\"detection_conf\"]\n",
    "    summary_records.append(record)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "display(summary_df.sort_values(\"mean_conf\", ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag problematic results\n",
    "if len(successes) > 0:\n",
    "    low_conf = [r for r in successes if r[\"mean_confidence\"] < 0.7]\n",
    "    very_low = [r for r in successes if r[\"min_confidence\"] < 0.3]\n",
    "\n",
    "    print(f\"Low mean confidence (<0.7): {len(low_conf)} boards\")\n",
    "    for r in low_conf:\n",
    "        print(f\"  {r['filename']}: mean={r['mean_confidence']:.3f}, min={r['min_confidence']:.3f}\")\n",
    "\n",
    "    print(f\"\\nVery low tile confidence (<0.3): {len(very_low)} boards\")\n",
    "    for r in very_low:\n",
    "        flat_confs = [c for row in r[\"confidences\"] for c in row]\n",
    "        flat_letters = [l for row in r[\"letters\"] for l in row]\n",
    "        bad_tiles = [(l, f\"{c:.2f}\") for l, c in zip(flat_letters, flat_confs) if c < 0.3]\n",
    "        print(f\"  {r['filename']}: {len(bad_tiles)} bad tiles: {bad_tiles}\")\n",
    "\n",
    "if failures:\n",
    "    print(f\"\\nFailures:\")\n",
    "    for r in failures:\n",
    "        print(f\"  {r['filename']}: {r['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(successes) > 0:\n",
    "    all_confs = [r[\"mean_confidence\"] for r in successes]\n",
    "    all_min_confs = [r[\"min_confidence\"] for r in successes]\n",
    "    grid_sizes = [r[\"grid_size\"] for r in successes]\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  BATCH EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Total images:        {len(batch_results)}\")\n",
    "    print(f\"  Successful:          {len(successes)} ({100 * len(successes) / len(batch_results):.1f}%)\")\n",
    "    print(f\"  Failed:              {len(failures)}\")\n",
    "    print(f\"  Mean confidence:     {np.mean(all_confs):.3f}\")\n",
    "    print(f\"  Median confidence:   {np.median(all_confs):.3f}\")\n",
    "    print(f\"  Min mean confidence: {np.min(all_confs):.3f}\")\n",
    "    print(f\"  Grid sizes:          {dict(zip(*np.unique(grid_sizes, return_counts=True)))}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.hist(all_confs, bins=15, edgecolor=\"black\", alpha=0.7)\n",
    "    ax.axvline(np.mean(all_confs), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(all_confs):.3f}\")\n",
    "    ax.set_xlabel(\"Mean CNN Confidence per Board\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Confidence Distribution Across All Boards\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. ONNX Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# YOLO ONNX export (writes next to the .pt file, i.e. prototyping/yolov8s-seg.onnx)\n",
    "yolo_onnx_path = yolo_model.export(\n",
    "    format=\"onnx\",\n",
    "    imgsz=YOLO_IMGSZ,\n",
    "    simplify=True,\n",
    "    dynamic=False,\n",
    "    half=False,\n",
    ")\n",
    "print(f\"YOLO ONNX exported to: {yolo_onnx_path}\")\n",
    "print(f\"YOLO ONNX size: {Path(yolo_onnx_path).stat().st_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 100, 100)\n",
    "cnn_onnx_path = MODELS_DIR / \"boggle_cnn.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    cnn_model,\n",
    "    dummy_input,\n",
    "    str(cnn_onnx_path),\n",
    "    input_names=[\"image\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"image\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
    "    opset_version=17,\n",
    ")\n",
    "\n",
    "onnx_model = onnx.load(str(cnn_onnx_path))\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "print(f\"CNN ONNX exported to: {cnn_onnx_path}\")\n",
    "print(f\"CNN ONNX size: {cnn_onnx_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(str(cnn_onnx_path))\n",
    "\n",
    "# Compare PyTorch vs ONNX on first 10 tiles\n",
    "n_test = min(10, len(tiles))\n",
    "test_tiles_proc = [preprocess_tile_v0(tiles[i]) for i in range(n_test)]\n",
    "\n",
    "print(\"PyTorch vs ONNX comparison:\")\n",
    "print(f\"{'Tile':>4} {'PT Letter':>10} {'PT Conf':>8} {'OX Letter':>10} {'OX Conf':>8} {'Match':>6}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "all_match = True\n",
    "for i, proc in enumerate(test_tiles_proc):\n",
    "    pt_letter, pt_conf = predict_tile(cnn_model, proc)\n",
    "\n",
    "    input_array = proc.astype(np.float32)[np.newaxis, np.newaxis, :, :]\n",
    "    onnx_logits = session.run(None, {\"image\": input_array})[0]\n",
    "    onnx_probs = np.exp(onnx_logits) / np.exp(onnx_logits).sum(axis=1, keepdims=True)\n",
    "    onnx_idx = onnx_probs.argmax(axis=1)[0]\n",
    "    onnx_conf = onnx_probs[0, onnx_idx]\n",
    "    onnx_letter = CLASS_LABELS[onnx_idx]\n",
    "\n",
    "    match = pt_letter == onnx_letter\n",
    "    if not match:\n",
    "        all_match = False\n",
    "\n",
    "    print(\n",
    "        f\"{i:>4} {pt_letter:>10} {pt_conf:>8.4f} {onnx_letter:>10} {onnx_conf:>8.4f} \"\n",
    "        f\"{'OK' if match else 'MISMATCH':>6}\"\n",
    "    )\n",
    "\n",
    "if all_match:\n",
    "    print(\"\\nAll predictions match between PyTorch and ONNX.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Some predictions differ between PyTorch and ONNX!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"  ONNX EXPORT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  YOLO model:  {yolo_onnx_path}\")\n",
    "print(f\"    Input:     (1, 3, {YOLO_IMGSZ}, {YOLO_IMGSZ}) float32\")\n",
    "print(f\"    Size:      {Path(yolo_onnx_path).stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "print(f\"  CNN model:   {cnn_onnx_path}\")\n",
    "print(f\"    Input:     (batch, 1, 100, 100) float32 [values 0-255]\")\n",
    "print(f\"    Output:    (batch, 32) logits\")\n",
    "print(f\"    Size:      {cnn_onnx_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print(f\"  Class labels: {len(CLASS_LABELS)} classes\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
