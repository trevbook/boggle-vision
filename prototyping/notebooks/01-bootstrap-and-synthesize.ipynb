{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Bootstrap Labels & Synthesize YOLO-Seg Dataset\n",
    "\n",
    "**Pipeline:** raw board photos → legacy OpenCV detection → geometric QA →\n",
    "YOLO-seg labels (real) → synthetic composites → YOLO-seg labels (synthetic) →\n",
    "final dataset with `data.yaml`\n",
    "\n",
    "**Inputs:**\n",
    "- `data/raw/` — 38 board photos (4000×3000 JPGs)\n",
    "- `data/backgrounds/` — 9 background textures (desk, fabric, wood)\n",
    "\n",
    "**Output:**\n",
    "- `data/yolo-seg-board/` — complete YOLO-seg dataset ready for `yolo segment train`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 67\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- Paths ---\n",
    "PROJECT_ROOT = Path.cwd().parent  # prototyping/\n",
    "DATA_DIR = PROJECT_ROOT / \"prototyping/data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "BG_DIR = DATA_DIR / \"backgrounds\"\n",
    "\n",
    "# Output dataset (YOLO format)\n",
    "DATASET_DIR = DATA_DIR / \"yolo-seg-board\"\n",
    "DATASET_IMAGES_TRAIN = DATASET_DIR / \"images\" / \"train\"\n",
    "DATASET_IMAGES_VAL = DATASET_DIR / \"images\" / \"val\"\n",
    "DATASET_LABELS_TRAIN = DATASET_DIR / \"labels\" / \"train\"\n",
    "DATASET_LABELS_VAL = DATASET_DIR / \"labels\" / \"val\"\n",
    "\n",
    "# --- Config ---\n",
    "RESIZE_HEIGHT = 1500  # matches parse_boggle_board() default\n",
    "SYNTH_CANVAS_SIZE = (1280, 1280)  # uniform canvas for synthetic images\n",
    "\n",
    "# Detection params (tuned values from parse_boggle_board, NOT the\n",
    "# detect_boggle_board_contour defaults — these work better on our photos)\n",
    "DETECTION_PARAMS = dict(\n",
    "    n_top_contours_to_consider=200,\n",
    "    min_board_area_threshold=0.15,\n",
    "    max_board_area_threshold=0.8,\n",
    "    board_contour_expansion_size=25,\n",
    "    polygon_approximation_epsilon=0.05,\n",
    "    binary_threshold_value=100,\n",
    ")\n",
    "\n",
    "print(f\"Raw photos:  {RAW_DIR} ({len(list(RAW_DIR.glob('*.jpg')))} files)\")\n",
    "print(f\"Backgrounds: {BG_DIR} ({len(list(BG_DIR.glob('*')))} files)\")\n",
    "print(f\"Output:      {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Board Detection Functions (inlined from legacy)\n",
    "\n",
    "Copied from `prototyping/legacy/board_detection.py` with broken imports removed.\n",
    "Only the board-contour-detection pipeline is needed — tile detection and OCR are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_greyscale(img):\n",
    "    \"\"\"Convert a BGR image to greyscale.\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "def apply_binary_thresholding(img, threshold=127):\n",
    "    \"\"\"Apply binary thresholding to a greyscale image.\"\"\"\n",
    "    _, thresholded_image = cv2.threshold(img, threshold, 255, cv2.THRESH_BINARY)\n",
    "    return thresholded_image\n",
    "\n",
    "\n",
    "def detect_contours(\n",
    "    img,\n",
    "    hierarchy_algorithm=cv2.RETR_TREE,\n",
    "    contour_approximation=cv2.CHAIN_APPROX_NONE,\n",
    "    apply_preprocessing=False,\n",
    "):\n",
    "    \"\"\"Detect contours in an image. Optionally preprocesses (greyscale + threshold).\"\"\"\n",
    "    if apply_preprocessing:\n",
    "        img = convert_to_greyscale(img)\n",
    "        img = apply_binary_thresholding(img)\n",
    "    contours, hierarchy = cv2.findContours(img, hierarchy_algorithm, contour_approximation)\n",
    "    return contours, hierarchy\n",
    "\n",
    "\n",
    "def hierarchy_to_dataframe(hierarchy):\n",
    "    \"\"\"Convert cv2.findContours hierarchy into a DataFrame with contour relationships.\"\"\"\n",
    "\n",
    "    def dfs(contour_idx, level, max_levels=4):\n",
    "        if max_levels is not None and level >= max_levels:\n",
    "            return\n",
    "        _, _, first_child, parent = hierarchy[0, contour_idx]\n",
    "        children = []\n",
    "        if first_child != -1:\n",
    "            child = first_child\n",
    "            while True:\n",
    "                children.append(child)\n",
    "                next_sibling, _, _, _ = hierarchy[0, child]\n",
    "                if next_sibling != -1:\n",
    "                    child = next_sibling\n",
    "                else:\n",
    "                    break\n",
    "        rows.append([\n",
    "            contour_idx,\n",
    "            level,\n",
    "            parent if parent != -1 else None,\n",
    "            children if children else None,\n",
    "        ])\n",
    "        for child in children:\n",
    "            dfs(child, level + 1, max_levels)\n",
    "\n",
    "    rows = []\n",
    "    root_contours = np.where(hierarchy[0, :, 3] == -1)[0]\n",
    "    for contour_idx in root_contours:\n",
    "        dfs(contour_idx, 0)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"contour_idx\", \"hierarchy_level\", \"parent\", \"children\"])\n",
    "    df[\"n_children\"] = df[\"children\"].apply(lambda x: len(x) if x else 0)\n",
    "    return df.drop_duplicates(subset=[\"contour_idx\"])\n",
    "\n",
    "\n",
    "def approximate_polygon_from_contour(contour, epsilon=0.05):\n",
    "    \"\"\"Approximate a polygon from a contour using cv2.approxPolyDP.\"\"\"\n",
    "    perimeter = cv2.arcLength(contour, True)\n",
    "    return cv2.approxPolyDP(contour, epsilon * perimeter, True)\n",
    "\n",
    "\n",
    "def expand_contour(input_image, contour, dilation_size=5):\n",
    "    \"\"\"Expand a contour by dilating its filled mask and re-extracting.\"\"\"\n",
    "    mask = np.zeros(input_image.shape[:2], dtype=np.uint8)\n",
    "    cv2.drawContours(mask, [contour], -1, (255), thickness=-1)\n",
    "    kernel = np.ones((dilation_size, dilation_size), np.uint8)\n",
    "    dilated_mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "    dilated_contours, _ = cv2.findContours(\n",
    "        dilated_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    dilated_contour = max(dilated_contours, key=cv2.contourArea)\n",
    "    dilated_contour = cv2.approxPolyDP(\n",
    "        dilated_contour, 0.02 * cv2.arcLength(dilated_contour, True), True\n",
    "    )\n",
    "    return dilated_contour\n",
    "\n",
    "\n",
    "def detect_boggle_board_contour(\n",
    "    input_img,\n",
    "    n_top_contours_to_consider=20,\n",
    "    min_board_area_threshold=0.2,\n",
    "    max_board_area_threshold=0.9,\n",
    "    board_contour_expansion_size=5,\n",
    "    polygon_approximation_epsilon=0.05,\n",
    "    binary_threshold_value=127,\n",
    "):\n",
    "    \"\"\"Detect the Boggle board contour in an image. Returns a polygon contour.\n",
    "\n",
    "    Raises Exception if no suitable quadrilateral contour is found.\n",
    "    \"\"\"\n",
    "    img_area = input_img.shape[0] * input_img.shape[1]\n",
    "    greyscale_img = convert_to_greyscale(input_img)\n",
    "    thresholded_img = apply_binary_thresholding(greyscale_img, threshold=binary_threshold_value)\n",
    "    contours, hierarchy = detect_contours(thresholded_img)\n",
    "    hierarchy_df = hierarchy_to_dataframe(hierarchy)\n",
    "    hierarchy_df[\"contour\"] = hierarchy_df[\"contour_idx\"].apply(\n",
    "        lambda idx: contours[idx] if idx >= 0 else None\n",
    "    )\n",
    "\n",
    "    contours_to_consider = hierarchy_df.sort_values(\"n_children\", ascending=False).head(\n",
    "        n_top_contours_to_consider\n",
    "    )\n",
    "    contours_to_consider[\"approx_polygon\"] = contours_to_consider[\"contour\"].apply(\n",
    "        lambda contour: approximate_polygon_from_contour(\n",
    "            contour, epsilon=polygon_approximation_epsilon\n",
    "        )\n",
    "    )\n",
    "    contours_to_consider[\"approx_polygon_n_sides\"] = contours_to_consider[\n",
    "        \"approx_polygon\"\n",
    "    ].apply(lambda x: len(x))\n",
    "    contours_to_consider = contours_to_consider.query(\"approx_polygon_n_sides == 4\").copy()\n",
    "\n",
    "    if len(contours_to_consider) == 0:\n",
    "        raise Exception(\"There were no square contours detected in the image.\")\n",
    "\n",
    "    contours_to_consider[\"approx_polygon_area\"] = contours_to_consider[\n",
    "        \"approx_polygon\"\n",
    "    ].apply(lambda x: cv2.contourArea(x))\n",
    "    contours_to_consider[\"approx_polygon_area_pct_of_image\"] = (\n",
    "        contours_to_consider[\"approx_polygon_area\"] / img_area\n",
    "    )\n",
    "\n",
    "    contours_to_consider = (\n",
    "        contours_to_consider.query(\n",
    "            \"approx_polygon_area_pct_of_image >= @min_board_area_threshold\"\n",
    "        )\n",
    "        .query(\"approx_polygon_area_pct_of_image <= @max_board_area_threshold\")\n",
    "        .copy()\n",
    "    )\n",
    "    if len(contours_to_consider) == 0:\n",
    "        raise Exception(\n",
    "            \"No contours were found that were within the specified area thresholds.\"\n",
    "        )\n",
    "\n",
    "    largest_contour_row = contours_to_consider.sort_values(\n",
    "        \"approx_polygon_area_pct_of_image\", ascending=False\n",
    "    ).iloc[0]\n",
    "    largest_contour_dict = largest_contour_row.to_dict()\n",
    "\n",
    "    largest_contour_dict[\"expanded_contour\"] = expand_contour(\n",
    "        input_img,\n",
    "        largest_contour_dict[\"contour\"],\n",
    "        dilation_size=board_contour_expansion_size,\n",
    "    )\n",
    "\n",
    "    return approximate_polygon_from_contour(\n",
    "        largest_contour_dict[\"expanded_contour\"], epsilon=polygon_approximation_epsilon\n",
    "    )\n",
    "\n",
    "\n",
    "def resize_image(image, desired_height):\n",
    "    \"\"\"Resize an image to a desired height, maintaining aspect ratio.\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    aspect_ratio = width / height\n",
    "    desired_width = int(desired_height * aspect_ratio)\n",
    "    return cv2.resize(image, (desired_width, desired_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "print(\"Legacy functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detect Board Contours in All Raw Photos\n",
    "\n",
    "Runs the legacy OpenCV pipeline on each photo. Catches exceptions for outright failures\n",
    "and applies geometric sanity checks to flag suspicious detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_sanity_checks(contour, img_shape):\n",
    "    \"\"\"Run geometric sanity checks on a detected contour.\n",
    "\n",
    "    Returns a list of issue strings (empty if all checks pass).\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    pts = contour.reshape(-1, 2).astype(float)\n",
    "    img_h, img_w = img_shape[:2]\n",
    "    img_area = img_h * img_w\n",
    "\n",
    "    # 1. Point count (should be 4 for a quadrilateral)\n",
    "    if len(pts) != 4:\n",
    "        issues.append(f\"n_points={len(pts)}\")\n",
    "\n",
    "    # 2. Aspect ratio (board is square — ratio should be 0.7–1.3)\n",
    "    _, (w, h), _ = cv2.minAreaRect(contour)\n",
    "    if max(w, h) > 0:\n",
    "        aspect_ratio = min(w, h) / max(w, h)\n",
    "        if aspect_ratio < 0.7:\n",
    "            issues.append(f\"aspect_ratio={aspect_ratio:.2f}\")\n",
    "\n",
    "    # 3. Convexity\n",
    "    if not cv2.isContourConvex(contour):\n",
    "        issues.append(\"non_convex\")\n",
    "\n",
    "    # 4. Area reasonableness (10–85% of image)\n",
    "    contour_area = cv2.contourArea(contour)\n",
    "    area_pct = contour_area / img_area\n",
    "    if not (0.10 <= area_pct <= 0.85):\n",
    "        issues.append(f\"area_pct={area_pct:.2f}\")\n",
    "\n",
    "    # 5. Interior angles (60–120 degrees for each corner)\n",
    "    if len(pts) >= 3:\n",
    "        for i in range(len(pts)):\n",
    "            v1 = pts[(i - 1) % len(pts)] - pts[i]\n",
    "            v2 = pts[(i + 1) % len(pts)] - pts[i]\n",
    "            denom = np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "            if denom < 1e-8:\n",
    "                issues.append(f\"degenerate_corner_{i}\")\n",
    "                continue\n",
    "            cos_angle = np.dot(v1, v2) / denom\n",
    "            angle_deg = np.degrees(np.arccos(np.clip(cos_angle, -1, 1)))\n",
    "            if not (60 <= angle_deg <= 120):\n",
    "                issues.append(f\"corner_{i}_angle={angle_deg:.1f}\")\n",
    "\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = sorted(f for f in RAW_DIR.iterdir() if f.suffix.lower() in (\".jpg\", \".jpeg\", \".png\"))\n",
    "print(f\"Found {len(raw_files)} raw images\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for fpath in tqdm(raw_files, desc=\"Detecting boards\"):\n",
    "    record = {\n",
    "        \"filename\": fpath.name,\n",
    "        \"filepath\": fpath,\n",
    "        \"status\": \"ok\",\n",
    "        \"contour\": None,\n",
    "        \"resized_img\": None,\n",
    "        \"issues\": [],\n",
    "    }\n",
    "\n",
    "    img = cv2.imread(str(fpath))\n",
    "    if img is None:\n",
    "        record[\"status\"] = \"load_failed\"\n",
    "        record[\"issues\"].append(\"cv2.imread returned None\")\n",
    "        results.append(record)\n",
    "        continue\n",
    "\n",
    "    img = resize_image(img, RESIZE_HEIGHT)\n",
    "    record[\"resized_img\"] = img\n",
    "\n",
    "    try:\n",
    "        contour = detect_boggle_board_contour(img, **DETECTION_PARAMS)\n",
    "    except Exception as e:\n",
    "        record[\"status\"] = \"detection_failed\"\n",
    "        record[\"issues\"].append(str(e))\n",
    "        results.append(record)\n",
    "        continue\n",
    "\n",
    "    record[\"contour\"] = contour\n",
    "\n",
    "    # Geometric sanity checks\n",
    "    issues = geometric_sanity_checks(contour, img.shape)\n",
    "    if issues:\n",
    "        record[\"status\"] = \"suspicious\"\n",
    "        record[\"issues\"] = issues\n",
    "\n",
    "    results.append(record)\n",
    "\n",
    "# Summary\n",
    "results_df = pd.DataFrame([{\n",
    "    \"filename\": r[\"filename\"],\n",
    "    \"status\": r[\"status\"],\n",
    "    \"n_points\": len(r[\"contour\"].reshape(-1, 2)) if r[\"contour\"] is not None else None,\n",
    "    \"issues\": \"; \".join(r[\"issues\"]) if r[\"issues\"] else \"\",\n",
    "} for r in results])\n",
    "\n",
    "print(\"\\n--- Detection Summary ---\")\n",
    "print(results_df[\"status\"].value_counts().to_string())\n",
    "print()\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visual QA — Review All Detections\n",
    "\n",
    "Grid of all images with detected contours overlaid.\n",
    "- **Green** = ok\n",
    "- **Orange** = suspicious (geometric check flagged)\n",
    "- **Red border** = detection failed\n",
    "\n",
    "Review the grid and add any bad filenames to `EXCLUDE_FILES` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATUS_COLORS = {\n",
    "    \"ok\": (0, 255, 0),\n",
    "    \"suspicious\": (0, 165, 255),\n",
    "    \"detection_failed\": (0, 0, 255),\n",
    "    \"load_failed\": (0, 0, 255),\n",
    "}\n",
    "\n",
    "n_images = len(results)\n",
    "n_cols = 6\n",
    "n_rows = math.ceil(n_images / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3.5 * n_cols, 3.5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, record in enumerate(results):\n",
    "    ax = axes[idx]\n",
    "    img = record[\"resized_img\"]\n",
    "\n",
    "    if img is None:\n",
    "        ax.text(0.5, 0.5, \"LOAD\\nFAILED\", ha=\"center\", va=\"center\", fontsize=12, color=\"red\")\n",
    "        ax.set_facecolor(\"black\")\n",
    "    else:\n",
    "        vis = img.copy()\n",
    "        if record[\"contour\"] is not None:\n",
    "            color = STATUS_COLORS[record[\"status\"]]\n",
    "            cv2.drawContours(vis, [record[\"contour\"]], -1, color, thickness=8)\n",
    "        vis_rgb = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)\n",
    "        # Downsample for display\n",
    "        display_h = 300\n",
    "        display_w = int(display_h * vis_rgb.shape[1] / vis_rgb.shape[0])\n",
    "        vis_small = cv2.resize(vis_rgb, (display_w, display_h))\n",
    "        ax.imshow(vis_small)\n",
    "\n",
    "    status = record[\"status\"]\n",
    "    title_color = {\"ok\": \"green\", \"suspicious\": \"orange\", \"detection_failed\": \"red\", \"load_failed\": \"red\"}[status]\n",
    "    title = f\"{record['filename'][:20]}\\n[{status}]\"\n",
    "    ax.set_title(title, fontsize=7, color=title_color)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "for idx in range(n_images, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusion List\n",
    "\n",
    "Review the grid above. Add filenames of any images with bad detections to the\n",
    "`EXCLUDE_FILES` set below. Detection failures are automatically excluded.\n",
    "Re-run from Section 5 onward after editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER EDIT: Add filenames to exclude from the dataset ---\n",
    "# Detection failures and suspicious detections are automatically excluded.\n",
    "# Add any \"ok\" images that look wrong upon visual inspection.\n",
    "EXCLUDE_FILES = {\n",
    "    \"20230809_201820(1).jpg\",\n",
    "    \"20230809_201820.jpg\",\n",
    "}\n",
    "\n",
    "# Automatically exclude detection failures and suspicious detections\n",
    "auto_excluded = {r[\"filename\"] for r in results if r[\"status\"] in (\"detection_failed\", \"load_failed\", \"suspicious\")}\n",
    "all_excluded = EXCLUDE_FILES | auto_excluded\n",
    "\n",
    "good_results = [r for r in results if r[\"filename\"] not in all_excluded and r[\"contour\"] is not None]\n",
    "\n",
    "print(f\"Total images:     {len(results)}\")\n",
    "print(f\"Auto-excluded:    {len(auto_excluded)} (detection failures + suspicious)\")\n",
    "print(f\"Manual-excluded:  {len(EXCLUDE_FILES)}\")\n",
    "print(f\"Good detections:  {len(good_results)}\")\n",
    "\n",
    "if auto_excluded:\n",
    "    print(\"\\nAuto-excluded files:\")\n",
    "    for f in sorted(auto_excluded):\n",
    "        print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate YOLO-Seg Labels for Real Photos\n",
    "\n",
    "For each good detection, normalize the polygon coordinates to `[0, 1]` and write\n",
    "a YOLO-seg format label file. Also save the resized images for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_to_yolo_seg_label(contour, img_height, img_width, class_id=0):\n",
    "    \"\"\"Convert an OpenCV contour to a YOLO-seg format label line.\n",
    "\n",
    "    YOLO-seg format: <class_id> <x1> <y1> <x2> <y2> ... (normalized 0-1)\n",
    "    \"\"\"\n",
    "    pts = contour.reshape(-1, 2).astype(float)\n",
    "    normalized = []\n",
    "    for x, y in pts:\n",
    "        normalized.append(f\"{x / img_width:.6f}\")\n",
    "        normalized.append(f\"{y / img_height:.6f}\")\n",
    "    return f\"{class_id} \" + \" \".join(normalized)\n",
    "\n",
    "\n",
    "def write_yolo_label(label_path, contour, img_h, img_w):\n",
    "    \"\"\"Write a single YOLO-seg label file.\"\"\"\n",
    "    label_line = contour_to_yolo_seg_label(contour, img_h, img_w, class_id=0)\n",
    "    label_path.write_text(label_line + \"\\n\")\n",
    "\n",
    "\n",
    "# Store real image/label pairs for later dataset assembly\n",
    "real_pairs = []  # list of {\"stem\": ..., \"img\": ..., \"contour\": ...}\n",
    "\n",
    "for r in good_results:\n",
    "    stem = Path(r[\"filename\"]).stem\n",
    "    real_pairs.append({\n",
    "        \"stem\": stem,\n",
    "        \"img\": r[\"resized_img\"],\n",
    "        \"contour\": r[\"contour\"],\n",
    "    })\n",
    "\n",
    "print(f\"Prepared {len(real_pairs)} real image/label pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Synthetic Composite Generation\n",
    "\n",
    "Cut out each detected board, paste onto varied backgrounds with randomized\n",
    "augmentation. Labels are computed from the known placement coordinates — they're free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_board_cutout(img, contour):\n",
    "    \"\"\"Extract board cutout with alpha channel from image using contour mask.\n",
    "\n",
    "    Returns:\n",
    "        bgra: Board region with transparent background (BGRA).\n",
    "        local_contour: Polygon in cropped coordinate system (Nx2 float).\n",
    "    \"\"\"\n",
    "    mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, [contour.reshape(-1, 1, 2)], 255)\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    cropped_img = img[y : y + h, x : x + w]\n",
    "    cropped_mask = mask[y : y + h, x : x + w]\n",
    "\n",
    "    bgra = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2BGRA)\n",
    "    bgra[:, :, 3] = cropped_mask\n",
    "\n",
    "    local_contour = contour.reshape(-1, 2).astype(float) - np.array([x, y], dtype=float)\n",
    "    return bgra, local_contour\n",
    "\n",
    "\n",
    "# Extract cutouts from all good detections\n",
    "cutouts = []\n",
    "for r in tqdm(good_results, desc=\"Extracting cutouts\"):\n",
    "    bgra, local_contour = extract_board_cutout(r[\"resized_img\"], r[\"contour\"])\n",
    "    cutouts.append({\n",
    "        \"filename\": r[\"filename\"],\n",
    "        \"bgra\": bgra,\n",
    "        \"local_contour\": local_contour,\n",
    "    })\n",
    "\n",
    "print(f\"Extracted {len(cutouts)} board cutouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load backgrounds\n",
    "bg_files = sorted(f for f in BG_DIR.iterdir() if f.suffix.lower() in (\".png\", \".jpg\", \".jpeg\"))\n",
    "backgrounds = []\n",
    "for bf in bg_files:\n",
    "    bg = cv2.imread(str(bf))\n",
    "    if bg is not None:\n",
    "        backgrounds.append({\"filename\": bf.name, \"img\": bg})\n",
    "\n",
    "print(f\"Loaded {len(backgrounds)} backgrounds:\")\n",
    "for b in backgrounds:\n",
    "    h, w = b[\"img\"].shape[:2]\n",
    "    print(f\"  {b['filename']}: {w}x{h}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_composite(\n",
    "    cutout_bgra,\n",
    "    local_contour,\n",
    "    bg_img,\n",
    "    canvas_size=(1280, 1280),\n",
    "    scale_range=(0.40, 0.80),\n",
    "    rotation_range=(-15.0, 15.0),\n",
    "    brightness_range=(0.7, 1.3),\n",
    "    contrast_range=(0.8, 1.2),\n",
    "    noise_sigma_max=15.0,\n",
    "    perspective_strength=0.05,\n",
    "    rng=None,\n",
    "):\n",
    "    \"\"\"Generate one synthetic composite image with computed polygon label.\n",
    "\n",
    "    Returns:\n",
    "        composite: BGR image of shape canvas_size.\n",
    "        polygon_pts: Nx2 float array of polygon points in canvas coordinates.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    canvas_h, canvas_w = canvas_size\n",
    "\n",
    "    # 1. Resize background to fill canvas\n",
    "    bg_resized = cv2.resize(bg_img, (canvas_w, canvas_h))\n",
    "\n",
    "    # 2. Scale the board cutout\n",
    "    scale = rng.uniform(*scale_range)\n",
    "    angle_deg = rng.uniform(*rotation_range)\n",
    "\n",
    "    cutout_h, cutout_w = cutout_bgra.shape[:2]\n",
    "    longest_edge = max(cutout_h, cutout_w)\n",
    "    target_longest = int(min(canvas_h, canvas_w) * scale)\n",
    "    resize_factor = target_longest / longest_edge\n",
    "    new_w = int(cutout_w * resize_factor)\n",
    "    new_h = int(cutout_h * resize_factor)\n",
    "\n",
    "    resized_cutout = cv2.resize(cutout_bgra, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    resized_contour = (local_contour * resize_factor).astype(np.float32)\n",
    "\n",
    "    # 3. Rotate\n",
    "    center = (new_w / 2, new_h / 2)\n",
    "    M_rot = cv2.getRotationMatrix2D(center, angle_deg, 1.0)\n",
    "\n",
    "    cos_a = abs(M_rot[0, 0])\n",
    "    sin_a = abs(M_rot[0, 1])\n",
    "    rot_w = int(new_h * sin_a + new_w * cos_a)\n",
    "    rot_h = int(new_h * cos_a + new_w * sin_a)\n",
    "    M_rot[0, 2] += (rot_w - new_w) / 2\n",
    "    M_rot[1, 2] += (rot_h - new_h) / 2\n",
    "\n",
    "    rotated_cutout = cv2.warpAffine(\n",
    "        resized_cutout, M_rot, (rot_w, rot_h),\n",
    "        flags=cv2.INTER_LINEAR,\n",
    "        borderMode=cv2.BORDER_CONSTANT,\n",
    "        borderValue=(0, 0, 0, 0),\n",
    "    )\n",
    "\n",
    "    # Transform contour through rotation\n",
    "    ones = np.ones((len(resized_contour), 1), dtype=np.float32)\n",
    "    pts_hom = np.hstack([resized_contour, ones])\n",
    "    rotated_contour = (M_rot @ pts_hom.T).T\n",
    "\n",
    "    # 4. Optional mild perspective distortion\n",
    "    if perspective_strength > 0:\n",
    "        src_pts = np.array(\n",
    "            [[0, 0], [rot_w, 0], [rot_w, rot_h], [0, rot_h]], dtype=np.float32\n",
    "        )\n",
    "        jitter = (\n",
    "            rng.uniform(-perspective_strength, perspective_strength, size=(4, 2))\n",
    "            * np.array([rot_w, rot_h])\n",
    "        )\n",
    "        dst_pts = (src_pts + jitter).astype(np.float32)\n",
    "        M_persp = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "        rotated_cutout = cv2.warpPerspective(\n",
    "            rotated_cutout, M_persp, (rot_w, rot_h),\n",
    "            borderMode=cv2.BORDER_CONSTANT,\n",
    "            borderValue=(0, 0, 0, 0),\n",
    "        )\n",
    "        pts_h = np.hstack(\n",
    "            [rotated_contour, np.ones((len(rotated_contour), 1))]\n",
    "        ).astype(np.float32)\n",
    "        transformed = (M_persp @ pts_h.T).T\n",
    "        rotated_contour = transformed[:, :2] / transformed[:, 2:3]\n",
    "\n",
    "    # 5. Random placement within canvas\n",
    "    max_x = canvas_w - rot_w\n",
    "    max_y = canvas_h - rot_h\n",
    "    if max_x < 0 or max_y < 0:\n",
    "        # Cutout too big — shrink to fit\n",
    "        shrink = min(canvas_w / rot_w, canvas_h / rot_h) * 0.9\n",
    "        rot_w_new = int(rot_w * shrink)\n",
    "        rot_h_new = int(rot_h * shrink)\n",
    "        rotated_cutout = cv2.resize(rotated_cutout, (rot_w_new, rot_h_new))\n",
    "        rotated_contour = rotated_contour * shrink\n",
    "        rot_w, rot_h = rot_w_new, rot_h_new\n",
    "        max_x = canvas_w - rot_w\n",
    "        max_y = canvas_h - rot_h\n",
    "\n",
    "    offset_x = rng.integers(0, max(max_x, 1))\n",
    "    offset_y = rng.integers(0, max(max_y, 1))\n",
    "\n",
    "    # 6. Alpha-blend onto background\n",
    "    composite = bg_resized.copy()\n",
    "    alpha = rotated_cutout[:, :, 3:4] / 255.0\n",
    "    rgb = rotated_cutout[:, :, :3]\n",
    "\n",
    "    roi = composite[offset_y : offset_y + rot_h, offset_x : offset_x + rot_w]\n",
    "    blended = (rgb * alpha + roi * (1 - alpha)).astype(np.uint8)\n",
    "    composite[offset_y : offset_y + rot_h, offset_x : offset_x + rot_w] = blended\n",
    "\n",
    "    canvas_contour = rotated_contour + np.array([offset_x, offset_y])\n",
    "\n",
    "    # 7. Photometric augmentation (applied to entire composite)\n",
    "    brightness = rng.uniform(*brightness_range)\n",
    "    contrast = rng.uniform(*contrast_range)\n",
    "    composite = np.clip(\n",
    "        composite.astype(np.float32) * contrast + (brightness - 1.0) * 127, 0, 255\n",
    "    ).astype(np.uint8)\n",
    "\n",
    "    # Gaussian noise\n",
    "    sigma = rng.uniform(0, noise_sigma_max)\n",
    "    if sigma > 0:\n",
    "        noise = rng.normal(0, sigma, composite.shape).astype(np.float32)\n",
    "        composite = np.clip(composite.astype(np.float32) + noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Slight per-channel color shift\n",
    "    for c in range(3):\n",
    "        shift = rng.uniform(-10, 10)\n",
    "        composite[:, :, c] = np.clip(\n",
    "            composite[:, :, c].astype(np.float32) + shift, 0, 255\n",
    "        ).astype(np.uint8)\n",
    "\n",
    "    return composite, canvas_contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate synthetic composites ---\n",
    "# Phase 1: one composite per (cutout, background) pair\n",
    "# Phase 2: fill to target with random re-pairings\n",
    "\n",
    "SYNTH_TARGET_COUNT = len(cutouts) * len(backgrounds)  # one per pair\n",
    "\n",
    "synthetic_records = []\n",
    "synth_idx = 0\n",
    "\n",
    "# Phase 1\n",
    "for cutout in tqdm(cutouts, desc=\"Generating synthetics\"):\n",
    "    for bg in backgrounds:\n",
    "        rng = np.random.default_rng(SEED + synth_idx)\n",
    "        composite, canvas_contour = generate_synthetic_composite(\n",
    "            cutout[\"bgra\"],\n",
    "            cutout[\"local_contour\"],\n",
    "            bg[\"img\"],\n",
    "            canvas_size=SYNTH_CANVAS_SIZE,\n",
    "            rng=rng,\n",
    "        )\n",
    "        synthetic_records.append({\n",
    "            \"synth_idx\": synth_idx,\n",
    "            \"source_file\": cutout[\"filename\"],\n",
    "            \"bg_file\": bg[\"filename\"],\n",
    "            \"composite\": composite,\n",
    "            \"contour\": canvas_contour,\n",
    "        })\n",
    "        synth_idx += 1\n",
    "\n",
    "# Phase 2: additional random pairings if we want more\n",
    "SYNTH_EXTRA_COUNT = 50  # extra images on top of base combos\n",
    "while len(synthetic_records) < SYNTH_TARGET_COUNT + SYNTH_EXTRA_COUNT:\n",
    "    rng = np.random.default_rng(SEED + synth_idx)\n",
    "    cutout = cutouts[rng.integers(len(cutouts))]\n",
    "    bg = backgrounds[rng.integers(len(backgrounds))]\n",
    "    composite, canvas_contour = generate_synthetic_composite(\n",
    "        cutout[\"bgra\"],\n",
    "        cutout[\"local_contour\"],\n",
    "        bg[\"img\"],\n",
    "        canvas_size=SYNTH_CANVAS_SIZE,\n",
    "        rng=rng,\n",
    "    )\n",
    "    synthetic_records.append({\n",
    "        \"synth_idx\": synth_idx,\n",
    "        \"source_file\": cutout[\"filename\"],\n",
    "        \"bg_file\": bg[\"filename\"],\n",
    "        \"composite\": composite,\n",
    "        \"contour\": canvas_contour,\n",
    "    })\n",
    "    synth_idx += 1\n",
    "\n",
    "print(f\"Generated {len(synthetic_records)} synthetic composites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual QA: sample of synthetic composites with polygon overlays\n",
    "n_sample = min(18, len(synthetic_records))\n",
    "sample_indices = random.sample(range(len(synthetic_records)), n_sample)\n",
    "\n",
    "n_cols_synth = 6\n",
    "n_rows_synth = math.ceil(n_sample / n_cols_synth)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows_synth, n_cols_synth, figsize=(4.5 * n_cols_synth, 4.5 * n_rows_synth))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax_idx, si in enumerate(sample_indices):\n",
    "    rec = synthetic_records[si]\n",
    "    vis = rec[\"composite\"].copy()\n",
    "    pts = rec[\"contour\"].reshape(-1, 1, 2).astype(np.int32)\n",
    "    cv2.polylines(vis, [pts], isClosed=True, color=(0, 255, 0), thickness=3)\n",
    "    axes[ax_idx].imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "    axes[ax_idx].set_title(f\"synth_{rec['synth_idx']:04d} | {rec['bg_file'][:12]}\", fontsize=8)\n",
    "    axes[ax_idx].axis(\"off\")\n",
    "\n",
    "for idx in range(n_sample, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"Synthetic Composite QA Sample\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Assemble Final YOLO-Seg Dataset\n",
    "\n",
    "Create the YOLO directory structure, perform train/val split, write all images\n",
    "and labels, and generate `data.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and create directory structure\n",
    "if DATASET_DIR.exists():\n",
    "    shutil.rmtree(DATASET_DIR)\n",
    "\n",
    "for d in [DATASET_IMAGES_TRAIN, DATASET_IMAGES_VAL, DATASET_LABELS_TRAIN, DATASET_LABELS_VAL]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Train/Val Split ---\n",
    "# Real images: 80/20 random split\n",
    "real_indices = list(range(len(real_pairs)))\n",
    "random.shuffle(real_indices)\n",
    "n_real_val = max(1, int(len(real_indices) * 0.2))\n",
    "real_val_indices = set(real_indices[:n_real_val])\n",
    "\n",
    "# Synthetic images: 80/20 random split\n",
    "synth_indices = list(range(len(synthetic_records)))\n",
    "random.shuffle(synth_indices)\n",
    "n_synth_val = max(1, int(len(synth_indices) * 0.2))\n",
    "synth_val_indices = set(synth_indices[:n_synth_val])\n",
    "\n",
    "n_real_train = len(real_pairs) - n_real_val\n",
    "n_synth_train = len(synthetic_records) - n_synth_val\n",
    "\n",
    "print(f\"Real:      {n_real_train} train / {n_real_val} val\")\n",
    "print(f\"Synthetic: {n_synth_train} train / {n_synth_val} val\")\n",
    "print(f\"Total:     {n_real_train + n_synth_train} train / {n_real_val + n_synth_val} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write real images and labels\n",
    "for idx, pair in enumerate(tqdm(real_pairs, desc=\"Writing real images\")):\n",
    "    split = \"val\" if idx in real_val_indices else \"train\"\n",
    "    img_dst = DATASET_DIR / \"images\" / split / f\"real_{pair['stem']}.jpg\"\n",
    "    lbl_dst = DATASET_DIR / \"labels\" / split / f\"real_{pair['stem']}.txt\"\n",
    "\n",
    "    h, w = pair[\"img\"].shape[:2]\n",
    "    cv2.imwrite(str(img_dst), pair[\"img\"])\n",
    "    write_yolo_label(lbl_dst, pair[\"contour\"], h, w)\n",
    "\n",
    "# Write synthetic images and labels\n",
    "for idx, rec in enumerate(tqdm(synthetic_records, desc=\"Writing synthetic images\")):\n",
    "    split = \"val\" if idx in synth_val_indices else \"train\"\n",
    "    name = f\"synth_{rec['synth_idx']:04d}\"\n",
    "    img_dst = DATASET_DIR / \"images\" / split / f\"{name}.jpg\"\n",
    "    lbl_dst = DATASET_DIR / \"labels\" / split / f\"{name}.txt\"\n",
    "\n",
    "    h, w = rec[\"composite\"].shape[:2]\n",
    "    cv2.imwrite(str(img_dst), rec[\"composite\"])\n",
    "    write_yolo_label(lbl_dst, rec[\"contour\"], h, w)\n",
    "\n",
    "print(\"Done writing images and labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data.yaml\n",
    "data_yaml = {\n",
    "    \"path\": str(DATASET_DIR.resolve()),\n",
    "    \"train\": \"images/train\",\n",
    "    \"val\": \"images/val\",\n",
    "    \"names\": {\n",
    "        0: \"board\",\n",
    "    },\n",
    "}\n",
    "\n",
    "yaml_path = DATASET_DIR / \"data.yaml\"\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(data_yaml, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"Wrote {yaml_path}\")\n",
    "print()\n",
    "print(yaml_path.read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Verification\n",
    "\n",
    "Spot-check random images from the written dataset by loading them back and rendering\n",
    "the labels. Verify the data.yaml is parseable by Ultralytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_visualize_yolo_label(img_path, label_path, ax):\n",
    "    \"\"\"Load an image and its YOLO-seg label, draw the polygon.\"\"\"\n",
    "    img = cv2.imread(str(img_path))\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    label_text = label_path.read_text().strip()\n",
    "    parts = label_text.split()\n",
    "    coords = [float(x) for x in parts[1:]]\n",
    "\n",
    "    # Denormalize\n",
    "    pts = []\n",
    "    for i in range(0, len(coords), 2):\n",
    "        px = coords[i] * w\n",
    "        py = coords[i + 1] * h\n",
    "        pts.append([px, py])\n",
    "    pts = np.array(pts, dtype=np.int32)\n",
    "\n",
    "    vis = img.copy()\n",
    "    cv2.polylines(vis, [pts.reshape(-1, 1, 2)], isClosed=True, color=(0, 255, 0), thickness=3)\n",
    "\n",
    "    # Also fill with semi-transparent overlay\n",
    "    overlay = vis.copy()\n",
    "    cv2.fillPoly(overlay, [pts.reshape(-1, 1, 2)], (0, 255, 0))\n",
    "    vis = cv2.addWeighted(overlay, 0.15, vis, 0.85, 0)\n",
    "\n",
    "    ax.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(img_path.name, fontsize=7)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "# Sample 3 from train, 3 from val\n",
    "train_imgs = sorted(DATASET_IMAGES_TRAIN.glob(\"*.jpg\"))\n",
    "val_imgs = sorted(DATASET_IMAGES_VAL.glob(\"*.jpg\"))\n",
    "\n",
    "sample = (\n",
    "    random.sample(train_imgs, min(3, len(train_imgs)))\n",
    "    + random.sample(val_imgs, min(3, len(val_imgs)))\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "for ax, img_path in zip(axes.flatten(), sample):\n",
    "    label_path = (\n",
    "        img_path.parent.parent.parent / \"labels\" / img_path.parent.name / (img_path.stem + \".txt\")\n",
    "    )\n",
    "    load_and_visualize_yolo_label(img_path, label_path, ax)\n",
    "\n",
    "plt.suptitle(\"Final Dataset Spot Check (green = label polygon)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics and integrity check\n",
    "train_count = len(list(DATASET_IMAGES_TRAIN.glob(\"*.jpg\")))\n",
    "val_count = len(list(DATASET_IMAGES_VAL.glob(\"*.jpg\")))\n",
    "train_labels = len(list(DATASET_LABELS_TRAIN.glob(\"*.txt\")))\n",
    "val_labels = len(list(DATASET_LABELS_VAL.glob(\"*.txt\")))\n",
    "\n",
    "print(\"=== YOLO-Seg Dataset Summary ===\")\n",
    "print(f\"Train images:  {train_count}\")\n",
    "print(f\"Train labels:  {train_labels}\")\n",
    "print(f\"Val images:    {val_count}\")\n",
    "print(f\"Val labels:    {val_labels}\")\n",
    "print(f\"Total:         {train_count + val_count}\")\n",
    "print(f\"data.yaml:     {DATASET_DIR / 'data.yaml'}\")\n",
    "print()\n",
    "\n",
    "# Integrity assertions\n",
    "assert train_count == train_labels, f\"Train image/label mismatch: {train_count} vs {train_labels}\"\n",
    "assert val_count == val_labels, f\"Val image/label mismatch: {val_count} vs {val_labels}\"\n",
    "assert train_count > 0, \"No training images!\"\n",
    "assert val_count > 0, \"No validation images!\"\n",
    "\n",
    "# Verify every image has a matching label and vice versa\n",
    "for split in [\"train\", \"val\"]:\n",
    "    img_stems = {p.stem for p in (DATASET_DIR / \"images\" / split).glob(\"*.jpg\")}\n",
    "    lbl_stems = {p.stem for p in (DATASET_DIR / \"labels\" / split).glob(\"*.txt\")}\n",
    "    assert img_stems == lbl_stems, f\"{split}: image/label stem mismatch\"\n",
    "\n",
    "print(\"All integrity checks passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Ultralytics can parse the dataset\n",
    "from ultralytics.data.utils import check_det_dataset\n",
    "\n",
    "dataset_info = check_det_dataset(str(DATASET_DIR / \"data.yaml\"))\n",
    "print(\"Ultralytics dataset check passed!\")\n",
    "print(f\"  Train: {dataset_info['train']}\")\n",
    "print(f\"  Val:   {dataset_info['val']}\")\n",
    "print(f\"  Names: {dataset_info['names']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
