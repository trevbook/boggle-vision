{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "After playing around with a couple of off-the-shelf OCR engines, I decided to try my hand at creating my own model. I want something that's lightweight and accurate.\n",
    "\n",
    "In this notebook, I'm going to try my hand at training a convolutional neural network to detect Boggle letters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "The cells below will set up the rest of the notebook.\n",
    "\n",
    "I'll start by configuring the kernel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\boggle-vision\\boggle-vision-prototyping\n"
     ]
    }
   ],
   "source": [
    "# Change the directory to the root of the repo \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the kernel is configured, I'm going to load in some modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Pytorch-related import statements\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Importing custom modules from this repo\n",
    "from utils.settings import allowed_boggle_tiles\n",
    "from utils.cnn import BoggleCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "I'm going to start by creating a custom `Dataset` for my network, and by specifying a `DataLoader`. Since it's been a bit, I'm going to follow [the Pytorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) for this.\n",
    "\n",
    "I'll start with the `Dataset`, which I'll call a `BoggleTileImageDataset`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoggleTileImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is a custom Dataset for handling the Boggle tile images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_directory):\n",
    "        \"\"\"\n",
    "        This is the initialization method. Here, I'll set some class variables\n",
    "        for the image directory and the image paths.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the img_dir class variable\n",
    "        self.img_dir = image_directory\n",
    "\n",
    "        # Create a mapping of letters to integers\n",
    "        self.letter_to_int_dict = {\n",
    "            letter: idx for idx, letter in enumerate(allowed_boggle_tiles)\n",
    "        }\n",
    "\n",
    "        # Create a class variable mapping each of the image paths to their labels\n",
    "        self.img_path_to_label_dict = {}\n",
    "        for child_file in Path(image_directory).iterdir():\n",
    "            if child_file.suffix == \".png\":\n",
    "                cur_file_letter = child_file.name.split(\"_\")[0]\n",
    "                self.img_path_to_label_dict[child_file] = cur_file_letter\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This method will return the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.img_path_to_label_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This method will return the image and the label for a given index.\n",
    "        \"\"\"\n",
    "        img_path = list(self.img_path_to_label_dict.keys())[idx]\n",
    "        img = read_image(str(img_path)).float()\n",
    "        label = self.img_path_to_label_dict[img_path]\n",
    "        label_int = self.letter_to_int_dict[label]\n",
    "        label = torch.tensor(label_int)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** After some experimentation, I decided that I ought to save raw `.npy` files instead of `.png` files. The following is a modified version of the `BoggleTileImageDataset` that loads these files instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoggleTileImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is a custom Dataset for handling the Boggle tile images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_directory):\n",
    "        \"\"\"\n",
    "        This is the initialization method. Here, I'll set some class variables\n",
    "        for the image directory and the image paths.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the img_dir class variable\n",
    "        self.img_dir = image_directory\n",
    "\n",
    "        # Create a mapping of letters to integers\n",
    "        self.letter_to_int_dict = {\n",
    "            letter: idx for idx, letter in enumerate(allowed_boggle_tiles)\n",
    "        }\n",
    "\n",
    "        # Create a class variable mapping each of the image paths to their labels\n",
    "        self.img_path_to_label_dict = {}\n",
    "        for child_file in Path(image_directory).iterdir():\n",
    "            if child_file.suffix == \".npy\":\n",
    "                cur_file_letter = child_file.name.split(\"_\")[0]\n",
    "                self.img_path_to_label_dict[child_file] = cur_file_letter\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This method will return the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.img_path_to_label_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This method will return the image and the label for a given index.\n",
    "        \"\"\"\n",
    "        img_path = list(self.img_path_to_label_dict.keys())[idx]\n",
    "        img = torch.tensor(np.load(img_path)).float()\n",
    "        label = self.img_path_to_label_dict[img_path]\n",
    "        label_int = self.letter_to_int_dict[label]\n",
    "        label = torch.tensor(label_int)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this custom `Dataset` in hand, I'm going to create an instance of it (as well as an accompanying `DataLoader`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the full dataset\n",
    "full_dataset = BoggleTileImageDataset(\"data/training-data-npy/\")\n",
    "# original_image_dataset = BoggleTileImageDataset(\"data/original-training-data\")\n",
    "\n",
    "# Calculate lengths for each split\n",
    "train_length = int(0.85 * len(full_dataset))\n",
    "val_length = int(0.1 * len(full_dataset))\n",
    "test_length = len(full_dataset) - train_length - val_length\n",
    "\n",
    "# Split the dataset\n",
    "train_data, val_data, test_data = random_split(\n",
    "    full_dataset, [train_length, val_length, test_length]\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "# original_image_loader = DataLoader(original_image_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MISC:** Below, I can show one of the images from the training data, as well as its corresponding label:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([256, 100, 100])\n",
      "Labels batch shape: torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 18 (T)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label} ({allowed_boggle_tiles[label]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to define a loss function and optimizer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "Next, I'm going to run through the training loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:26<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 1.5753009537700564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [04:10<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.011037888740299552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:03<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 0.010621043305828998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = BoggleCNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0025)\n",
    "\n",
    "# Iterate through the data\n",
    "epoch_amt = 3\n",
    "for epoch in range(epoch_amt):\n",
    "    # We're going to keep track of the loss for each epoch\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Iterate through the data\n",
    "    for data, target in tqdm(train_loader):\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.unsqueeze(1)  # Adds a channel dimension\n",
    "\n",
    "        # Make predictions\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add the loss to the epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Print the epoch loss\n",
    "    print(f\"Epoch {epoch + 1} loss: {epoch_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Loop\n",
    "\n",
    "Now that we've got a trained model, we ought to test it on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "incorrect_labels = []\n",
    "\n",
    "# Iterate through the test set\n",
    "for data, target in tqdm(test_loader):\n",
    "    \n",
    "    data = data.unsqueeze(1)  # Adds a channel dimension\n",
    "    # Make predictions\n",
    "    output = model(data)\n",
    "\n",
    "    # Get the predicted class\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "    # Add the number of correct predictions to the total correct\n",
    "    total += target.size(0)\n",
    "    correct += (predicted == target).sum().item()\n",
    "\n",
    "    # Collect the true labels of the incorrectly predicted images\n",
    "    for idx, label in enumerate(target):\n",
    "        if predicted[idx] != label:\n",
    "            incorrect_labels.append(allowed_boggle_tiles[label])\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on the test set: {correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model\n",
    "\n",
    "Now that we've created a model, we ought to save it to disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path to save the model\n",
    "model_folder = Path(\"models\")\n",
    "model_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), model_folder / \"boggle_cnn.pth\")\n",
    "\n",
    "app_model_folder = Path(\"boggle-vision-app/boggle-vision-api/models\")\n",
    "app_model_folder.mkdir(exist_ok=True, parents=True)\n",
    "torch.save(model.state_dict(), app_model_folder / \"boggle_cnn.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Model to GCP Cloud Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
