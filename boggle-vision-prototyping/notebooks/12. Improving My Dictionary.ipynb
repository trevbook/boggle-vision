{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "In Notebook 11, I used [a GitHub repo that contained different words and their definitions](https://github.com/ssvivian/WebstersDictionary). Issue here: a ton of these words were obscure, and their definitions weren't that great. \n",
    "\n",
    "So, instead, I'm going to try and put together my own dictionary - mostly based on [Princeton's WordNet](https://wordnet.princeton.edu/). I found [a repo that contains all of the words in a more parsable JSON format](https://github.com/fluhus/wordnet-to-json). In addition, I found [a library called word-forms](https://pypi.org/project/word-forms/) that tries to generate some of the different inflections of that word. \n",
    "\n",
    "In this notebook, I'm going to try and create a more robust dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help to set up the rest of my notebook. \n",
    "\n",
    "I'll start by configuring my kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\boggle-vision\\boggle-vision-prototyping\n"
     ]
    }
   ],
   "source": [
    "# Changing the cwd to the root of the project\n",
    "%cd ..\n",
    "\n",
    "# Enabling the autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm going to load in the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from word_forms.word_forms import get_word_forms\n",
    "from Levenshtein import ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the WordNet Files\n",
    "I'm going to kick things off by parsing all of the WordNet JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:03<00:00,  7.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Collect a list of smaller DataFrames that we'll merge together eventually\n",
    "wordnet_df_list = []\n",
    "\n",
    "# Iterate through each of the JSON files in the data/wordnet-json-files directory\n",
    "for child_file in tqdm(list(Path(\"data\\wordnet-json-files\").iterdir())):\n",
    "    if child_file.suffix == \".json\":\n",
    "        with open(child_file, \"r\") as f:\n",
    "            cur_file_data = json.load(f)\n",
    "\n",
    "        # Create a DataFrame from the data in the JSON file and store it\n",
    "        wordnet_df_list.append(\n",
    "            pd.DataFrame.from_records([val for key, val in cur_file_data.items()])\n",
    "        )\n",
    "\n",
    "# Concatenate all of the DataFrames together into a single DataFrame\n",
    "wordnet_df = pd.concat(wordnet_df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want a dictionary mapping each word to the first definition associated with each of the possible parts of speech: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_first_word_for_pos(meanings_list):\n",
    "    \n",
    "    # Iterate through each of the meanings, and extract the first one corresponding to each pos tag\n",
    "    first_meaning_for_pos = {}\n",
    "    for meaning in meanings_list:\n",
    "        if meaning[\"part_of_speech\"] not in first_meaning_for_pos:\n",
    "            first_meaning_for_pos[meaning[\"part_of_speech\"]] = meaning[\"def\"]\n",
    "            \n",
    "    # Return the resulting dictionary\n",
    "    return first_meaning_for_pos\n",
    "\n",
    "# Create a mapping between word --> possible pos tags\n",
    "word_to_pos_and_def_mapping = {row.word: generate_first_word_for_pos(row.meanings) for row in wordnet_df.itertuples()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: we're going to clean up this `wordnet_df` a bit by doing the following: \n",
    "\n",
    "- Extract the first part of speech & definition from the `meanings` column\n",
    "- Remove any word that has non-alphabet characters (i.e., compound words, words with apostrophes, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy that we'll modify\n",
    "cleaned_wordnet_df = wordnet_df.copy()\n",
    "\n",
    "# Add the \"pos\" and \"def\" columns\n",
    "cleaned_wordnet_df[\"pos\"] = cleaned_wordnet_df[\"meanings\"].apply(lambda x: x[0]['part_of_speech'])\n",
    "cleaned_wordnet_df[\"definition\"] = cleaned_wordnet_df[\"meanings\"].apply(lambda x: x[0]['def'])\n",
    "\n",
    "# Drop the \"meanings\" column\n",
    "cleaned_wordnet_df = cleaned_wordnet_df.drop(columns=[\"meanings\", \"pos_order\"])\n",
    "\n",
    "# Filter out words that aren't entirely made with alphabetic characters\n",
    "cleaned_wordnet_df = cleaned_wordnet_df[\n",
    "    cleaned_wordnet_df[\"word\"].apply(\n",
    "        lambda x: not any([not char.isalpha() for char in x])\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Generation\n",
    "Now, I'm going to use the `word-forms` library to try and find different forms of all of the words in the `cleaned_wordnet_df`. \n",
    "\n",
    "I'll start by trying to \"expand\" all of the words that're already in the `cleaned_wordnet_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78925/78925 [00:12<00:00, 6129.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# We're going to store new word additions in a dictionary\n",
    "new_word_forms = {}\n",
    "\n",
    "# We'll create a set containing all of the words in the dictionary; this can\n",
    "# be used for easily checking if a word is already in the dictionary\n",
    "initial_word_set = set(cleaned_wordnet_df[\"word\"])\n",
    "\n",
    "# We're going to map the pos tags to their full names\n",
    "pos_mapping = {\n",
    "    \"n\": \"noun\",\n",
    "    \"v\": \"verb\",\n",
    "    \"a\": \"adjective\",\n",
    "    \"r\": \"adverb\",\n",
    "}\n",
    "\n",
    "# Iterate through each of the rows in the DataFrame and determine the extra forms of the word\n",
    "for row in tqdm(list(cleaned_wordnet_df.itertuples())):\n",
    "    cur_word_forms = get_word_forms(row.word)\n",
    "\n",
    "    # Iterate through all of the forms, and check if they ought to be added to the dictionary\n",
    "    for pos, form_set in cur_word_forms.items():\n",
    "        for word_form in form_set:\n",
    "            if word_form not in initial_word_set:\n",
    "                # We're only going to add it if the pos matches the pos of the original word\n",
    "                if pos_mapping[pos] in word_to_pos_and_def_mapping[row.word]:\n",
    "                    \n",
    "                    # Extract the definition of the original word's pos\n",
    "                    cur_word_def = word_to_pos_and_def_mapping[row.word][pos_mapping[pos]]\n",
    "                    \n",
    "                    # If this word isn't already in the dictionary, we'll add it\n",
    "                    if word_form not in new_word_forms:\n",
    "                        new_word_forms[word_form] = {\n",
    "                            \"word\": word_form,\n",
    "                            \"pos\": pos,\n",
    "                            \"definition\": cur_word_def,\n",
    "                            \"linked_word\": row.word,\n",
    "                        }\n",
    "\n",
    "                    # If it's already in there, we'll check which linked word is closer, and\n",
    "                    # then update the linked word if the new one is closer\n",
    "                    else:\n",
    "                        cur_linked_word = new_word_forms[word_form][\"linked_word\"]\n",
    "                        if ratio(row.word, word_form) > ratio(\n",
    "                            cur_linked_word, word_form\n",
    "                        ):\n",
    "                            new_word_forms[word_form][\"linked_word\"] = row.word\n",
    "                            new_word_forms[word_form][\"definition\"] = cur_word_def\n",
    "\n",
    "# Create an \"expanded\" wordnet_df that contains all of the new words as well as the old ones\n",
    "expanded_wordnet_df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame.from_records(list(new_word_forms.values())),\n",
    "        pd.DataFrame.from_records(\n",
    "            [\n",
    "                {\n",
    "                    \"word\": row.word,\n",
    "                    \"pos\": row.pos,\n",
    "                    \"definition\": row.definition,\n",
    "                    \"linked_word\": None,\n",
    "                }\n",
    "                for row in cleaned_wordnet_df.itertuples()\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a Dictionary\n",
    "Now that we've created the `expanded_wordnet_df`, we can save it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/word_to_definition.json\", \"w\") as json_file:\n",
    "    json.dump({\n",
    "        row.word: {\n",
    "            \"pos\": row.pos,\n",
    "            \"definition\": row.definition,\n",
    "        }\n",
    "        for row in expanded_wordnet_df.itertuples()\n",
    "    }, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
