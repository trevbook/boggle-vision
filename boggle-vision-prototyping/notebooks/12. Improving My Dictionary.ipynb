{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "In Notebook 11, I used [a GitHub repo that contained different words and their definitions](https://github.com/ssvivian/WebstersDictionary). Issue here: a ton of these words were obscure, and their definitions weren't that great. \n",
    "\n",
    "So, instead, I'm going to try and put together my own dictionary - mostly based on [Princeton's WordNet](https://wordnet.princeton.edu/). I found [a repo that contains all of the words in a more parsable JSON format](https://github.com/fluhus/wordnet-to-json). In addition, I found [a library called word-forms](https://pypi.org/project/word-forms/) that tries to generate some of the different inflections of that word. \n",
    "\n",
    "In this notebook, I'm going to try and create a more robust dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will help to set up the rest of my notebook. \n",
    "\n",
    "I'll start by configuring my kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\boggle-vision\\boggle-vision-prototyping\n"
     ]
    }
   ],
   "source": [
    "# Changing the cwd to the root of the project\n",
    "%cd ..\n",
    "\n",
    "# Enabling the autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm going to load in the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from word_forms.word_forms import get_word_forms\n",
    "from Levenshtein import ratio\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from time import sleep\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the WordNet Files\n",
    "I'm going to kick things off by parsing all of the WordNet JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:04<00:00,  6.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Collect a list of smaller DataFrames that we'll merge together eventually\n",
    "wordnet_df_list = []\n",
    "\n",
    "# Iterate through each of the JSON files in the data/wordnet-json-files directory\n",
    "for child_file in tqdm(list(Path(\"data\\wordnet-json-files\").iterdir())):\n",
    "    if child_file.suffix == \".json\":\n",
    "        with open(child_file, \"r\") as f:\n",
    "            cur_file_data = json.load(f)\n",
    "\n",
    "        # Create a DataFrame from the data in the JSON file and store it\n",
    "        wordnet_df_list.append(\n",
    "            pd.DataFrame.from_records([val for key, val in cur_file_data.items()])\n",
    "        )\n",
    "\n",
    "# Concatenate all of the DataFrames together into a single DataFrame\n",
    "wordnet_df = pd.concat(wordnet_df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want a dictionary mapping each word to the first definition associated with each of the possible parts of speech: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_first_word_for_pos(meanings_list):\n",
    "    \n",
    "    # Iterate through each of the meanings, and extract the first one corresponding to each pos tag\n",
    "    first_meaning_for_pos = {}\n",
    "    for meaning in meanings_list:\n",
    "        if meaning[\"part_of_speech\"] not in first_meaning_for_pos:\n",
    "            first_meaning_for_pos[meaning[\"part_of_speech\"]] = meaning[\"def\"]\n",
    "            \n",
    "    # Return the resulting dictionary\n",
    "    return first_meaning_for_pos\n",
    "\n",
    "# Create a mapping between word --> possible pos tags\n",
    "word_to_pos_and_def_mapping = {row.word: generate_first_word_for_pos(row.meanings) for row in wordnet_df.itertuples()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: we're going to clean up this `wordnet_df` a bit by doing the following: \n",
    "\n",
    "- Extract the first part of speech & definition from the `meanings` column\n",
    "- Remove any word that has non-alphabet characters (i.e., compound words, words with apostrophes, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy that we'll modify\n",
    "cleaned_wordnet_df = wordnet_df.copy()\n",
    "\n",
    "# Add the \"pos\" and \"def\" columns\n",
    "cleaned_wordnet_df[\"pos\"] = cleaned_wordnet_df[\"meanings\"].apply(lambda x: x[0]['part_of_speech'])\n",
    "cleaned_wordnet_df[\"definition\"] = cleaned_wordnet_df[\"meanings\"].apply(lambda x: x[0]['def'])\n",
    "\n",
    "# Drop the \"meanings\" column\n",
    "cleaned_wordnet_df = cleaned_wordnet_df.drop(columns=[\"meanings\", \"pos_order\"])\n",
    "\n",
    "# Filter out words that aren't entirely made with alphabetic characters\n",
    "cleaned_wordnet_df = cleaned_wordnet_df[\n",
    "    cleaned_wordnet_df[\"word\"].apply(\n",
    "        lambda x: not any([not char.isalpha() for char in x])\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Generation\n",
    "Now, I'm going to use the `word-forms` library to try and find different forms of all of the words in the `cleaned_wordnet_df`. \n",
    "\n",
    "I'll start by trying to \"expand\" all of the words that're already in the `cleaned_wordnet_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78925/78925 [00:16<00:00, 4685.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# We're going to store new word additions in a dictionary\n",
    "new_word_forms = {}\n",
    "\n",
    "# We'll create a set containing all of the words in the dictionary; this can\n",
    "# be used for easily checking if a word is already in the dictionary\n",
    "initial_word_set = set(cleaned_wordnet_df[\"word\"])\n",
    "\n",
    "# We're going to map the pos tags to their full names\n",
    "pos_mapping = {\n",
    "    \"n\": \"noun\",\n",
    "    \"v\": \"verb\",\n",
    "    \"a\": \"adjective\",\n",
    "    \"r\": \"adverb\",\n",
    "}\n",
    "\n",
    "# Iterate through each of the rows in the DataFrame and determine the extra forms of the word\n",
    "for row in tqdm(list(cleaned_wordnet_df.itertuples())):\n",
    "    cur_word_forms = get_word_forms(row.word)\n",
    "\n",
    "    # Iterate through all of the forms, and check if they ought to be added to the dictionary\n",
    "    for pos, form_set in cur_word_forms.items():\n",
    "        for word_form in form_set:\n",
    "            if word_form not in initial_word_set:\n",
    "                # We're only going to add it if the pos matches the pos of the original word\n",
    "                if pos_mapping[pos] in word_to_pos_and_def_mapping[row.word]:\n",
    "                    \n",
    "                    # Extract the definition of the original word's pos\n",
    "                    cur_word_def = word_to_pos_and_def_mapping[row.word][pos_mapping[pos]]\n",
    "                    \n",
    "                    # If this word isn't already in the dictionary, we'll add it\n",
    "                    if word_form not in new_word_forms:\n",
    "                        new_word_forms[word_form] = {\n",
    "                            \"word\": word_form,\n",
    "                            \"pos\": pos,\n",
    "                            \"definition\": cur_word_def,\n",
    "                            \"linked_word\": row.word,\n",
    "                        }\n",
    "\n",
    "                    # If it's already in there, we'll check which linked word is closer, and\n",
    "                    # then update the linked word if the new one is closer\n",
    "                    else:\n",
    "                        cur_linked_word = new_word_forms[word_form][\"linked_word\"]\n",
    "                        if ratio(row.word, word_form) > ratio(\n",
    "                            cur_linked_word, word_form\n",
    "                        ):\n",
    "                            new_word_forms[word_form][\"linked_word\"] = row.word\n",
    "                            new_word_forms[word_form][\"definition\"] = cur_word_def\n",
    "\n",
    "# Create an \"expanded\" wordnet_df that contains all of the new words as well as the old ones\n",
    "expanded_wordnet_df = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame.from_records(list(new_word_forms.values())),\n",
    "        pd.DataFrame.from_records(\n",
    "            [\n",
    "                {\n",
    "                    \"word\": row.word,\n",
    "                    \"pos\": row.pos,\n",
    "                    \"definition\": row.definition,\n",
    "                    \"linked_word\": None,\n",
    "                }\n",
    "                for row in cleaned_wordnet_df.itertuples()\n",
    "            ]\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Words from the Scrabble Dictionary\n",
    "I also have [a `scrabble-dictionary` file that I got from this GitHub repo.](https://github.com/benjamincrom/scrabble/blob/master/scrabble/dictionary.json) I'm going to add some words from that dictionary! \n",
    "\n",
    "The one issue: these don't have definitions. In order to overcome that, I'm going to try and use [the DataMuse API](https://www.datamuse.com/api/) to find some definitions! \n",
    "\n",
    "I'll start by loading in the Scrabble dictionary, and determining which words need to be defined: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary into a DataFrame\n",
    "with open(\"data/scrabble-dictionary.json\", \"r\") as json_file:\n",
    "    scrabble_words_list = json.load(json_file)\n",
    "scrabble_df = pd.DataFrame.from_records(\n",
    "    [{\"word\": word} for word in scrabble_words_list]\n",
    ")\n",
    "\n",
    "# Determine which words aren't in the expanded wordnet_df\n",
    "only_in_scrabble_df = (\n",
    "    scrabble_df.merge(expanded_wordnet_df, on=\"word\", how=\"left\", indicator=True)\n",
    "    .query('_merge == \"left_only\"')\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# We won't look for words that're less than 4 characters long\n",
    "only_in_scrabble_df = only_in_scrabble_df[\n",
    "    only_in_scrabble_df[\"word\"].apply(lambda x: len(x) >= 4)\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'll use the DataMuse API to try and define each of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_datamuse_for_definition(word):\n",
    "    \"\"\"\n",
    "    This helper method will query the DataMuse API in order to\n",
    "    obtain the definition of a word.\n",
    "    \"\"\"\n",
    "    # Query the DataMuse API for the word\n",
    "    response = requests.get(f\"https://api.datamuse.com/words?sp={word}&md=d&max=1\")\n",
    "    \n",
    "    # Sleep for a random amount of time between 0.5 and 1.5 seconds to avoid rate limiting\n",
    "    sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "    # We'll wrap this in a try/except block in case the response is invalid\n",
    "    try:\n",
    "        # If the response was successful, we'll return the definition\n",
    "        if response.status_code == 200:\n",
    "            # If there are no definitions, we'll return None\n",
    "            if len(response.json()) == 0 or \"defs\" not in response.json()[0]:\n",
    "                return None\n",
    "\n",
    "            # Otherwise, we'll return the first definition\n",
    "            return response.json()[0][\"defs\"][0].split(\"\\t\")[1]\n",
    "\n",
    "        # Otherwise, we'll return None\n",
    "        else:\n",
    "            return None\n",
    "    # If we run into any errors, we'll return None\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined this method, we should be able to try and produce definitions for all of the words in the Scrabble dictionary that aren't in the expanded dictionary. We'll do this in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73317/73317 [2:02:12<00:00, 10.00it/s]  \n"
     ]
    }
   ],
   "source": [
    "word_to_future = {}\n",
    "word_to_def = {}\n",
    "with ThreadPoolExecutor(max_workers=12) as executor:\n",
    "    # Add all of the futures\n",
    "    for word in list(only_in_scrabble_df[\"word\"]):\n",
    "        word_to_future[word] = executor.submit(query_datamuse_for_definition, word)\n",
    "\n",
    "    # Iterate through each of the futures and get the result\n",
    "    for word, future in tqdm(word_to_future.items()):\n",
    "        word_to_def[word] = future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the results in hand, I want to save them immediately. It took 2 hours to run this for ~73,000 Scrabble words that weren't in my `expanded_wordnet_df`, so this is a data pull I don't want to run again 😅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the dictionary\n",
    "datamuse_df = pd.DataFrame.from_records(\n",
    "    [\n",
    "        {\"word\": word, \"definition\": definition}\n",
    "        for word, definition in word_to_def.items()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Remove any words that don't have a definition\n",
    "datamuse_df.dropna(inplace=True)\n",
    "\n",
    "# Save the results to a JSON file\n",
    "datamuse_df.to_json(\n",
    "    \"data/datamuse-definitions-of-scrabble-words.json\", orient=\"records\", indent=2\n",
    ")\n",
    "\n",
    "# For right now, we'll add all None values for the \"pos\" and \"linked_word\" columns,\n",
    "# since these are eventually dropped when the results are finally saved\n",
    "datamuse_df[\"pos\"] = None\n",
    "datamuse_df[\"linked_word\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I've saved this, I can merge it with the `expanded_wordnet_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames together\n",
    "further_expanded_wordnet_df = pd.concat([expanded_wordnet_df, datamuse_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a Dictionary\n",
    "Now that we've created the `expanded_wordnet_df`, we can save it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/word_to_definition.json\", \"w\") as json_file:\n",
    "    json.dump(\n",
    "        {\n",
    "            row.word: {\n",
    "                \"pos\": row.pos,\n",
    "                \"definition\": row.definition,\n",
    "            }\n",
    "            for row in further_expanded_wordnet_df[[\"word\", \"definition\", \"pos\"]]\n",
    "            .drop_duplicates(subset=[\"word\"])\n",
    "            .itertuples()\n",
    "        },\n",
    "        json_file,\n",
    "        indent=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
