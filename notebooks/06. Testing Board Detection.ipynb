{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "Now that I've got a preliminary method to detect Boggle tiles (from **`03. Finalizing Board Detection`**), I want to try and test it on the data that I've got. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of this notebook. \n",
    "\n",
    "First, I'll configure the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\boggle-vision\n"
     ]
    }
   ],
   "source": [
    "# Change directories to the root of the project\n",
    "%cd ..\n",
    "\n",
    "# Enable autoreload of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll import some relevant libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import statements\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from statistics import mode\n",
    "import utils\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "import traceback\n",
    "\n",
    "# Importing custom modules\n",
    "import utils.board_detection as board_detect\n",
    "from utils.cnn import BoggleCNN, EnhancedBoggleCNN\n",
    "from utils.settings import allowed_boggle_tiles\n",
    "\n",
    "# # Set up an EasyOCR reader\n",
    "# import easyocr\n",
    "# reader = easyocr.Reader(['en'], gpu=False)\n",
    "\n",
    "# Set up the model\n",
    "net = EnhancedBoggleCNN()\n",
    "net.load_state_dict(torch.load(\"models/boggle_cnn.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Here, I'm going to load in all of the pictures, as well as some information about each of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the .csv file containing the labeled boards\n",
    "board_data_df = pd.read_csv(\"data/labeled-boards.csv\")\n",
    "\n",
    "# Add a column which is the parsed letter sequence\n",
    "board_data_df[\"parsed_letter_sequence\"] = board_data_df[\"letter_sequence\"].apply(\n",
    "    lambda letter_list: letter_list.split(\";\")\n",
    ")\n",
    "\n",
    "# Load all of the images using cv2\n",
    "file_path_to_image = {}\n",
    "for row in board_data_df.itertuples():\n",
    "    file_path_to_image[row.file_path] = cv2.imread(row.file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Boards\n",
    "Below, I'm going to run each of the boards through a \"parsing\" method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:22<00:00,  1.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# We'll collect some results about the board data here\n",
    "all_parsed_boards_df_records = []\n",
    "\n",
    "# Iterate through all of the rows in the board data\n",
    "for row in tqdm(list(board_data_df.query(\"difficulty == 'easy'\").itertuples())):\n",
    "    \n",
    "    # Try and parse the board\n",
    "    error_msg = None\n",
    "    try:\n",
    "        parsed_board_df = board_detect.parse_boggle_board(\n",
    "            file_path_to_image[row.file_path],\n",
    "            max_image_height=1200,\n",
    "            # easyocr_reader=reader,\n",
    "            model=net\n",
    "        )\n",
    "\n",
    "        letter_sequence = list(parsed_board_df[\"letter\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        letter_sequence = None\n",
    "\n",
    "    # Add some information to the all_parsed_boards_df_records\n",
    "    all_parsed_boards_df_records.append(\n",
    "        {\n",
    "            \"file_path\": row.file_path,\n",
    "            \"letter_sequence\": letter_sequence,\n",
    "            \"error_msg\": error_msg,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Parse the results into a dataframe\n",
    "all_parsed_boards_df = pd.DataFrame(all_parsed_boards_df_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Results\n",
    "Now that I've got the boards parsed, I want to spend some time validating the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DataFrames together\n",
    "results_to_validate_df = board_data_df.merge(\n",
    "    all_parsed_boards_df.rename(\n",
    "        columns={\"letter_sequence\": \"predicted_letter_sequence\"}\n",
    "    ),\n",
    "    on=\"file_path\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "\n",
    "def match_letter_sequence(sequence_one, sequence_two):\n",
    "    \"\"\"\n",
    "    This function will compare two letter sequences and return a list of\n",
    "    booleans indicating whether the letters match or not. Each index of the\n",
    "    list will correspond to a letter in the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    # If either of the sequences are empty or None, return None\n",
    "    if not sequence_one or not sequence_two:\n",
    "        return None\n",
    "\n",
    "    # If the sequences are not the same length, return None\n",
    "    if len(sequence_one) != len(sequence_two):\n",
    "        return None\n",
    "\n",
    "    # We'll store the results in a list\n",
    "    results = []\n",
    "\n",
    "    # Iterate through all of the letters in the sequence\n",
    "    for index, letter in enumerate(sequence_one):\n",
    "        other_letter = sequence_two[index]\n",
    "        results.append(letter == other_letter)\n",
    "\n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "\n",
    "results_to_validate_df[\"letter_sequence_match\"] = results_to_validate_df.apply(\n",
    "    lambda row: match_letter_sequence(\n",
    "        row[\"parsed_letter_sequence\"], row[\"predicted_letter_sequence\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Add a column indicating the percent of letters that match\n",
    "results_to_validate_df[\"percent_match\"] = results_to_validate_df.apply(\n",
    "    lambda row: sum(row[\"letter_sequence_match\"]) / len(row[\"letter_sequence_match\"])\n",
    "    if row[\"letter_sequence_match\"]\n",
    "    else None,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Add a column indicating which letters don't match\n",
    "results_to_validate_df[\"errors\"] = results_to_validate_df.apply(\n",
    "    lambda row: [\n",
    "        {\n",
    "            \"actual\": row.parsed_letter_sequence[letter_idx],\n",
    "            \"predicted\": row.predicted_letter_sequence[letter_idx],\n",
    "            \"idx\": letter_idx\n",
    "        }\n",
    "        for letter_idx, letter_match in enumerate(row.letter_sequence_match)\n",
    "        if not letter_match\n",
    "    ]\n",
    "    if row.letter_sequence_match\n",
    "    else None,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "results_to_validate_df[\"correct\"] = results_to_validate_df.apply(\n",
    "    lambda row: [\n",
    "        {\n",
    "            \"actual\": row.parsed_letter_sequence[letter_idx],\n",
    "            \"predicted\": row.predicted_letter_sequence[letter_idx],\n",
    "            \"idx\": letter_idx\n",
    "        }\n",
    "        for letter_idx, letter_match in enumerate(row.letter_sequence_match)\n",
    "        if letter_match\n",
    "    ] \n",
    "    if row.letter_sequence_match\n",
    "    else None,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Add a column indicating how many errors there are \n",
    "results_to_validate_df[\"num_errors\"] = results_to_validate_df.apply(\n",
    "    lambda row: len(row.errors) if row.errors else None,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average percent match: 0.8064814814814816\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average percent match: {results_to_validate_df.percent_match.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_path                                       data\\test-pictures\\easy-01.png\n",
       "difficulty                                                                easy\n",
       "letter_sequence              Y;G;R;L;H;N;E;T;T;N;T;O;Th;F;E;E;E;N;C;L;E;T;H...\n",
       "parsed_letter_sequence       [Y, G, R, L, H, N, E, T, T, N, T, O, Th, F, E,...\n",
       "predicted_letter_sequence    [T, G, R, L, H, N, L, Y, T, N, T, O, Th, F, E,...\n",
       "error_msg                                                                 None\n",
       "letter_sequence_match        [False, True, True, True, True, True, False, F...\n",
       "percent_match                                                         0.833333\n",
       "errors                       [{'actual': 'Y', 'predicted': 'T', 'idx': 0}, ...\n",
       "correct                      [{'actual': 'G', 'predicted': 'G', 'idx': 1}, ...\n",
       "num_errors                                                                   6\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_to_validate_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'actual': 'Y', 'predicted': 'T', 'idx': 0},\n",
       " {'actual': 'E', 'predicted': 'L', 'idx': 6},\n",
       " {'actual': 'T', 'predicted': 'Y', 'idx': 7},\n",
       " {'actual': 'C', 'predicted': 'U', 'idx': 18},\n",
       " {'actual': 'R', 'predicted': 'H', 'idx': 25},\n",
       " {'actual': 'L', 'predicted': 'U', 'idx': 29}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_to_validate_df.iloc[0].errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
