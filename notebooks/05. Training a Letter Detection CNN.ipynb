{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "After playing around with a couple of off-the-shelf OCR engines, I decided to try my hand at creating my own model. I want something that's lightweight and accurate.\n",
    "\n",
    "In this notebook, I'm going to try my hand at training a convolutional neural network to detect Boggle letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook. \n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\boggle-vision\n"
     ]
    }
   ],
   "source": [
    "# Change the directory to the root of the repo \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the kernel is configured, I'm going to load in some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch-related import statements\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Importing custom modules from this repo\n",
    "from utils.settings import allowed_boggle_tiles\n",
    "from utils.cnn import BoggleCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "I'm going to start by creating a custom `Dataset` for my network, and by specifying a `DataLoader`. Since it's been a bit, I'm going to follow [the Pytorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) for this.\n",
    "\n",
    "I'll start with the `Dataset`, which I'll call a `BoggleTileImageDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoggleTileImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is a custom Dataset for handling the Boggle tile images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_directory):\n",
    "        \"\"\"\n",
    "        This is the initialization method. Here, I'll set some class variables\n",
    "        for the image directory and the image paths.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the img_dir class variable\n",
    "        self.img_dir = image_directory\n",
    "        \n",
    "        # Create a mapping of letters to integers\n",
    "        self.letter_to_int_dict = {letter: idx for idx, letter in enumerate(allowed_boggle_tiles)}\n",
    "\n",
    "        # Create a class variable mapping each of the image paths to their labels\n",
    "        self.img_path_to_label_dict = {}\n",
    "        for child_file in Path(image_directory).iterdir():\n",
    "            if child_file.suffix == \".png\":\n",
    "                cur_file_letter = child_file.name.split(\"_\")[0]\n",
    "                self.img_path_to_label_dict[child_file] = cur_file_letter\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This method will return the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.img_path_to_label_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This method will return the image and the label for a given index.\n",
    "        \"\"\"\n",
    "        img_path = list(self.img_path_to_label_dict.keys())[idx]\n",
    "        img = read_image(str(img_path)).float()\n",
    "        label = self.img_path_to_label_dict[img_path]\n",
    "        label_int = self.letter_to_int_dict[label]\n",
    "        label = torch.tensor(label_int)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this custom `Dataset` in hand, I'm going to create an instance of it (as well as an accompanying `DataLoader`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the full dataset\n",
    "full_dataset = BoggleTileImageDataset(\"data/training-data\")\n",
    "original_image_dataset = BoggleTileImageDataset(\"data/original-training-data\")\n",
    "\n",
    "# Calculate lengths for each split\n",
    "train_length = int(0.93 * len(full_dataset))\n",
    "val_length = int(0.05 * len(full_dataset))\n",
    "test_length = len(full_dataset) - train_length - val_length\n",
    "\n",
    "# Split the dataset\n",
    "train_data, val_data, test_data = random_split(full_dataset, [train_length, val_length, test_length])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "original_image_loader = DataLoader(original_image_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MISC:** Below, I can show one of the images from the training data, as well as its corresponding label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([256, 1, 100, 100])\n",
      "Labels batch shape: torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa9UlEQVR4nO3df2xV9f3H8VdL29tC21spci+VFisxqwoO5EctmG350kgciTCI06RsVcwMUpDSbQhzYDKHZTMq6FDEbMxtIrOJP0mmIUWakJVfdaCIK2ySUIVbRrT3IkjB3s/3D7MzDz+Etpe+b8vzkXySe849vX1zFnnu3Hu5N8U55wQAQA9LtR4AAHB5IkAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATlyxAq1at0tVXX63MzEyVlpZq+/btl+pXAQB6oZRL8Vlwf/3rX/XjH/9Yq1evVmlpqVasWKG6ujo1Nzdr8ODB3/iz8Xhchw4dUk5OjlJSUhI9GgDgEnPO6dixYyooKFBq6jdc57hLYPz48a6qqsrb7ujocAUFBa62tvaCP9vS0uIksVgsFquXr5aWlm/8+z7hT8GdOnVKTU1NKi8v9/alpqaqvLxcjY2NZx3f3t6uWCzmLceHcwNAn5CTk/ON9yc8QEePHlVHR4dCoZBvfygUUiQSOev42tpaBYNBbxUVFSV6JACAgQu9jGL+LrjFixcrGo16q6WlxXokAEAPSEv0Aw4aNEj9+vVTa2urb39ra6vC4fBZxwcCAQUCgUSPAQBIcgm/AsrIyNCYMWNUX1/v7YvH46qvr1dZWVmifx0AoJdK+BWQJNXU1KiyslJjx47V+PHjtWLFCh0/flz33HPPpfh1AIBe6JIE6M4779R//vMfLV26VJFIRKNGjdJbb7111hsTAACXr0vyD1G7IxaLKRgMWo8BAOimaDSq3Nzc895v/i44AMDliQABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjoVIBqa2s1btw45eTkaPDgwZo2bZqam5t9x5w8eVJVVVXKz89Xdna2ZsyYodbW1oQODQDo/ToVoIaGBlVVVWnr1q3auHGjTp8+rVtvvVXHjx/3jlmwYIHefPNN1dXVqaGhQYcOHdL06dMTPjgAoJdz3XDkyBEnyTU0NDjnnGtra3Pp6emurq7OO+bDDz90klxjY+M5H+PkyZMuGo16q6WlxUlisVgsVi9f0Wj0GxvSrdeAotGoJGngwIGSpKamJp0+fVrl5eXeMSUlJSoqKlJjY+M5H6O2tlbBYNBbhYWF3RkJANBLdDlA8Xhc1dXVmjhxokaMGCFJikQiysjIUF5enu/YUCikSCRyzsdZvHixotGot1paWro6EgCgF0nr6g9WVVVpz5492rJlS7cGCAQCCgQC3XoMAEDv06UroLlz52rDhg165513NHToUG9/OBzWqVOn1NbW5ju+tbVV4XC4W4MCAPqWTgXIOae5c+fq1Vdf1aZNm1RcXOy7f8yYMUpPT1d9fb23r7m5WQcPHlRZWVliJgYA9AmdegquqqpK69at0+uvv66cnBzvdZ1gMKisrCwFg0Hde++9qqmp0cCBA5Wbm6t58+aprKxMN9988yX5AwDJ4sorr/RtP/3000aTJLeZM2f6tr/88kujSWCtUwF69tlnJUnf+973fPvXrl2ru+++W5L05JNPKjU1VTNmzFB7e7smT56sZ555JiHDAgD6jk4FyDl3wWMyMzO1atUqrVq1qstDAQD6Pj4LDgBgostvwwYuRk5Ojnf7scceM5zk0vv6n1WS7rzzTqNJklssFvNt//SnP/VuHzt2rKfHgSGugAAAJggQAMAEAQIAmEhxF/PWth4Ui8UUDAatx0AXDRo0yLe9dOlS7/a8efN6ehz0AqFQyLt95MgRw0mQaNFoVLm5uee9nysgAIAJAgQAMMHbsNEtQ4YM8W3X1NT4tnnaDcD5cAUEADBBgAAAJggQAMAErwGhWwoLC33bP/vZz4wmAdDbcAUEADBBgAAAJggQAMAErwEBMDV16lTv9vr163338fUMfRtXQAAAEwQIAGCCAAEATPAaEABTa9as8W43NDT47uM1oL6NKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmEizHgC9286dO33bo0aN8m3v2rWr54YB0KtwBQQAMEGAAAAmeAoO3RKPx33bH3zwgW/729/+tnd79+7dPTLTmZYsWeLbXrt2rckcPeHxxx/3bd95551GkwAXxhUQAMAEAQIAmCBAAAATvAaEhPryyy9923v37vVujxgxoqfHkSQdPnzYt/3pp5+azNETjh8/bj0CcNG4AgIAmCBAAAATBAgAYILXgHBJff01oTP/jRCAyxtXQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPdCtDy5cuVkpKi6upqb9/JkydVVVWl/Px8ZWdna8aMGWptbe3unACAPqbLAdqxY4eee+453Xjjjb79CxYs0Jtvvqm6ujo1NDTo0KFDmj59ercHBQD0LV0K0Oeff66Kigo9//zzuuKKK7z90WhUv//97/XEE0/o//7v/zRmzBitXbtWf//737V169ZzPlZ7e7tisZhvAQD6vi4FqKqqSlOmTFF5eblvf1NTk06fPu3bX1JSoqKiIjU2Np7zsWpraxUMBr1VWFjYlZEAAL1MpwO0fv16vfvuu6qtrT3rvkgkooyMDOXl5fn2h0IhRSKRcz7e4sWLFY1GvdXS0tLZkQAAvVCnvo6hpaVF8+fP18aNG5WZmZmQAQKBgAKBQEIeC0DvNn/+fN/2mf9H9+OPP+7JcXCJdeoKqKmpSUeOHNFNN92ktLQ0paWlqaGhQU899ZTS0tIUCoV06tQptbW1+X6utbVV4XA4kXMDAHq5Tl0BTZo0Se+//75v3z333KOSkhI9+OCDKiwsVHp6uurr6zVjxgxJUnNzsw4ePKiysrLETQ0A6PU6FaCcnByNGDHCt2/AgAHKz8/39t97772qqanRwIEDlZubq3nz5qmsrEw333xz4qYG0CfNmTPHt/3CCy/4tnkKrm9J+FdyP/nkk0pNTdWMGTPU3t6uyZMn65lnnkn0rwEA9HLdDtDmzZt925mZmVq1apVWrVrV3YcGAPRhfBYcAMAEAQIAmCBAAAATBAgAYIIAAQBMJPxt2EBfdeWVV/q2b7nlFqNJzm/YsGHWIwAXjSsgAIAJAgQAMMFTcOgVxo8f79seMGBAj88watQo3/YTTzzR4zMAfQlXQAAAEwQIAGCCAAEATPAaEJLSmV/7sW7dOt/28OHDe3IcAJcAV0AAABMECABgggABAEzwGhCSxtc/Rmbjxo2++8LhcE+PA+AS4woIAGCCAAEATBAgAIAJXgNC0tizZ493Ozs723ASAD2BKyAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADARJr1AMB/zZkzx7u9cuVK331XXHFFT4/TaUePHvVtL1y40GiS/7nrrrt827feeqvRJMDZuAICAJggQAAAEwQIAGCC14CQNP785z97t7Oysnz3DRgwoKfH6bRYLObbXrt2rdEk/zNixAjfNq8BIZlwBQQAMEGAAAAmCBAAwASvASEprVmzxnoEAJcYV0AAABMECABgggABAEwQIACACQIEADDR6QB98sknmjlzpvLz85WVlaWRI0dq586d3v3OOS1dulRDhgxRVlaWysvLtX///oQODQDo/ToVoM8++0wTJ05Uenq6/va3v2nv3r16/PHHfR+V/9vf/lZPPfWUVq9erW3btmnAgAGaPHmyTp48mfDhAQC9V6f+HdBvfvMbFRYW+j7jqri42LvtnNOKFSv0y1/+UlOnTpUk/elPf1IoFNJrr7121neTSFJ7e7va29u97TM/TwsA0Dd16grojTfe0NixY3XHHXdo8ODBGj16tJ5//nnv/gMHDigSiai8vNzbFwwGVVpaqsbGxnM+Zm1trYLBoLcKCwu7+EcBAPQmnQrQRx99pGeffVbXXnut3n77bd1///164IEH9MILL0iSIpGIJCkUCvl+LhQKefedafHixYpGo95qaWnpyp8DANDLdOopuHg8rrFjx+rRRx+VJI0ePVp79uzR6tWrVVlZ2aUBAoGAAoFAl34WANB7deoKaMiQIbr++ut9+6677jodPHhQkhQOhyVJra2tvmNaW1u9+wAAkDoZoIkTJ6q5udm3b9++fRo2bJikr96QEA6HVV9f790fi8W0bds2lZWVJWBcAEBf0amn4BYsWKAJEybo0Ucf1Q9/+ENt375da9as8T65OCUlRdXV1fr1r3+ta6+9VsXFxVqyZIkKCgo0bdq0SzE/AKCX6lSAxo0bp1dffVWLFy/Wr371KxUXF2vFihWqqKjwjlm4cKGOHz+u++67T21tbbrlllv01ltvKTMzM+HDAwB6rxTnnLMe4utisZiCwaD1GECf8Pjjj/u2a2pqjCbpmtLSUt/29u3bjSZBV0SjUeXm5p73fj4LDgBgggABAEwQIACACQIEADBBgAAAJjr1NmwA/5Oenu7bHjx4sNEk55ednW09AnBeXAEBAEwQIACACZ6CQ1Lq37+/bzslJcVokvMbOXKkb/t833kF4Ny4AgIAmCBAAAATBAgAYILXgJCUzvzeqaFDhxpNAuBS4QoIAGCCAAEATBAgAIAJXgNC0vjss8+823wpIdD3cQUEADBBgAAAJggQAMAErwGhx5z59QUff/yxb/vrr/sk42e/AUgsroAAACYIEADABE/BwUwyfoMogJ7DFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATKRZDwAA/7VmzRrf9uHDh40mQU/gCggAYIIAAQBMECAAgAleA0KPicfjvu3nnnvOtz1r1izvdnp6eo/M1Nds3rzZt93c3GwzSBc9/PDDvu3W1lajSdATuAICAJggQAAAEwQIAGCC14DQYzo6Onzbs2fP9m1nZWV5twOBQI/M1NesXr3at33ma0JAMuEKCABgggABAEwQIACAiRTnnLMe4utisZiCwaD1GACAbopGo8rNzT3v/VwBAQBMECAAgAkCBAAwQYAAACYIEADARKcC1NHRoSVLlqi4uFhZWVkaPny4HnnkEX39jXTOOS1dulRDhgxRVlaWysvLtX///oQPDgDo5VwnLFu2zOXn57sNGza4AwcOuLq6Opedne1WrlzpHbN8+XIXDAbda6+95nbv3u1uv/12V1xc7L744ouL+h3RaNRJYrFYLFYvX9Fo9Bv/vu9UgKZMmeJmzZrl2zd9+nRXUVHhnHMuHo+7cDjsHnvsMe/+trY2FwgE3EsvvXTOxzx58qSLRqPeamlpMT9pLBaLxer+ulCAOvUU3IQJE1RfX699+/ZJknbv3q0tW7botttukyQdOHBAkUhE5eXl3s8Eg0GVlpaqsbHxnI9ZW1urYDDorcLCws6MBADopTr1adiLFi1SLBZTSUmJ+vXrp46ODi1btkwVFRWSpEgkIkkKhUK+nwuFQt59Z1q8eLFqamq87VgsRoQA4DLQqQC9/PLLevHFF7Vu3TrdcMMN2rVrl6qrq1VQUKDKysouDRAIBPjofQC4HHXmNaChQ4e63/3ud759jzzyiPvWt77lnHPu3//+t5Pk/vGPf/iO+c53vuMeeOCBi/odvAmBxWKx+sZK6GtAJ06cUGqq/0f69euneDwuSSouLlY4HFZ9fb13fywW07Zt21RWVtaZXwUA6Osu/vrHucrKSnfVVVd5b8N+5ZVX3KBBg9zChQu9Y5YvX+7y8vLc66+/7t577z03depU3obNYrFYl+FK6NuwY7GYmz9/visqKnKZmZnummuucQ899JBrb2/3jonH427JkiUuFAq5QCDgJk2a5Jqbmy/6dxAgFovF6hvrQgHi+4AAAJcE3wcEAEhKBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwETSBcg5Zz0CACABLvT3edIF6NixY9YjAAAS4EJ/n6e4JLvkiMfjOnTokJxzKioqUktLi3Jzc63HSlqxWEyFhYWcpwvgPF0cztPF4Tx9M+ecjh07poKCAqWmnv86J60HZ7ooqampGjp0qGKxmCQpNzeX/4EvAufp4nCeLg7n6eJwns4vGAxe8JikewoOAHB5IEAAABNJG6BAIKCHH35YgUDAepSkxnm6OJyni8N5ujicp8RIujchAAAuD0l7BQQA6NsIEADABAECAJggQAAAEwQIAGAiaQO0atUqXX311crMzFRpaam2b99uPZKZ2tpajRs3Tjk5ORo8eLCmTZum5uZm3zEnT55UVVWV8vPzlZ2drRkzZqi1tdVo4uSwfPlypaSkqLq62tvHefrKJ598opkzZyo/P19ZWVkaOXKkdu7c6d3vnNPSpUs1ZMgQZWVlqby8XPv37zecuOd1dHRoyZIlKi4uVlZWloYPH65HHnnE9wGbnKducklo/fr1LiMjw/3hD39wH3zwgfvJT37i8vLyXGtrq/VoJiZPnuzWrl3r9uzZ43bt2uW+//3vu6KiIvf55597x8yePdsVFha6+vp6t3PnTnfzzTe7CRMmGE5ta/v27e7qq692N954o5s/f763n/Pk3KeffuqGDRvm7r77brdt2zb30Ucfubffftv961//8o5Zvny5CwaD7rXXXnO7d+92t99+uysuLnZffPGF4eQ9a9myZS4/P99t2LDBHThwwNXV1bns7Gy3cuVK7xjOU/ckZYDGjx/vqqqqvO2Ojg5XUFDgamtrDadKHkeOHHGSXENDg3POuba2Npeenu7q6uq8Yz788EMnyTU2NlqNaebYsWPu2muvdRs3bnTf/e53vQBxnr7y4IMPultuueW898fjcRcOh91jjz3m7Wtra3OBQMC99NJLPTFiUpgyZYqbNWuWb9/06dNdRUWFc47zlAhJ9xTcqVOn1NTUpPLycm9famqqysvL1djYaDhZ8ohGo5KkgQMHSpKampp0+vRp3zkrKSlRUVHRZXnOqqqqNGXKFN/5kDhP//XGG29o7NixuuOOOzR48GCNHj1azz//vHf/gQMHFIlEfOcpGAyqtLT0sjpPEyZMUH19vfbt2ydJ2r17t7Zs2aLbbrtNEucpEZLu07CPHj2qjo4OhUIh3/5QKKR//vOfRlMlj3g8rurqak2cOFEjRoyQJEUiEWVkZCgvL893bCgUUiQSMZjSzvr16/Xuu+9qx44dZ93HefrKRx99pGeffVY1NTX6xS9+oR07duiBBx5QRkaGKisrvXNxrv8GL6fztGjRIsViMZWUlKhfv37q6OjQsmXLVFFRIUmcpwRIugDhm1VVVWnPnj3asmWL9ShJp6WlRfPnz9fGjRuVmZlpPU7SisfjGjt2rB599FFJ0ujRo7Vnzx6tXr1alZWVxtMlj5dfflkvvvii1q1bpxtuuEG7du1SdXW1CgoKOE8JknRPwQ0aNEj9+vU7651Jra2tCofDRlMlh7lz52rDhg165513NHToUG9/OBzWqVOn1NbW5jv+cjtnTU1NOnLkiG666SalpaUpLS1NDQ0Neuqpp5SWlqZQKMR5kjRkyBBdf/31vn3XXXedDh48KEneubjc/xv8+c9/rkWLFumuu+7SyJEj9aMf/UgLFixQbW2tJM5TIiRdgDIyMjRmzBjV19d7++LxuOrr61VWVmY4mR3nnObOnatXX31VmzZtUnFxse/+MWPGKD093XfOmpubdfDgwcvqnE2aNEnvv/++du3a5a2xY8eqoqLCu815kiZOnHjW2/j37dunYcOGSZKKi4sVDod95ykWi2nbtm2X1Xk6ceLEWd/m2a9fP8XjcUmcp4SwfhfEuaxfv94FAgH3xz/+0e3du9fdd999Li8vz0UiEevRTNx///0uGAy6zZs3u8OHD3vrxIkT3jGzZ892RUVFbtOmTW7nzp2urKzMlZWVGU6dHL7+LjjnOE/OffUW9bS0NLds2TK3f/9+9+KLL7r+/fu7v/zlL94xy5cvd3l5ee7111937733nps6depl9/biyspKd9VVV3lvw37llVfcoEGD3MKFC71jOE/dk5QBcs65p59+2hUVFbmMjAw3fvx4t3XrVuuRzEg651q7dq13zBdffOHmzJnjrrjiCte/f3/3gx/8wB0+fNhu6CRxZoA4T19588033YgRI1wgEHAlJSVuzZo1vvvj8bhbsmSJC4VCLhAIuEmTJrnm5majaW3EYjE3f/58V1RU5DIzM90111zjHnroIdfe3u4dw3nqHr4PCABgIuleAwIAXB4IEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCY+H+bpJZiShNLcQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4 (E)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label} ({allowed_boggle_tiles[label]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to define a loss function and optimizer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Next, I'm going to run through the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [05:02<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.954178271060003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [03:26<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.0009480756674200795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [03:18<00:00,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 0.00013201554637597084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = BoggleCNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Iterate through the data \n",
    "epoch_amt = 3\n",
    "for epoch in range(epoch_amt):\n",
    "    \n",
    "    # We're going to keep track of the loss for each epoch\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Iterate through the data\n",
    "    for (data, target) in tqdm(train_loader):\n",
    "        \n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make predictions\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_function(output, target)\n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add the loss to the epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # Print the epoch loss\n",
    "    print(f\"Epoch {epoch + 1} loss: {epoch_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Loop \n",
    "Now that we've got a trained model, we ought to test it on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "incorrect_labels = []\n",
    "\n",
    "# Iterate through the test set\n",
    "for (data, target) in tqdm(original_image_loader):\n",
    "        \n",
    "        # Make predictions\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the predicted class\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        \n",
    "        # Add the number of correct predictions to the total correct\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Collect the true labels of the incorrectly predicted images\n",
    "        for idx, label in enumerate(target):\n",
    "            if predicted[idx] != label:\n",
    "                incorrect_labels.append(allowed_boggle_tiles[label])\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on the test set: {correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model\n",
    "Now that we've created a model, we ought to save it to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path to save the model\n",
    "model_folder = Path(\"models\")\n",
    "model_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), model_folder / \"boggle_cnn.pth\")\n",
    "\n",
    "app_model_folder = Path(\"boggle-vision-app/boggle-vision-api/models\")\n",
    "app_model_folder.mkdir(exist_ok=True, parents=True)\n",
    "torch.save(model.state_dict(), app_model_folder / \"boggle_cnn.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
