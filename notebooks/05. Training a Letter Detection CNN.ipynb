{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "After playing around with a couple of off-the-shelf OCR engines, I decided to try my hand at creating my own model. I want something that's lightweight and accurate.\n",
    "\n",
    "In this notebook, I'm going to try my hand at training a convolutional neural network to detect Boggle letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "The cells below will set up the rest of the notebook. \n",
    "\n",
    "I'll start by configuring the kernel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\data\\programming\\boggle-vision\n"
     ]
    }
   ],
   "source": [
    "# Change the directory to the root of the repo \n",
    "%cd ..\n",
    "\n",
    "# Enable the autoreload extension\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the kernel is configured, I'm going to load in some modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import statements\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch-related import statements\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Importing custom modules from this repo\n",
    "from utils.settings import allowed_boggle_tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "I'm going to start by creating a custom `Dataset` for my network, and by specifying a `DataLoader`. Since it's been a bit, I'm going to follow [the Pytorch documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) for this.\n",
    "\n",
    "I'll start with the `Dataset`, which I'll call a `BoggleTileImageDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoggleTileImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is a custom Dataset for handling the Boggle tile images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_directory):\n",
    "        \"\"\"\n",
    "        This is the initialization method. Here, I'll set some class variables\n",
    "        for the image directory and the image paths.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set the img_dir class variable\n",
    "        self.img_dir = image_directory\n",
    "        \n",
    "        # Create a mapping of letters to integers\n",
    "        self.letter_to_int_dict = {letter: idx for idx, letter in enumerate(allowed_boggle_tiles)}\n",
    "\n",
    "        # Create a class variable mapping each of the image paths to their labels\n",
    "        self.img_path_to_label_dict = {}\n",
    "        for child_file in Path(image_directory).iterdir():\n",
    "            if child_file.suffix == \".png\":\n",
    "                cur_file_letter = child_file.name.split(\"_\")[0]\n",
    "                self.img_path_to_label_dict[child_file] = cur_file_letter\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        This method will return the length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.img_path_to_label_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        This method will return the image and the label for a given index.\n",
    "        \"\"\"\n",
    "        img_path = list(self.img_path_to_label_dict.keys())[idx]\n",
    "        img = read_image(str(img_path)).float()\n",
    "        label = self.img_path_to_label_dict[img_path]\n",
    "        label_int = self.letter_to_int_dict[label]\n",
    "        label = torch.tensor(label_int)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this custom `Dataset` in hand, I'm going to create an instance of it (as well as an accompanying `DataLoader`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the full dataset\n",
    "full_dataset = BoggleTileImageDataset(\"data/training-data\")\n",
    "original_image_dataset = BoggleTileImageDataset(\"data/original-training-data\")\n",
    "\n",
    "# Calculate lengths for each split\n",
    "train_length = int(0.93 * len(full_dataset))\n",
    "val_length = int(0.05 * len(full_dataset))\n",
    "test_length = len(full_dataset) - train_length - val_length\n",
    "\n",
    "# Split the dataset\n",
    "train_data, val_data, test_data = random_split(full_dataset, [train_length, val_length, test_length])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "original_image_loader = DataLoader(original_image_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MISC:** Below, I can show one of the images from the training data, as well as its corresponding label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([256, 1, 100, 100])\n",
      "Labels batch shape: torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbeklEQVR4nO3df2xV9f3H8VdL29vyo7dQwr1UWqyGWBWNSAULZstGnXEYQYjOpGqnRqMWpbAM7Rwsw2G7GSfomCjZmGYqs4mCskxHijZhVpA6UKYWFCKdcMvc1nsRaGG9n+8ffnfGkR+29NL3ve3zkXyS8znnc89994P0xeee47lpzjknAAD6WLp1AQCAgYkAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJg4YwG0fPlynX322crOztbkyZO1efPmM/VWAIAUlHYmngX3hz/8QbfccotWrFihyZMna+nSpaqvr1dLS4tGjRp1ytfG43Ht3btXw4YNU1paWqJLAwCcYc45HThwQAUFBUpPP8U6x50BkyZNclVVVV6/q6vLFRQUuNra2q99bWtrq5NEo9FotBRvra2tp/x9n/CP4I4cOaLm5maVl5d7+9LT01VeXq6mpqbjxnd2dioWi3nN8XBuAOgXhg0bdsrjCQ+gzz//XF1dXQqFQr79oVBIkUjkuPG1tbUKBoNeKyoqSnRJAAADX3cZxfwuuJqaGkWjUa+1trZalwQA6AMZiT7hyJEjNWjQILW1tfn2t7W1KRwOHzc+EAgoEAgkugwAQJJL+AooKytLEydOVENDg7cvHo+roaFBZWVliX47AECKSvgKSJLmz5+vyspKlZaWatKkSVq6dKkOHjyoW2+99Uy8HQAgBZ2RAPre976nf/zjH1q0aJEikYguueQSvfbaa8fdmAAAGLjOyP+I2huxWEzBYNC6DABAL0WjUeXm5p70uPldcACAgYkAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZ6FEC1tbW67LLLNGzYMI0aNUozZ85US0uLb0xHR4eqqqqUn5+voUOHavbs2Wpra0to0QCA1NejAGpsbFRVVZXefvttrV+/XkePHtV3vvMdHTx40Bszb948vfrqq6qvr1djY6P27t2rWbNmJbxwAECKc72wf/9+J8k1NjY655xrb293mZmZrr6+3hvz4YcfOkmuqanphOfo6Ohw0WjUa62trU4SjUaj0VK8RaPRU2ZIr64BRaNRSdKIESMkSc3NzTp69KjKy8u9MSUlJSoqKlJTU9MJz1FbW6tgMOi1wsLC3pQEAEgRpx1A8Xhc1dXVmjp1qsaPHy9JikQiysrKUl5enm9sKBRSJBI54XlqamoUjUa91traerolAQBSSMbpvrCqqkrbt2/Xxo0be1VAIBBQIBDo1TkAAKnntFZAc+bM0bp16/TGG29ozJgx3v5wOKwjR46ovb3dN76trU3hcLhXhQIA+pceBZBzTnPmzNHLL7+sDRs2qLi42Hd84sSJyszMVENDg7evpaVFe/bsUVlZWWIqBgD0Cz36CK6qqkrPP/+81q5dq2HDhnnXdYLBoHJychQMBnX77bdr/vz5GjFihHJzc3XvvfeqrKxMl19++Rn5AQAAKaont13rJLfarVq1yhtz+PBhd88997jhw4e7wYMHu+uuu87t27ev2+8RjUbNbx2k0Wg0Wu/b192Gnfb/wZI0YrGYgsGgdRkAgF6KRqPKzc096XGeBQcAMEEAAQBMEEAAABMEEADABAEEADBx2o/iAWBjyJAh3vaUKVMMK+mer34f2HvvvWdUCZINKyAAgAkCCABggo/ggBRz7Hdm/fnPfzaspHvWr1/v699zzz2+/scff9yX5SCJsAICAJgggAAAJgggAIAJrgEBOKOuvPJKX/+pp57y9adNm9aX5SCJsAICAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmeBQPkOS++q2nGzZsMKoESCxWQAAAEwQQAMAEAQQAMME1ICDJpaf7/50YCASMKgESixUQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABN+ICiS5Tz/91NdftmyZtz137ty+LgdIGFZAAAATBBAAwAQBBAAwwTUgIMm1trb6+itWrPC2uQaEVMYKCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgolcBVFdXp7S0NFVXV3v7Ojo6VFVVpfz8fA0dOlSzZ89WW1tbb+sEAPQzpx1A77zzjp566ildfPHFvv3z5s3Tq6++qvr6ejU2Nmrv3r2aNWtWrwsFBqrs7GxfGzt2rNeAVHZaAfTFF1+ooqJCK1eu1PDhw7390WhUv/nNb/TLX/5S3/72tzVx4kStWrVKb731lt5+++0Tnquzs1OxWMzXAAD932kFUFVVlaZPn67y8nLf/ubmZh09etS3v6SkREVFRWpqajrhuWpraxUMBr1WWFh4OiUBAFJMjwNo9erVevfdd1VbW3vcsUgkoqysLOXl5fn2h0IhRSKRE56vpqZG0WjUa1/99kcAQP/Uo6/kbm1t1dy5c7V+/XplZ2cnpIBAIKBAIJCQcwH9UWlpqa//2muvGVUCJFaPVkDNzc3av3+/Lr30UmVkZCgjI0ONjY16/PHHlZGRoVAopCNHjqi9vd33ura2NoXD4UTWDQBIcT1aAU2bNk3vv/++b9+tt96qkpIS3X///SosLFRmZqYaGho0e/ZsSVJLS4v27NmjsrKyxFUNAEh5PQqgYcOGafz48b59Q4YMUX5+vrf/9ttv1/z58zVixAjl5ubq3nvvVVlZmS6//PLEVQ0ASHk9CqDueOyxx5Senq7Zs2ers7NTV111lX79618n+m0AACmu1wH05ptv+vrZ2dlavny5li9f3ttTAwD6MZ4FBwAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARMKfhAAgsd59911f/5prrvG2161b19flAAnDCggAYIIAAgCYIIAAACa4BgQkuUOHDvn6n3zyiVElQGKxAgIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJjIsC4A+K9wOOxt5+XlmdTw0UcfmbzvqQwePNjXP/fcc40qARKLFRAAwAQBBAAwwUdwMBMMBn39uro6b7uysrKvy5EkjR492tffv3+/tx2Px/u6HEnSpZde6uuvW7fOpA4g0VgBAQBMEEAAABMEEADABNeAYGblypW+/vXXX29Uyf/s27fP1z/2mlAkEunrcoB+jRUQAMAEAQQAMEEAAQBMcA0IOIVjrwmdf/75vmPJ+NgeIJWwAgIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCY6HEAffbZZ7rpppuUn5+vnJwcXXTRRdqyZYt33DmnRYsWafTo0crJyVF5ebl27tyZ0KIBAKmvRwH073//W1OnTlVmZqb+9Kc/6YMPPtCjjz6q4cOHe2N+8Ytf6PHHH9eKFSu0adMmDRkyRFdddZU6OjoSXjwAIHX16FlwP//5z1VYWKhVq1Z5+4qLi71t55yWLl2qH//4x5oxY4Yk6dlnn1UoFNKaNWt04403HnfOzs5OdXZ2ev1YLNbjHwIAkHp6tAJ65ZVXVFpaquuvv16jRo3ShAkTfF8qtnv3bkUiEZWXl3v7gsGgJk+erKamphOes7a2VsFg0GuFhYWn+aMAAFJJjwJo165devLJJzVu3Di9/vrruvvuu3XffffpmWeekfS/b4wMhUK+14VCoZN+m2RNTY2i0ajXWltbT+fnAACkmB59BBePx1VaWqqHH35YkjRhwgRt375dK1asUGVl5WkVEAgEFAgETuu1AIDU1aMV0OjRo3XBBRf49p1//vnas2ePJCkcDkuS2trafGPa2tq8YwAASD0MoKlTp6qlpcW3b8eOHRo7dqykL29ICIfDamho8I7HYjFt2rRJZWVlCSgXANBf9OgjuHnz5mnKlCl6+OGHdcMNN2jz5s16+umn9fTTT0uS0tLSVF1drZ/97GcaN26ciouLtXDhQhUUFGjmzJlnon4ASW7z5s2+fl1dnVElSDY9CqDLLrtML7/8smpqarR48WIVFxdr6dKlqqio8MYsWLBABw8e1J133qn29nZdccUVeu2115SdnZ3w4gEAqatHASRJ11xzja655pqTHk9LS9PixYu1ePHiXhUGAOjfeBYcAMBEj1dAwEBVU1Pj6//0pz/19Xft2pWQ9xk/fryvf++99ybkvFY+/fRTX3/9+vVGlSDZsAICAJgggAAAJgggAIAJrgEB3XTLLbf4+sc+xV06/lrHH//4R29769atpzz3JZdc4m3/4Ac/8B274YYbelAlkDpYAQEATBBAAAATfAQHnKY77rjjlMfPO+88b/utt9465dgpU6Z42zfddFPvCgNSBCsgAIAJAggAYIIAAgCY4BoQcIbcfPPNJ9zu7z755BNfv6mpyagSJDtWQAAAEwQQAMAEAQQAMME1IJjZvn27r19WVuZtjxkzpq/LQYJ89esWHnvsMaNKkOxYAQEATBBAAAATBBAAwATXgGBm8eLFvn5XV5e3/dWvoQ6FQn1SE05PW1ubt/33v//dsBKkElZAAAATBBAAwAQfwSFpLFmyxNs+cuSI79iDDz7o6weDwT6pCd2zcuVKb/vYP0fgVFgBAQBMEEAAABMEEADABNeAkJQeeeQRX7+jo8PXf/TRR339zMzMM14T/ufo0aO+/n/+8x+jSpDKWAEBAEwQQAAAEwQQAMAE14CQEp544glfv7293dd/9tln+7AaLFiwwNdfunSpTSFIaayAAAAmCCAAgAk+gkNKeuGFF3z9L774wtt+6aWX+rqcfu+OO+7w9Z955hmjStCfsAICAJgggAAAJgggAICJNOecsy7iWLFYjEfto8dycnK87SuvvNJ3bO3atX1dTkqaMWOGr79161Zv+5///Kfv2MGDB/uiJKS4aDSq3Nzckx5nBQQAMEEAAQBMEEAAABNcA0K/M2TIEF9/3Lhxvv6x/329+eabfVFSUvrqNZ+GhgZfn+s86C2uAQEAkhIBBAAwwUdwGHAyMv73BKpvfetbpxz74osvett5eXlnqqTT9tWngt9www3dfu1f/vIXX//QoUOJKAnw8BEcACApEUAAABMEEADABNeAgFOYOXOmt52dnW1XyEl0dHT4+mvWrLEpBDgBrgEBAJISAQQAMEEAAQBMcA0IAHBGcA0IAJCUCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZ6FEBdXV1auHChiouLlZOTo3PPPVcPPfSQjn2YgnNOixYt0ujRo5WTk6Py8nLt3Lkz4YUDAFKc64ElS5a4/Px8t27dOrd7925XX1/vhg4d6pYtW+aNqaurc8Fg0K1Zs8Zt27bNXXvtta64uNgdPny4W+8RjUadJBqNRqOleItGo6f8fd+jAJo+fbq77bbbfPtmzZrlKioqnHPOxeNxFw6H3SOPPOIdb29vd4FAwL3wwgsnPGdHR4eLRqNea21tNZ80Go1Go/W+fV0A9egjuClTpqihoUE7duyQJG3btk0bN27U1VdfLUnavXu3IpGIysvLvdcEg0FNnjxZTU1NJzxnbW2tgsGg1woLC3tSEgAgRWX0ZPADDzygWCymkpISDRo0SF1dXVqyZIkqKiokSZFIRJIUCoV8rwuFQt6xr6qpqdH8+fO9fiwWI4QAYADoUQC9+OKLeu655/T888/rwgsv1NatW1VdXa2CggJVVlaeVgGBQECBQOC0XgsASGE9uQY0ZswY96tf/cq376GHHnLnnXeec865Tz75xElyf/3rX31jvvGNb7j77ruvW+/BTQg0Go3WP1pCrwEdOnRI6en+lwwaNEjxeFySVFxcrHA4rIaGBu94LBbTpk2bVFZW1pO3AgD0d91f/zhXWVnpzjrrLO827JdeesmNHDnSLViwwBtTV1fn8vLy3Nq1a917773nZsyYwW3YNBqNNgBbQm/DjsVibu7cua6oqMhlZ2e7c845xz344IOus7PTGxOPx93ChQtdKBRygUDATZs2zbW0tHT7PQggGo1G6x/t6wIozbljHmOQBGKxmILBoHUZAIBeikajys3NPelxngUHADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATSRdAzjnrEgAACfB1v8+TLoAOHDhgXQIAIAG+7vd5mkuyJUc8HtfevXvlnFNRUZFaW1uVm5trXVbSisViKiwsZJ6+BvPUPcxT9zBPp+ac04EDB1RQUKD09JOvczL6sKZuSU9P15gxYxSLxSRJubm5/AF3A/PUPcxT9zBP3cM8nVwwGPzaMUn3ERwAYGAggAAAJpI2gAKBgH7yk58oEAhYl5LUmKfuYZ66h3nqHuYpMZLuJgQAwMCQtCsgAED/RgABAEwQQAAAEwQQAMAEAQQAMJG0AbR8+XKdffbZys7O1uTJk7V582brkszU1tbqsssu07BhwzRq1CjNnDlTLS0tvjEdHR2qqqpSfn6+hg4dqtmzZ6utrc2o4uRQV1entLQ0VVdXe/uYpy999tlnuummm5Sfn6+cnBxddNFF2rJli3fcOadFixZp9OjRysnJUXl5uXbu3GlYcd/r6urSwoULVVxcrJycHJ177rl66KGHfA/YZJ56ySWh1atXu6ysLPfb3/7W/e1vf3N33HGHy8vLc21tbdalmbjqqqvcqlWr3Pbt293WrVvdd7/7XVdUVOS++OILb8xdd93lCgsLXUNDg9uyZYu7/PLL3ZQpUwyrtrV582Z39tlnu4svvtjNnTvX2888Ofevf/3LjR071n3/+993mzZtcrt27XKvv/66+/jjj70xdXV1LhgMujVr1rht27a5a6+91hUXF7vDhw8bVt63lixZ4vLz8926devc7t27XX19vRs6dKhbtmyZN4Z56p2kDKBJkya5qqoqr9/V1eUKCgpcbW2tYVXJY//+/U6Sa2xsdM45197e7jIzM119fb035sMPP3SSXFNTk1WZZg4cOODGjRvn1q9f7775zW96AcQ8fen+++93V1xxxUmPx+NxFw6H3SOPPOLta29vd4FAwL3wwgt9UWJSmD59urvtttt8+2bNmuUqKiqcc8xTIiTdR3BHjhxRc3OzysvLvX3p6ekqLy9XU1OTYWXJIxqNSpJGjBghSWpubtbRo0d9c1ZSUqKioqIBOWdVVVWaPn26bz4k5um/XnnlFZWWlur666/XqFGjNGHCBK1cudI7vnv3bkUiEd88BYNBTZ48eUDN05QpU9TQ0KAdO3ZIkrZt26aNGzfq6quvlsQ8JULSPQ37888/V1dXl0KhkG9/KBTSRx99ZFRV8ojH46qurtbUqVM1fvx4SVIkElFWVpby8vJ8Y0OhkCKRiEGVdlavXq13331X77zzznHHmKcv7dq1S08++aTmz5+vH/3oR3rnnXd03333KSsrS5WVld5cnOjv4ECapwceeECxWEwlJSUaNGiQurq6tGTJElVUVEgS85QASRdAOLWqqipt375dGzdutC4l6bS2tmru3Llav369srOzrctJWvF4XKWlpXr44YclSRMmTND27du1YsUKVVZWGleXPF588UU999xzev7553XhhRdq69atqq6uVkFBAfOUIEn3EdzIkSM1aNCg4+5MamtrUzgcNqoqOcyZM0fr1q3TG2+8oTFjxnj7w+Gwjhw5ovb2dt/4gTZnzc3N2r9/vy699FJlZGQoIyNDjY2Nevzxx5WRkaFQKMQ8SRo9erQuuOAC377zzz9fe/bskSRvLgb638Ef/vCHeuCBB3TjjTfqoosu0s0336x58+aptrZWEvOUCEkXQFlZWZo4caIaGhq8ffF4XA0NDSorKzOszI5zTnPmzNHLL7+sDRs2qLi42Hd84sSJyszM9M1ZS0uL9uzZM6DmbNq0aXr//fe1detWr5WWlqqiosLbZp6kqVOnHncb/44dOzR27FhJUnFxscLhsG+eYrGYNm3aNKDm6dChQ8d9m+egQYMUj8clMU8JYX0XxImsXr3aBQIB97vf/c598MEH7s4773R5eXkuEolYl2bi7rvvdsFg0L355ptu3759Xjt06JA35q677nJFRUVuw4YNbsuWLa6srMyVlZUZVp0cjr0Lzjnmybkvb1HPyMhwS5YscTt37nTPPfecGzx4sPv973/vjamrq3N5eXlu7dq17r333nMzZswYcLcXV1ZWurPOOsu7Dfull15yI0eOdAsWLPDGME+9k5QB5JxzTzzxhCsqKnJZWVlu0qRJ7u2337YuyYykE7ZVq1Z5Yw4fPuzuueceN3z4cDd48GB33XXXuX379tkVnSS+GkDM05deffVVN378eBcIBFxJSYl7+umnfcfj8bhbuHChC4VCLhAIuGnTprmWlhajam3EYjE3d+5cV1RU5LKzs90555zjHnzwQdfZ2emNYZ56h+8DAgCYSLprQACAgYEAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJv4PW0Te76Y5CYYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9 (J)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label} ({allowed_boggle_tiles[label]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Network Architecture\n",
    "Next, I'm going to define an architecture for the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoggleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BoggleCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Placeholder for the feature map size, to be filled in later\n",
    "        self.feature_map_size = None \n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1, 128),  # Placeholder size, will adjust dynamically\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Pass through the convolutional layers\n",
    "        \n",
    "        # Dynamically calculate the feature map size if not done already\n",
    "        if self.feature_map_size is None:\n",
    "            self.feature_map_size = x.size(1) * x.size(2) * x.size(3)\n",
    "            self.classifier[0] = nn.Linear(self.feature_map_size, 128)  # Adjust the FC layer size\n",
    "            \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.classifier(x)     # Pass through the fully connected layers\n",
    "        return x\n",
    "    \n",
    "class EnhancedBoggleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EnhancedBoggleCNN, self).__init__()\n",
    "        \n",
    "        # More Convolutional layers\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Placeholder for the feature map size, to be filled in later\n",
    "        self.feature_map_size = None\n",
    "        \n",
    "        # More Fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1, 256),  # Placeholder size, will adjust dynamically\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Added Dropout to prevent overfitting\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Added Dropout to prevent overfitting\n",
    "            \n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Dynamically calculate the feature map size\n",
    "        if self.feature_map_size is None:\n",
    "            self.feature_map_size = x.size(1) * x.size(2) * x.size(3)\n",
    "            self.classifier[0] = nn.Linear(self.feature_map_size, 256)\n",
    "            \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = EnhancedBoggleCNN(num_classes=len(allowed_boggle_tiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to define a loss function and optimizer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Next, I'm going to run through the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [02:37<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 2.2871280049348806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 21/77 [00:28<01:16,  1.36s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\data\\programming\\boggle-vision\\notebooks\\05. Training a Letter Detection CNN.ipynb Cell 17\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Iterate through the data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m (data, target) \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Zero out the gradients\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# Make predictions\u001b[39;00m\n",
      "File \u001b[1;32md:\\data\\programming\\boggle-vision\\.venv\\Lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\data\\programming\\boggle-vision\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\data\\programming\\boggle-vision\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\data\\programming\\boggle-vision\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\data\\programming\\boggle-vision\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\data\\programming\\boggle-vision\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32md:\\data\\programming\\boggle-vision\\notebooks\\05. Training a Letter Detection CNN.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mThis method will return the image and the label for a given index.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_path_to_label_dict\u001b[39m.\u001b[39mkeys())[idx]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m img \u001b[39m=\u001b[39m read_image(\u001b[39mstr\u001b[39;49m(img_path))\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_path_to_label_dict[img_path]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/data/programming/boggle-vision/notebooks/05.%20Training%20a%20Letter%20Detection%20CNN.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m label_int \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mletter_to_int_dict[label]\n",
      "File \u001b[1;32md:\\data\\programming\\boggle-vision\\.venv\\Lib\\site-packages\\torchvision\\io\\image.py:258\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path, mode)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m    257\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m--> 258\u001b[0m data \u001b[39m=\u001b[39m read_file(path)\n\u001b[0;32m    259\u001b[0m \u001b[39mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[1;32md:\\data\\programming\\boggle-vision\\.venv\\Lib\\site-packages\\torchvision\\io\\image.py:52\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[0;32m     51\u001b[0m     _log_api_usage_once(read_file)\n\u001b[1;32m---> 52\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mread_file(path)\n\u001b[0;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32md:\\data\\programming\\boggle-vision\\.venv\\Lib\\site-packages\\torch\\_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    498\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    500\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 502\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate through the data \n",
    "epoch_amt = 5\n",
    "for epoch in range(epoch_amt):\n",
    "    \n",
    "    # We're going to keep track of the loss for each epoch\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Iterate through the data\n",
    "    for (data, target) in tqdm(train_loader):\n",
    "        \n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make predictions\n",
    "        output = model(data)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = loss_function(output, target)\n",
    "        \n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add the loss to the epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # Print the epoch loss\n",
    "    print(f\"Epoch {epoch + 1} loss: {epoch_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Loop \n",
    "Now that we've got a trained model, we ought to test it on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.8462962962962963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "incorrect_labels = []\n",
    "\n",
    "# Iterate through the test set\n",
    "for (data, target) in tqdm(original_image_loader):\n",
    "        \n",
    "        # Make predictions\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the predicted class\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        \n",
    "        # Add the number of correct predictions to the total correct\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Collect the true labels of the incorrectly predicted images\n",
    "        for idx, label in enumerate(target):\n",
    "            if predicted[idx] != label:\n",
    "                incorrect_labels.append(allowed_boggle_tiles[label])\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Accuracy on the test set: {correct / total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "letter\n",
       "T    20\n",
       "A    18\n",
       "E    16\n",
       "C    13\n",
       "R    12\n",
       "O    11\n",
       "S     9\n",
       "N     9\n",
       "P     7\n",
       "I     7\n",
       "D     6\n",
       "Y     6\n",
       "U     6\n",
       "M     5\n",
       "L     5\n",
       "G     5\n",
       "V     4\n",
       "B     3\n",
       "J     2\n",
       "F     1\n",
       "Z     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([{\n",
    "    \"letter\": letter\n",
    "} for letter in incorrect_labels]).groupby(\"letter\").size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model\n",
    "Now that we've created a model, we ought to save it to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a path to save the model\n",
    "model_folder = Path(\"models\")\n",
    "model_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), model_folder / \"boggle_cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
